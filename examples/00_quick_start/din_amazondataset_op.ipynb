{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/din_op/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/din_op/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "                          attention_mode=\"outer_product\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['relu', 'relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'attention_mod': 'inner_product',\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/din_op/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/din_op/',\n",
       " 'attention_mode': 'outer_product'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6a1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-05 12:13:02.206734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:02.212000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:02.212223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.137716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-05 12:13:03.138723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.138886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.138996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.410576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.410714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.410813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 12:13:03.410891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8844 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-05 12:13:03.423876: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-06-05 12:14:53.187257: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.5636, data_loss: 1.5636\n",
      "step 40 , total_loss: 1.5724, data_loss: 1.5724\n",
      "step 60 , total_loss: 1.5581, data_loss: 1.5581\n",
      "step 80 , total_loss: 1.5031, data_loss: 1.5031\n",
      "step 100 , total_loss: 1.5225, data_loss: 1.5225\n",
      "step 120 , total_loss: 1.5605, data_loss: 1.5605\n",
      "step 140 , total_loss: 1.5286, data_loss: 1.5286\n",
      "step 160 , total_loss: 1.5158, data_loss: 1.5158\n",
      "step 180 , total_loss: 1.5446, data_loss: 1.5446\n",
      "step 200 , total_loss: 1.5366, data_loss: 1.5366\n",
      "step 220 , total_loss: 1.5122, data_loss: 1.5122\n",
      "step 240 , total_loss: 1.5104, data_loss: 1.5104\n",
      "step 260 , total_loss: 1.5143, data_loss: 1.5143\n",
      "step 280 , total_loss: 1.4912, data_loss: 1.4912\n",
      "step 300 , total_loss: 1.4262, data_loss: 1.4262\n",
      "step 320 , total_loss: 1.4686, data_loss: 1.4686\n",
      "step 340 , total_loss: 1.4898, data_loss: 1.4898\n",
      "step 360 , total_loss: 1.4233, data_loss: 1.4233\n",
      "step 380 , total_loss: 1.4516, data_loss: 1.4516\n",
      "step 400 , total_loss: 1.4411, data_loss: 1.4411\n",
      "step 420 , total_loss: 1.4699, data_loss: 1.4699\n",
      "step 440 , total_loss: 1.4465, data_loss: 1.4465\n",
      "step 460 , total_loss: 1.3689, data_loss: 1.3689\n",
      "step 480 , total_loss: 1.4150, data_loss: 1.4150\n",
      "step 500 , total_loss: 1.4073, data_loss: 1.4073\n",
      "step 520 , total_loss: 1.3673, data_loss: 1.3673\n",
      "step 540 , total_loss: 1.3611, data_loss: 1.3611\n",
      "step 560 , total_loss: 1.3609, data_loss: 1.3609\n",
      "step 580 , total_loss: 1.4573, data_loss: 1.4573\n",
      "step 600 , total_loss: 1.3771, data_loss: 1.3771\n",
      "step 620 , total_loss: 1.3738, data_loss: 1.3738\n",
      "step 640 , total_loss: 1.3972, data_loss: 1.3972\n",
      "step 660 , total_loss: 1.3298, data_loss: 1.3298\n",
      "step 680 , total_loss: 1.3287, data_loss: 1.3287\n",
      "step 700 , total_loss: 1.3109, data_loss: 1.3109\n",
      "step 720 , total_loss: 1.3235, data_loss: 1.3235\n",
      "step 740 , total_loss: 1.3652, data_loss: 1.3652\n",
      "step 760 , total_loss: 1.3711, data_loss: 1.3711\n",
      "step 780 , total_loss: 1.3303, data_loss: 1.3303\n",
      "step 800 , total_loss: 1.3170, data_loss: 1.3170\n",
      "step 820 , total_loss: 1.3494, data_loss: 1.3494\n",
      "step 840 , total_loss: 1.3266, data_loss: 1.3266\n",
      "step 860 , total_loss: 1.2743, data_loss: 1.2743\n",
      "step 880 , total_loss: 1.2783, data_loss: 1.2783\n",
      "step 900 , total_loss: 1.2465, data_loss: 1.2465\n",
      "step 920 , total_loss: 1.3386, data_loss: 1.3386\n",
      "step 940 , total_loss: 1.2291, data_loss: 1.2291\n",
      "step 960 , total_loss: 1.2991, data_loss: 1.2991\n",
      "step 980 , total_loss: 1.2519, data_loss: 1.2519\n",
      "step 1000 , total_loss: 1.2977, data_loss: 1.2977\n",
      "step 1020 , total_loss: 1.2816, data_loss: 1.2816\n",
      "step 1040 , total_loss: 1.2730, data_loss: 1.2730\n",
      "step 1060 , total_loss: 1.2670, data_loss: 1.2670\n",
      "step 1080 , total_loss: 1.2872, data_loss: 1.2872\n",
      "step 1100 , total_loss: 1.3182, data_loss: 1.3182\n",
      "step 1120 , total_loss: 1.2250, data_loss: 1.2250\n",
      "step 1140 , total_loss: 1.2210, data_loss: 1.2210\n",
      "step 1160 , total_loss: 1.2300, data_loss: 1.2300\n",
      "step 1180 , total_loss: 1.1922, data_loss: 1.1922\n",
      "step 1200 , total_loss: 1.2680, data_loss: 1.2680\n",
      "step 1220 , total_loss: 1.2608, data_loss: 1.2608\n",
      "step 1240 , total_loss: 1.2061, data_loss: 1.2061\n",
      "step 1260 , total_loss: 1.2350, data_loss: 1.2350\n",
      "step 1280 , total_loss: 1.2175, data_loss: 1.2175\n",
      "step 1300 , total_loss: 1.1963, data_loss: 1.1963\n",
      "step 1320 , total_loss: 1.3182, data_loss: 1.3182\n",
      "step 1340 , total_loss: 1.1940, data_loss: 1.1940\n",
      "step 1360 , total_loss: 1.1978, data_loss: 1.1978\n",
      "step 1380 , total_loss: 1.2277, data_loss: 1.2277\n",
      "step 1400 , total_loss: 1.2091, data_loss: 1.2091\n",
      "step 1420 , total_loss: 1.2184, data_loss: 1.2184\n",
      "step 1440 , total_loss: 1.1222, data_loss: 1.1222\n",
      "step 1460 , total_loss: 1.2272, data_loss: 1.2272\n",
      "step 1480 , total_loss: 1.0854, data_loss: 1.0854\n",
      "step 1500 , total_loss: 1.1944, data_loss: 1.1944\n",
      "step 1520 , total_loss: 1.2169, data_loss: 1.2169\n",
      "step 1540 , total_loss: 1.2667, data_loss: 1.2667\n",
      "step 1560 , total_loss: 1.2069, data_loss: 1.2069\n",
      "step 1580 , total_loss: 1.2151, data_loss: 1.2151\n",
      "step 1600 , total_loss: 1.2804, data_loss: 1.2804\n",
      "step 1620 , total_loss: 1.2329, data_loss: 1.2329\n",
      "step 1640 , total_loss: 1.1131, data_loss: 1.1131\n",
      "step 1660 , total_loss: 1.1793, data_loss: 1.1793\n",
      "step 1680 , total_loss: 1.2203, data_loss: 1.2203\n",
      "step 1700 , total_loss: 1.2336, data_loss: 1.2336\n",
      "step 1720 , total_loss: 1.2183, data_loss: 1.2183\n",
      "step 1740 , total_loss: 1.2209, data_loss: 1.2209\n",
      "step 1760 , total_loss: 1.2163, data_loss: 1.2163\n",
      "step 1780 , total_loss: 1.1569, data_loss: 1.1569\n",
      "step 1800 , total_loss: 1.1900, data_loss: 1.1900\n",
      "step 1820 , total_loss: 1.1366, data_loss: 1.1366\n",
      "step 1840 , total_loss: 1.1793, data_loss: 1.1793\n",
      "step 1860 , total_loss: 1.1229, data_loss: 1.1229\n",
      "step 1880 , total_loss: 1.1714, data_loss: 1.1714\n",
      "step 1900 , total_loss: 1.1861, data_loss: 1.1861\n",
      "step 1920 , total_loss: 1.1225, data_loss: 1.1225\n",
      "step 1940 , total_loss: 1.2500, data_loss: 1.2500\n",
      "step 1960 , total_loss: 1.1324, data_loss: 1.1324\n",
      "step 1980 , total_loss: 1.1499, data_loss: 1.1499\n",
      "step 2000 , total_loss: 1.1801, data_loss: 1.1801\n",
      "step 2020 , total_loss: 1.1214, data_loss: 1.1214\n",
      "step 2040 , total_loss: 1.1800, data_loss: 1.1800\n",
      "step 2060 , total_loss: 1.1354, data_loss: 1.1354\n",
      "step 2080 , total_loss: 1.1068, data_loss: 1.1068\n",
      "step 2100 , total_loss: 1.1262, data_loss: 1.1262\n",
      "step 2120 , total_loss: 1.1329, data_loss: 1.1329\n",
      "step 2140 , total_loss: 1.1616, data_loss: 1.1616\n",
      "step 2160 , total_loss: 1.0608, data_loss: 1.0608\n",
      "step 2180 , total_loss: 1.0783, data_loss: 1.0783\n",
      "step 2200 , total_loss: 1.1361, data_loss: 1.1361\n",
      "step 2220 , total_loss: 1.1091, data_loss: 1.1091\n",
      "step 2240 , total_loss: 1.1232, data_loss: 1.1232\n",
      "step 2260 , total_loss: 1.1440, data_loss: 1.1440\n",
      "step 2280 , total_loss: 1.1309, data_loss: 1.1309\n",
      "step 2300 , total_loss: 1.1921, data_loss: 1.1921\n",
      "step 2320 , total_loss: 1.1102, data_loss: 1.1102\n",
      "step 2340 , total_loss: 1.1392, data_loss: 1.1392\n",
      "step 2360 , total_loss: 1.0935, data_loss: 1.0935\n",
      "step 2380 , total_loss: 1.0908, data_loss: 1.0908\n",
      "step 2400 , total_loss: 1.2598, data_loss: 1.2598\n",
      "step 2420 , total_loss: 1.1392, data_loss: 1.1392\n",
      "step 2440 , total_loss: 1.1456, data_loss: 1.1456\n",
      "step 2460 , total_loss: 1.0605, data_loss: 1.0605\n",
      "step 2480 , total_loss: 1.0050, data_loss: 1.0050\n",
      "step 2500 , total_loss: 1.1401, data_loss: 1.1401\n",
      "step 2520 , total_loss: 1.1704, data_loss: 1.1704\n",
      "step 2540 , total_loss: 1.0814, data_loss: 1.0814\n",
      "step 2560 , total_loss: 1.1980, data_loss: 1.1980\n",
      "step 2580 , total_loss: 1.1030, data_loss: 1.1030\n",
      "step 2600 , total_loss: 1.0396, data_loss: 1.0396\n",
      "step 2620 , total_loss: 1.1514, data_loss: 1.1514\n",
      "step 2640 , total_loss: 1.0767, data_loss: 1.0767\n",
      "step 2660 , total_loss: 1.0990, data_loss: 1.0990\n",
      "step 2680 , total_loss: 0.9962, data_loss: 0.9962\n",
      "step 2700 , total_loss: 1.1392, data_loss: 1.1392\n",
      "step 2720 , total_loss: 1.1244, data_loss: 1.1244\n",
      "step 2740 , total_loss: 1.1713, data_loss: 1.1713\n",
      "step 2760 , total_loss: 1.0626, data_loss: 1.0626\n",
      "step 2780 , total_loss: 1.1030, data_loss: 1.1030\n",
      "step 2800 , total_loss: 1.1427, data_loss: 1.1427\n",
      "step 2820 , total_loss: 1.0935, data_loss: 1.0935\n",
      "step 2840 , total_loss: 1.2187, data_loss: 1.2187\n",
      "step 2860 , total_loss: 1.1381, data_loss: 1.1381\n",
      "step 2880 , total_loss: 1.0821, data_loss: 1.0821\n",
      "step 2900 , total_loss: 1.1137, data_loss: 1.1137\n",
      "step 2920 , total_loss: 1.1140, data_loss: 1.1140\n",
      "step 2940 , total_loss: 1.1477, data_loss: 1.1477\n",
      "step 2960 , total_loss: 1.0990, data_loss: 1.0990\n",
      "step 2980 , total_loss: 1.0744, data_loss: 1.0744\n",
      "step 3000 , total_loss: 1.0895, data_loss: 1.0895\n",
      "step 3020 , total_loss: 1.1669, data_loss: 1.1669\n",
      "step 3040 , total_loss: 1.1314, data_loss: 1.1314\n",
      "step 3060 , total_loss: 1.1044, data_loss: 1.1044\n",
      "step 3080 , total_loss: 1.0530, data_loss: 1.0530\n",
      "step 3100 , total_loss: 1.0633, data_loss: 1.0633\n",
      "step 3120 , total_loss: 1.0767, data_loss: 1.0767\n",
      "step 3140 , total_loss: 1.0846, data_loss: 1.0846\n",
      "step 3160 , total_loss: 0.9965, data_loss: 0.9965\n",
      "step 3180 , total_loss: 1.0804, data_loss: 1.0804\n",
      "step 3200 , total_loss: 1.0985, data_loss: 1.0985\n",
      "step 3220 , total_loss: 1.0451, data_loss: 1.0451\n",
      "step 3240 , total_loss: 1.0891, data_loss: 1.0891\n",
      "step 3260 , total_loss: 1.0950, data_loss: 1.0950\n",
      "step 3280 , total_loss: 1.0690, data_loss: 1.0690\n",
      "step 3300 , total_loss: 1.0927, data_loss: 1.0927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval valid at epoch 1: auc:0.799,logloss:0.708,mean_mrr:0.7345,ndcg@2:0.6963,ndcg@4:0.7837,ndcg@6:0.801,group_auc:0.7948\n",
      "step 20 , total_loss: 1.0572, data_loss: 1.0572\n",
      "step 40 , total_loss: 1.0475, data_loss: 1.0475\n",
      "step 60 , total_loss: 1.0667, data_loss: 1.0667\n",
      "step 80 , total_loss: 1.0161, data_loss: 1.0161\n",
      "step 100 , total_loss: 1.0342, data_loss: 1.0342\n",
      "step 120 , total_loss: 1.0101, data_loss: 1.0101\n",
      "step 140 , total_loss: 1.0430, data_loss: 1.0430\n",
      "step 160 , total_loss: 1.0717, data_loss: 1.0717\n",
      "step 180 , total_loss: 1.0128, data_loss: 1.0128\n",
      "step 200 , total_loss: 1.0701, data_loss: 1.0701\n",
      "step 220 , total_loss: 1.0144, data_loss: 1.0144\n",
      "step 240 , total_loss: 1.0659, data_loss: 1.0659\n",
      "step 260 , total_loss: 0.9965, data_loss: 0.9965\n",
      "step 280 , total_loss: 1.0010, data_loss: 1.0010\n",
      "step 300 , total_loss: 0.9819, data_loss: 0.9819\n",
      "step 320 , total_loss: 1.0348, data_loss: 1.0348\n",
      "step 340 , total_loss: 1.0105, data_loss: 1.0105\n",
      "step 360 , total_loss: 1.0943, data_loss: 1.0943\n",
      "step 380 , total_loss: 0.9972, data_loss: 0.9972\n",
      "step 400 , total_loss: 1.0642, data_loss: 1.0642\n",
      "step 420 , total_loss: 1.1319, data_loss: 1.1319\n",
      "step 440 , total_loss: 1.0346, data_loss: 1.0346\n",
      "step 460 , total_loss: 0.9548, data_loss: 0.9548\n",
      "step 480 , total_loss: 1.0569, data_loss: 1.0569\n",
      "step 500 , total_loss: 1.0183, data_loss: 1.0183\n",
      "step 520 , total_loss: 1.0681, data_loss: 1.0681\n",
      "step 540 , total_loss: 0.9685, data_loss: 0.9685\n",
      "step 560 , total_loss: 1.0174, data_loss: 1.0174\n",
      "step 580 , total_loss: 1.0823, data_loss: 1.0823\n",
      "step 600 , total_loss: 1.0758, data_loss: 1.0758\n",
      "step 620 , total_loss: 1.0750, data_loss: 1.0750\n",
      "step 640 , total_loss: 1.0143, data_loss: 1.0143\n",
      "step 660 , total_loss: 1.0648, data_loss: 1.0648\n",
      "step 680 , total_loss: 0.9290, data_loss: 0.9290\n",
      "step 700 , total_loss: 0.9819, data_loss: 0.9819\n",
      "step 720 , total_loss: 0.9863, data_loss: 0.9863\n",
      "step 740 , total_loss: 1.0785, data_loss: 1.0785\n",
      "step 760 , total_loss: 0.9772, data_loss: 0.9772\n",
      "step 780 , total_loss: 1.0637, data_loss: 1.0637\n",
      "step 800 , total_loss: 0.9659, data_loss: 0.9659\n",
      "step 820 , total_loss: 1.0416, data_loss: 1.0416\n",
      "step 840 , total_loss: 1.0481, data_loss: 1.0481\n",
      "step 860 , total_loss: 1.0599, data_loss: 1.0599\n",
      "step 880 , total_loss: 1.0196, data_loss: 1.0196\n",
      "step 900 , total_loss: 1.0199, data_loss: 1.0199\n",
      "step 920 , total_loss: 1.0526, data_loss: 1.0526\n",
      "step 940 , total_loss: 1.0129, data_loss: 1.0129\n",
      "step 960 , total_loss: 0.9558, data_loss: 0.9558\n",
      "step 980 , total_loss: 0.9658, data_loss: 0.9658\n",
      "step 1000 , total_loss: 1.0809, data_loss: 1.0809\n",
      "step 1020 , total_loss: 1.1119, data_loss: 1.1119\n",
      "step 1040 , total_loss: 1.0265, data_loss: 1.0265\n",
      "step 1060 , total_loss: 1.0519, data_loss: 1.0519\n",
      "step 1080 , total_loss: 1.1250, data_loss: 1.1250\n",
      "step 1100 , total_loss: 0.9702, data_loss: 0.9702\n",
      "step 1120 , total_loss: 0.9924, data_loss: 0.9924\n",
      "step 1140 , total_loss: 0.9920, data_loss: 0.9920\n",
      "step 1160 , total_loss: 0.9777, data_loss: 0.9777\n",
      "step 1180 , total_loss: 0.9797, data_loss: 0.9797\n",
      "step 1200 , total_loss: 1.0412, data_loss: 1.0412\n",
      "step 1220 , total_loss: 1.0218, data_loss: 1.0218\n",
      "step 1240 , total_loss: 1.0404, data_loss: 1.0404\n",
      "step 1260 , total_loss: 0.9898, data_loss: 0.9898\n",
      "step 1280 , total_loss: 0.9743, data_loss: 0.9743\n",
      "step 1300 , total_loss: 1.0820, data_loss: 1.0820\n",
      "step 1320 , total_loss: 0.9839, data_loss: 0.9839\n",
      "step 1340 , total_loss: 0.9921, data_loss: 0.9921\n",
      "step 1360 , total_loss: 0.9375, data_loss: 0.9375\n",
      "step 1380 , total_loss: 1.0464, data_loss: 1.0464\n",
      "step 1400 , total_loss: 1.0083, data_loss: 1.0083\n",
      "step 1420 , total_loss: 0.9777, data_loss: 0.9777\n",
      "step 1440 , total_loss: 0.9354, data_loss: 0.9354\n",
      "step 1460 , total_loss: 1.0013, data_loss: 1.0013\n",
      "step 1480 , total_loss: 0.9547, data_loss: 0.9547\n",
      "step 1500 , total_loss: 0.9895, data_loss: 0.9895\n",
      "step 1520 , total_loss: 0.9807, data_loss: 0.9807\n",
      "step 1540 , total_loss: 1.0015, data_loss: 1.0015\n",
      "step 1560 , total_loss: 1.1391, data_loss: 1.1391\n",
      "step 1580 , total_loss: 0.9977, data_loss: 0.9977\n",
      "step 1600 , total_loss: 1.0061, data_loss: 1.0061\n",
      "step 1620 , total_loss: 1.0212, data_loss: 1.0212\n",
      "step 1640 , total_loss: 1.0576, data_loss: 1.0576\n",
      "step 1660 , total_loss: 0.9480, data_loss: 0.9480\n",
      "step 1680 , total_loss: 0.9083, data_loss: 0.9083\n",
      "step 1700 , total_loss: 1.0594, data_loss: 1.0594\n",
      "step 1720 , total_loss: 1.0106, data_loss: 1.0106\n",
      "step 1740 , total_loss: 1.0008, data_loss: 1.0008\n",
      "step 1760 , total_loss: 0.9325, data_loss: 0.9325\n",
      "step 1780 , total_loss: 0.9661, data_loss: 0.9661\n",
      "step 1800 , total_loss: 0.9705, data_loss: 0.9705\n",
      "step 1820 , total_loss: 0.9887, data_loss: 0.9887\n",
      "step 1840 , total_loss: 0.9678, data_loss: 0.9678\n",
      "step 1860 , total_loss: 0.9545, data_loss: 0.9545\n",
      "step 1880 , total_loss: 0.9653, data_loss: 0.9653\n",
      "step 1900 , total_loss: 1.0143, data_loss: 1.0143\n",
      "step 1920 , total_loss: 0.9321, data_loss: 0.9321\n",
      "step 1940 , total_loss: 1.0649, data_loss: 1.0649\n",
      "step 1960 , total_loss: 1.0067, data_loss: 1.0067\n",
      "step 1980 , total_loss: 1.0026, data_loss: 1.0026\n",
      "step 2000 , total_loss: 0.9995, data_loss: 0.9995\n",
      "step 2020 , total_loss: 0.9431, data_loss: 0.9431\n",
      "step 2040 , total_loss: 0.9788, data_loss: 0.9788\n",
      "step 2060 , total_loss: 1.0874, data_loss: 1.0874\n",
      "step 2080 , total_loss: 0.9593, data_loss: 0.9593\n",
      "step 2100 , total_loss: 0.9719, data_loss: 0.9719\n",
      "step 2120 , total_loss: 1.0303, data_loss: 1.0303\n",
      "step 2140 , total_loss: 0.9548, data_loss: 0.9548\n",
      "step 2160 , total_loss: 1.0082, data_loss: 1.0082\n",
      "step 2180 , total_loss: 1.0187, data_loss: 1.0187\n",
      "step 2200 , total_loss: 1.0210, data_loss: 1.0210\n",
      "step 2220 , total_loss: 0.9937, data_loss: 0.9937\n",
      "step 2240 , total_loss: 0.9720, data_loss: 0.9720\n",
      "step 2260 , total_loss: 0.9724, data_loss: 0.9724\n",
      "step 2280 , total_loss: 0.9369, data_loss: 0.9369\n",
      "step 2300 , total_loss: 1.0388, data_loss: 1.0388\n",
      "step 2320 , total_loss: 0.9630, data_loss: 0.9630\n",
      "step 2340 , total_loss: 0.9889, data_loss: 0.9889\n",
      "step 2360 , total_loss: 1.0713, data_loss: 1.0713\n",
      "step 2380 , total_loss: 0.9038, data_loss: 0.9038\n",
      "step 2400 , total_loss: 0.9948, data_loss: 0.9948\n",
      "step 2420 , total_loss: 0.9274, data_loss: 0.9274\n",
      "step 2440 , total_loss: 1.0555, data_loss: 1.0555\n",
      "step 2460 , total_loss: 1.0596, data_loss: 1.0596\n",
      "step 2480 , total_loss: 0.9223, data_loss: 0.9223\n",
      "step 2500 , total_loss: 1.0023, data_loss: 1.0023\n",
      "step 2520 , total_loss: 0.9957, data_loss: 0.9957\n",
      "step 2540 , total_loss: 1.0258, data_loss: 1.0258\n",
      "step 2560 , total_loss: 0.9040, data_loss: 0.9040\n",
      "step 2580 , total_loss: 1.0459, data_loss: 1.0459\n",
      "step 2600 , total_loss: 0.9701, data_loss: 0.9701\n",
      "step 2620 , total_loss: 0.9752, data_loss: 0.9752\n",
      "step 2640 , total_loss: 0.9656, data_loss: 0.9656\n",
      "step 2660 , total_loss: 0.9962, data_loss: 0.9962\n",
      "step 2680 , total_loss: 0.9709, data_loss: 0.9709\n",
      "step 2700 , total_loss: 0.9084, data_loss: 0.9084\n",
      "step 2720 , total_loss: 1.0855, data_loss: 1.0855\n",
      "step 2740 , total_loss: 0.9645, data_loss: 0.9645\n",
      "step 2760 , total_loss: 0.9447, data_loss: 0.9447\n",
      "step 2780 , total_loss: 0.9419, data_loss: 0.9419\n",
      "step 2800 , total_loss: 1.0297, data_loss: 1.0297\n",
      "step 2820 , total_loss: 0.9389, data_loss: 0.9389\n",
      "step 2840 , total_loss: 1.0122, data_loss: 1.0122\n",
      "step 2860 , total_loss: 0.9478, data_loss: 0.9478\n",
      "step 2880 , total_loss: 0.9763, data_loss: 0.9763\n",
      "step 2900 , total_loss: 0.9487, data_loss: 0.9487\n",
      "step 2920 , total_loss: 1.0644, data_loss: 1.0644\n",
      "step 2940 , total_loss: 1.0298, data_loss: 1.0298\n",
      "step 2960 , total_loss: 0.9191, data_loss: 0.9191\n",
      "step 2980 , total_loss: 0.9188, data_loss: 0.9188\n",
      "step 3000 , total_loss: 1.0063, data_loss: 1.0063\n",
      "step 3020 , total_loss: 0.9745, data_loss: 0.9745\n",
      "step 3040 , total_loss: 0.9147, data_loss: 0.9147\n",
      "step 3060 , total_loss: 0.9942, data_loss: 0.9942\n",
      "step 3080 , total_loss: 0.9485, data_loss: 0.9485\n",
      "step 3100 , total_loss: 0.9457, data_loss: 0.9457\n",
      "step 3120 , total_loss: 0.9522, data_loss: 0.9522\n",
      "step 3140 , total_loss: 0.9166, data_loss: 0.9166\n",
      "step 3160 , total_loss: 0.9429, data_loss: 0.9429\n",
      "step 3180 , total_loss: 0.9922, data_loss: 0.9922\n",
      "step 3200 , total_loss: 1.0257, data_loss: 1.0257\n",
      "step 3220 , total_loss: 0.9702, data_loss: 0.9702\n",
      "step 3240 , total_loss: 0.9107, data_loss: 0.9107\n",
      "step 3260 , total_loss: 0.9895, data_loss: 0.9895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3280 , total_loss: 0.9243, data_loss: 0.9243\n",
      "step 3300 , total_loss: 1.0097, data_loss: 1.0097\n",
      "eval valid at epoch 2: auc:0.8207,logloss:0.7483,mean_mrr:0.7616,ndcg@2:0.7298,ndcg@4:0.807,ndcg@6:0.8214,group_auc:0.8185\n",
      "step 20 , total_loss: 0.9879, data_loss: 0.9879\n",
      "step 40 , total_loss: 0.9269, data_loss: 0.9269\n",
      "step 60 , total_loss: 0.9587, data_loss: 0.9587\n",
      "step 80 , total_loss: 0.9065, data_loss: 0.9065\n",
      "step 100 , total_loss: 0.9041, data_loss: 0.9041\n",
      "step 120 , total_loss: 0.8675, data_loss: 0.8675\n",
      "step 140 , total_loss: 0.9343, data_loss: 0.9343\n",
      "step 160 , total_loss: 0.9113, data_loss: 0.9113\n",
      "step 180 , total_loss: 0.9460, data_loss: 0.9460\n",
      "step 200 , total_loss: 0.9052, data_loss: 0.9052\n",
      "step 220 , total_loss: 0.9408, data_loss: 0.9408\n",
      "step 240 , total_loss: 0.9501, data_loss: 0.9501\n",
      "step 260 , total_loss: 0.8492, data_loss: 0.8492\n",
      "step 280 , total_loss: 0.8868, data_loss: 0.8868\n",
      "step 300 , total_loss: 0.9194, data_loss: 0.9194\n",
      "step 320 , total_loss: 0.8355, data_loss: 0.8355\n",
      "step 340 , total_loss: 0.8941, data_loss: 0.8941\n",
      "step 360 , total_loss: 0.9210, data_loss: 0.9210\n",
      "step 380 , total_loss: 0.9904, data_loss: 0.9904\n",
      "step 400 , total_loss: 0.9333, data_loss: 0.9333\n",
      "step 420 , total_loss: 0.9591, data_loss: 0.9591\n",
      "step 440 , total_loss: 0.9353, data_loss: 0.9353\n",
      "step 460 , total_loss: 0.9863, data_loss: 0.9863\n",
      "step 480 , total_loss: 0.8704, data_loss: 0.8704\n",
      "step 500 , total_loss: 0.9452, data_loss: 0.9452\n",
      "step 520 , total_loss: 0.8705, data_loss: 0.8705\n",
      "step 540 , total_loss: 0.9710, data_loss: 0.9710\n",
      "step 560 , total_loss: 0.8630, data_loss: 0.8630\n",
      "step 580 , total_loss: 0.9466, data_loss: 0.9466\n",
      "step 600 , total_loss: 0.9267, data_loss: 0.9267\n",
      "step 620 , total_loss: 0.9551, data_loss: 0.9551\n",
      "step 640 , total_loss: 0.9663, data_loss: 0.9663\n",
      "step 660 , total_loss: 0.9103, data_loss: 0.9103\n",
      "step 680 , total_loss: 0.9371, data_loss: 0.9371\n",
      "step 700 , total_loss: 0.9029, data_loss: 0.9029\n",
      "step 720 , total_loss: 0.8833, data_loss: 0.8833\n",
      "step 740 , total_loss: 0.9384, data_loss: 0.9384\n",
      "step 760 , total_loss: 0.8507, data_loss: 0.8507\n",
      "step 780 , total_loss: 0.8594, data_loss: 0.8594\n",
      "step 800 , total_loss: 0.9262, data_loss: 0.9262\n",
      "step 820 , total_loss: 0.9232, data_loss: 0.9232\n",
      "step 840 , total_loss: 0.9298, data_loss: 0.9298\n",
      "step 860 , total_loss: 0.8877, data_loss: 0.8877\n",
      "step 880 , total_loss: 0.9231, data_loss: 0.9231\n",
      "step 900 , total_loss: 0.8745, data_loss: 0.8745\n",
      "step 920 , total_loss: 0.8660, data_loss: 0.8660\n",
      "step 940 , total_loss: 0.9364, data_loss: 0.9364\n",
      "step 960 , total_loss: 0.9353, data_loss: 0.9353\n",
      "step 980 , total_loss: 0.9292, data_loss: 0.9292\n",
      "step 1000 , total_loss: 0.8702, data_loss: 0.8702\n",
      "step 1020 , total_loss: 0.8622, data_loss: 0.8622\n",
      "step 1040 , total_loss: 0.8718, data_loss: 0.8718\n",
      "step 1060 , total_loss: 0.8826, data_loss: 0.8826\n",
      "step 1080 , total_loss: 0.9346, data_loss: 0.9346\n",
      "step 1100 , total_loss: 0.8463, data_loss: 0.8463\n",
      "step 1120 , total_loss: 0.9276, data_loss: 0.9276\n",
      "step 1140 , total_loss: 0.9040, data_loss: 0.9040\n",
      "step 1160 , total_loss: 0.8807, data_loss: 0.8807\n",
      "step 1180 , total_loss: 0.9203, data_loss: 0.9203\n",
      "step 1200 , total_loss: 1.0459, data_loss: 1.0459\n",
      "step 1220 , total_loss: 0.8730, data_loss: 0.8730\n",
      "step 1240 , total_loss: 0.9356, data_loss: 0.9356\n",
      "step 1260 , total_loss: 0.8648, data_loss: 0.8648\n",
      "step 1280 , total_loss: 1.0120, data_loss: 1.0120\n",
      "step 1300 , total_loss: 0.9725, data_loss: 0.9725\n",
      "step 1320 , total_loss: 0.8873, data_loss: 0.8873\n",
      "step 1340 , total_loss: 0.9539, data_loss: 0.9539\n",
      "step 1360 , total_loss: 0.9551, data_loss: 0.9551\n",
      "step 1380 , total_loss: 0.8727, data_loss: 0.8727\n",
      "step 1400 , total_loss: 0.9733, data_loss: 0.9733\n",
      "step 1420 , total_loss: 0.9516, data_loss: 0.9516\n",
      "step 1440 , total_loss: 0.9291, data_loss: 0.9291\n",
      "step 1460 , total_loss: 0.9160, data_loss: 0.9160\n",
      "step 1480 , total_loss: 0.9612, data_loss: 0.9612\n",
      "step 1500 , total_loss: 0.9897, data_loss: 0.9897\n",
      "step 1520 , total_loss: 0.9603, data_loss: 0.9603\n",
      "step 1540 , total_loss: 0.9715, data_loss: 0.9715\n",
      "step 1560 , total_loss: 0.8359, data_loss: 0.8359\n",
      "step 1580 , total_loss: 0.9338, data_loss: 0.9338\n",
      "step 1600 , total_loss: 0.9447, data_loss: 0.9447\n",
      "step 1620 , total_loss: 0.9768, data_loss: 0.9768\n",
      "step 1640 , total_loss: 0.8721, data_loss: 0.8721\n",
      "step 1660 , total_loss: 0.9179, data_loss: 0.9179\n",
      "step 1680 , total_loss: 0.9328, data_loss: 0.9328\n",
      "step 1700 , total_loss: 0.9193, data_loss: 0.9193\n",
      "step 1720 , total_loss: 0.8859, data_loss: 0.8859\n",
      "step 1740 , total_loss: 0.9041, data_loss: 0.9041\n",
      "step 1760 , total_loss: 0.8835, data_loss: 0.8835\n",
      "step 1780 , total_loss: 0.8786, data_loss: 0.8786\n",
      "step 1800 , total_loss: 0.8790, data_loss: 0.8790\n",
      "step 1820 , total_loss: 0.9362, data_loss: 0.9362\n",
      "step 1840 , total_loss: 0.9192, data_loss: 0.9192\n",
      "step 1860 , total_loss: 0.9315, data_loss: 0.9315\n",
      "step 1880 , total_loss: 0.9025, data_loss: 0.9025\n",
      "step 1900 , total_loss: 0.9204, data_loss: 0.9204\n",
      "step 1920 , total_loss: 0.8282, data_loss: 0.8282\n",
      "step 1940 , total_loss: 0.9393, data_loss: 0.9393\n",
      "step 1960 , total_loss: 0.9897, data_loss: 0.9897\n",
      "step 1980 , total_loss: 0.8544, data_loss: 0.8544\n",
      "step 2000 , total_loss: 0.9405, data_loss: 0.9405\n",
      "step 2020 , total_loss: 0.8948, data_loss: 0.8948\n",
      "step 2040 , total_loss: 0.9873, data_loss: 0.9873\n",
      "step 2060 , total_loss: 0.8592, data_loss: 0.8592\n",
      "step 2080 , total_loss: 0.9632, data_loss: 0.9632\n",
      "step 2100 , total_loss: 0.9380, data_loss: 0.9380\n",
      "step 2120 , total_loss: 0.9144, data_loss: 0.9144\n",
      "step 2140 , total_loss: 0.9849, data_loss: 0.9849\n",
      "step 2160 , total_loss: 0.9125, data_loss: 0.9125\n",
      "step 2180 , total_loss: 0.9961, data_loss: 0.9961\n",
      "step 2200 , total_loss: 0.9594, data_loss: 0.9594\n",
      "step 2220 , total_loss: 0.8445, data_loss: 0.8445\n",
      "step 2240 , total_loss: 0.9850, data_loss: 0.9850\n",
      "step 2260 , total_loss: 0.9339, data_loss: 0.9339\n",
      "step 2280 , total_loss: 0.9290, data_loss: 0.9290\n",
      "step 2300 , total_loss: 0.9533, data_loss: 0.9533\n",
      "step 2320 , total_loss: 0.8492, data_loss: 0.8492\n",
      "step 2340 , total_loss: 0.8929, data_loss: 0.8929\n",
      "step 2360 , total_loss: 0.9545, data_loss: 0.9545\n",
      "step 2380 , total_loss: 0.8525, data_loss: 0.8525\n",
      "step 2400 , total_loss: 0.9334, data_loss: 0.9334\n",
      "step 2420 , total_loss: 0.9793, data_loss: 0.9793\n",
      "step 2440 , total_loss: 0.9283, data_loss: 0.9283\n",
      "step 2460 , total_loss: 0.9295, data_loss: 0.9295\n",
      "step 2480 , total_loss: 0.8751, data_loss: 0.8751\n",
      "step 2500 , total_loss: 0.9334, data_loss: 0.9334\n",
      "step 2520 , total_loss: 0.8948, data_loss: 0.8948\n",
      "step 2540 , total_loss: 0.9440, data_loss: 0.9440\n",
      "step 2560 , total_loss: 0.8749, data_loss: 0.8749\n",
      "step 2580 , total_loss: 0.8706, data_loss: 0.8706\n",
      "step 2600 , total_loss: 0.9263, data_loss: 0.9263\n",
      "step 2620 , total_loss: 0.9083, data_loss: 0.9083\n",
      "step 2640 , total_loss: 0.8837, data_loss: 0.8837\n",
      "step 2660 , total_loss: 0.9069, data_loss: 0.9069\n",
      "step 2680 , total_loss: 1.0512, data_loss: 1.0512\n",
      "step 2700 , total_loss: 0.8418, data_loss: 0.8418\n",
      "step 2720 , total_loss: 0.9164, data_loss: 0.9164\n",
      "step 2740 , total_loss: 0.8633, data_loss: 0.8633\n",
      "step 2760 , total_loss: 1.0341, data_loss: 1.0341\n",
      "step 2780 , total_loss: 0.8706, data_loss: 0.8706\n",
      "step 2800 , total_loss: 0.9101, data_loss: 0.9101\n",
      "step 2820 , total_loss: 0.9331, data_loss: 0.9331\n",
      "step 2840 , total_loss: 0.9179, data_loss: 0.9179\n",
      "step 2860 , total_loss: 0.9043, data_loss: 0.9043\n",
      "step 2880 , total_loss: 1.0482, data_loss: 1.0482\n",
      "step 2900 , total_loss: 0.8110, data_loss: 0.8110\n",
      "step 2920 , total_loss: 0.9150, data_loss: 0.9150\n",
      "step 2940 , total_loss: 0.9254, data_loss: 0.9254\n",
      "step 2960 , total_loss: 0.9496, data_loss: 0.9496\n",
      "step 2980 , total_loss: 0.9073, data_loss: 0.9073\n",
      "step 3000 , total_loss: 0.9582, data_loss: 0.9582\n",
      "step 3020 , total_loss: 0.9670, data_loss: 0.9670\n",
      "step 3040 , total_loss: 0.9194, data_loss: 0.9194\n",
      "step 3060 , total_loss: 0.8944, data_loss: 0.8944\n",
      "step 3080 , total_loss: 0.8724, data_loss: 0.8724\n",
      "step 3100 , total_loss: 0.9167, data_loss: 0.9167\n",
      "step 3120 , total_loss: 0.9230, data_loss: 0.9230\n",
      "step 3140 , total_loss: 0.9276, data_loss: 0.9276\n",
      "step 3160 , total_loss: 0.9782, data_loss: 0.9782\n",
      "step 3180 , total_loss: 0.9466, data_loss: 0.9466\n",
      "step 3200 , total_loss: 0.9330, data_loss: 0.9330\n",
      "step 3220 , total_loss: 0.9386, data_loss: 0.9386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3240 , total_loss: 0.9337, data_loss: 0.9337\n",
      "step 3260 , total_loss: 0.9383, data_loss: 0.9383\n",
      "step 3280 , total_loss: 1.0179, data_loss: 1.0179\n",
      "step 3300 , total_loss: 0.9379, data_loss: 0.9379\n",
      "eval valid at epoch 3: auc:0.8307,logloss:0.8383,mean_mrr:0.7732,ndcg@2:0.7445,ndcg@4:0.8173,ndcg@6:0.8301,group_auc:0.829\n",
      "step 20 , total_loss: 0.8084, data_loss: 0.8084\n",
      "step 40 , total_loss: 0.8672, data_loss: 0.8672\n",
      "step 60 , total_loss: 0.8894, data_loss: 0.8894\n",
      "step 80 , total_loss: 0.8788, data_loss: 0.8788\n",
      "step 100 , total_loss: 0.8729, data_loss: 0.8729\n",
      "step 120 , total_loss: 0.8556, data_loss: 0.8556\n",
      "step 140 , total_loss: 0.8022, data_loss: 0.8022\n",
      "step 160 , total_loss: 0.8857, data_loss: 0.8857\n",
      "step 180 , total_loss: 0.8365, data_loss: 0.8365\n",
      "step 200 , total_loss: 0.7918, data_loss: 0.7918\n",
      "step 220 , total_loss: 0.7948, data_loss: 0.7948\n",
      "step 240 , total_loss: 0.7439, data_loss: 0.7439\n",
      "step 260 , total_loss: 0.8582, data_loss: 0.8582\n",
      "step 280 , total_loss: 0.8830, data_loss: 0.8830\n",
      "step 300 , total_loss: 0.9068, data_loss: 0.9068\n",
      "step 320 , total_loss: 0.8995, data_loss: 0.8995\n",
      "step 340 , total_loss: 0.8443, data_loss: 0.8443\n",
      "step 360 , total_loss: 0.8422, data_loss: 0.8422\n",
      "step 380 , total_loss: 0.8184, data_loss: 0.8184\n",
      "step 400 , total_loss: 0.8979, data_loss: 0.8979\n",
      "step 420 , total_loss: 0.9243, data_loss: 0.9243\n",
      "step 440 , total_loss: 0.8393, data_loss: 0.8393\n",
      "step 460 , total_loss: 0.8540, data_loss: 0.8540\n",
      "step 480 , total_loss: 0.8000, data_loss: 0.8000\n",
      "step 500 , total_loss: 0.8530, data_loss: 0.8530\n",
      "step 520 , total_loss: 0.8071, data_loss: 0.8071\n",
      "step 540 , total_loss: 0.8484, data_loss: 0.8484\n",
      "step 560 , total_loss: 0.8605, data_loss: 0.8605\n",
      "step 580 , total_loss: 0.8812, data_loss: 0.8812\n",
      "step 600 , total_loss: 0.8462, data_loss: 0.8462\n",
      "step 620 , total_loss: 0.9023, data_loss: 0.9023\n",
      "step 640 , total_loss: 0.8405, data_loss: 0.8405\n",
      "step 660 , total_loss: 0.8228, data_loss: 0.8228\n",
      "step 680 , total_loss: 0.7875, data_loss: 0.7875\n",
      "step 700 , total_loss: 0.9674, data_loss: 0.9674\n",
      "step 720 , total_loss: 0.8382, data_loss: 0.8382\n",
      "step 740 , total_loss: 0.8283, data_loss: 0.8283\n",
      "step 760 , total_loss: 0.8790, data_loss: 0.8790\n",
      "step 780 , total_loss: 0.7537, data_loss: 0.7537\n",
      "step 800 , total_loss: 0.8815, data_loss: 0.8815\n",
      "step 820 , total_loss: 0.8613, data_loss: 0.8613\n",
      "step 840 , total_loss: 0.9066, data_loss: 0.9066\n",
      "step 860 , total_loss: 0.8250, data_loss: 0.8250\n",
      "step 880 , total_loss: 0.9053, data_loss: 0.9053\n",
      "step 900 , total_loss: 0.9532, data_loss: 0.9532\n",
      "step 920 , total_loss: 0.8042, data_loss: 0.8042\n",
      "step 940 , total_loss: 0.7674, data_loss: 0.7674\n",
      "step 960 , total_loss: 0.9160, data_loss: 0.9160\n",
      "step 980 , total_loss: 0.9572, data_loss: 0.9572\n",
      "step 1000 , total_loss: 0.8576, data_loss: 0.8576\n",
      "step 1020 , total_loss: 0.9360, data_loss: 0.9360\n",
      "step 1040 , total_loss: 0.8569, data_loss: 0.8569\n",
      "step 1060 , total_loss: 0.9043, data_loss: 0.9043\n",
      "step 1080 , total_loss: 0.8667, data_loss: 0.8667\n",
      "step 1100 , total_loss: 0.9044, data_loss: 0.9044\n",
      "step 1120 , total_loss: 0.8756, data_loss: 0.8756\n",
      "step 1140 , total_loss: 0.8670, data_loss: 0.8670\n",
      "step 1160 , total_loss: 0.8522, data_loss: 0.8522\n",
      "step 1180 , total_loss: 0.9755, data_loss: 0.9755\n",
      "step 1200 , total_loss: 0.7897, data_loss: 0.7897\n",
      "step 1220 , total_loss: 0.9302, data_loss: 0.9302\n",
      "step 1240 , total_loss: 0.8876, data_loss: 0.8876\n",
      "step 1260 , total_loss: 0.8508, data_loss: 0.8508\n",
      "step 1280 , total_loss: 0.8505, data_loss: 0.8505\n",
      "step 1300 , total_loss: 0.8953, data_loss: 0.8953\n",
      "step 1320 , total_loss: 0.8356, data_loss: 0.8356\n",
      "step 1340 , total_loss: 0.8850, data_loss: 0.8850\n",
      "step 1360 , total_loss: 0.8841, data_loss: 0.8841\n",
      "step 1380 , total_loss: 0.8880, data_loss: 0.8880\n",
      "step 1400 , total_loss: 0.9113, data_loss: 0.9113\n",
      "step 1420 , total_loss: 0.8601, data_loss: 0.8601\n",
      "step 1440 , total_loss: 0.8446, data_loss: 0.8446\n",
      "step 1460 , total_loss: 0.8276, data_loss: 0.8276\n",
      "step 1480 , total_loss: 0.8302, data_loss: 0.8302\n",
      "step 1500 , total_loss: 0.9283, data_loss: 0.9283\n",
      "step 1520 , total_loss: 0.8589, data_loss: 0.8589\n",
      "step 1540 , total_loss: 0.8535, data_loss: 0.8535\n",
      "step 1560 , total_loss: 0.8747, data_loss: 0.8747\n",
      "step 1580 , total_loss: 0.9251, data_loss: 0.9251\n",
      "step 1600 , total_loss: 0.8341, data_loss: 0.8341\n",
      "step 1620 , total_loss: 0.8663, data_loss: 0.8663\n",
      "step 1640 , total_loss: 0.9751, data_loss: 0.9751\n",
      "step 1660 , total_loss: 0.8622, data_loss: 0.8622\n",
      "step 1680 , total_loss: 0.8584, data_loss: 0.8584\n",
      "step 1700 , total_loss: 0.8677, data_loss: 0.8677\n",
      "step 1720 , total_loss: 0.9956, data_loss: 0.9956\n",
      "step 1740 , total_loss: 0.8933, data_loss: 0.8933\n",
      "step 1760 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 1780 , total_loss: 0.8847, data_loss: 0.8847\n",
      "step 1800 , total_loss: 0.7050, data_loss: 0.7050\n",
      "step 1820 , total_loss: 0.8545, data_loss: 0.8545\n",
      "step 1840 , total_loss: 0.8454, data_loss: 0.8454\n",
      "step 1860 , total_loss: 0.8735, data_loss: 0.8735\n",
      "step 1880 , total_loss: 0.9088, data_loss: 0.9088\n",
      "step 1900 , total_loss: 0.8820, data_loss: 0.8820\n",
      "step 1920 , total_loss: 0.8482, data_loss: 0.8482\n",
      "step 1940 , total_loss: 0.9139, data_loss: 0.9139\n",
      "step 1960 , total_loss: 0.8534, data_loss: 0.8534\n",
      "step 1980 , total_loss: 0.8653, data_loss: 0.8653\n",
      "step 2000 , total_loss: 0.8913, data_loss: 0.8913\n",
      "step 2020 , total_loss: 0.9357, data_loss: 0.9357\n",
      "step 2040 , total_loss: 0.9729, data_loss: 0.9729\n",
      "step 2060 , total_loss: 0.8154, data_loss: 0.8154\n",
      "step 2080 , total_loss: 0.8939, data_loss: 0.8939\n",
      "step 2100 , total_loss: 0.8699, data_loss: 0.8699\n",
      "step 2120 , total_loss: 0.8980, data_loss: 0.8980\n",
      "step 2140 , total_loss: 0.8556, data_loss: 0.8556\n",
      "step 2160 , total_loss: 0.8601, data_loss: 0.8601\n",
      "step 2180 , total_loss: 0.9159, data_loss: 0.9159\n",
      "step 2200 , total_loss: 0.8795, data_loss: 0.8795\n",
      "step 2220 , total_loss: 0.7948, data_loss: 0.7948\n",
      "step 2240 , total_loss: 0.9850, data_loss: 0.9850\n",
      "step 2260 , total_loss: 0.8772, data_loss: 0.8772\n",
      "step 2280 , total_loss: 0.9128, data_loss: 0.9128\n",
      "step 2300 , total_loss: 0.8975, data_loss: 0.8975\n",
      "step 2320 , total_loss: 0.7594, data_loss: 0.7594\n",
      "step 2340 , total_loss: 0.9543, data_loss: 0.9543\n",
      "step 2360 , total_loss: 0.9044, data_loss: 0.9044\n",
      "step 2380 , total_loss: 0.8930, data_loss: 0.8930\n",
      "step 2400 , total_loss: 0.9406, data_loss: 0.9406\n",
      "step 2420 , total_loss: 0.9250, data_loss: 0.9250\n",
      "step 2440 , total_loss: 0.8192, data_loss: 0.8192\n",
      "step 2460 , total_loss: 0.8960, data_loss: 0.8960\n",
      "step 2480 , total_loss: 0.9346, data_loss: 0.9346\n",
      "step 2500 , total_loss: 0.7879, data_loss: 0.7879\n",
      "step 2520 , total_loss: 0.8642, data_loss: 0.8642\n",
      "step 2540 , total_loss: 0.8175, data_loss: 0.8175\n",
      "step 2560 , total_loss: 0.9766, data_loss: 0.9766\n",
      "step 2580 , total_loss: 0.8573, data_loss: 0.8573\n",
      "step 2600 , total_loss: 0.8778, data_loss: 0.8778\n",
      "step 2620 , total_loss: 0.9152, data_loss: 0.9152\n",
      "step 2640 , total_loss: 0.8812, data_loss: 0.8812\n",
      "step 2660 , total_loss: 0.8137, data_loss: 0.8137\n",
      "step 2680 , total_loss: 0.8537, data_loss: 0.8537\n",
      "step 2700 , total_loss: 0.9189, data_loss: 0.9189\n",
      "step 2720 , total_loss: 0.9049, data_loss: 0.9049\n",
      "step 2740 , total_loss: 0.9652, data_loss: 0.9652\n",
      "step 2760 , total_loss: 0.8805, data_loss: 0.8805\n",
      "step 2780 , total_loss: 0.8366, data_loss: 0.8366\n",
      "step 2800 , total_loss: 0.8799, data_loss: 0.8799\n",
      "step 2820 , total_loss: 0.9261, data_loss: 0.9261\n",
      "step 2840 , total_loss: 0.9581, data_loss: 0.9581\n",
      "step 2860 , total_loss: 0.8782, data_loss: 0.8782\n",
      "step 2880 , total_loss: 0.9143, data_loss: 0.9143\n",
      "step 2900 , total_loss: 0.8221, data_loss: 0.8221\n",
      "step 2920 , total_loss: 0.9110, data_loss: 0.9110\n",
      "step 2940 , total_loss: 0.8317, data_loss: 0.8317\n",
      "step 2960 , total_loss: 0.8246, data_loss: 0.8246\n",
      "step 2980 , total_loss: 0.9345, data_loss: 0.9345\n",
      "step 3000 , total_loss: 0.9012, data_loss: 0.9012\n",
      "step 3020 , total_loss: 0.8342, data_loss: 0.8342\n",
      "step 3040 , total_loss: 0.8482, data_loss: 0.8482\n",
      "step 3060 , total_loss: 0.8297, data_loss: 0.8297\n",
      "step 3080 , total_loss: 0.8920, data_loss: 0.8920\n",
      "step 3100 , total_loss: 0.8686, data_loss: 0.8686\n",
      "step 3120 , total_loss: 0.8719, data_loss: 0.8719\n",
      "step 3140 , total_loss: 0.8881, data_loss: 0.8881\n",
      "step 3160 , total_loss: 0.8295, data_loss: 0.8295\n",
      "step 3180 , total_loss: 0.8241, data_loss: 0.8241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200 , total_loss: 0.9042, data_loss: 0.9042\n",
      "step 3220 , total_loss: 0.8444, data_loss: 0.8444\n",
      "step 3240 , total_loss: 0.7959, data_loss: 0.7959\n",
      "step 3260 , total_loss: 0.8967, data_loss: 0.8967\n",
      "step 3280 , total_loss: 0.8532, data_loss: 0.8532\n",
      "step 3300 , total_loss: 0.8374, data_loss: 0.8374\n",
      "eval valid at epoch 4: auc:0.8365,logloss:0.9544,mean_mrr:0.781,ndcg@2:0.754,ndcg@4:0.8239,ndcg@6:0.836,group_auc:0.8357\n",
      "step 20 , total_loss: 0.7827, data_loss: 0.7827\n",
      "step 40 , total_loss: 0.8881, data_loss: 0.8881\n",
      "step 60 , total_loss: 0.8327, data_loss: 0.8327\n",
      "step 80 , total_loss: 0.7891, data_loss: 0.7891\n",
      "step 100 , total_loss: 0.9026, data_loss: 0.9026\n",
      "step 120 , total_loss: 0.7891, data_loss: 0.7891\n",
      "step 140 , total_loss: 0.7610, data_loss: 0.7610\n",
      "step 160 , total_loss: 0.8584, data_loss: 0.8584\n",
      "step 180 , total_loss: 0.7657, data_loss: 0.7657\n",
      "step 200 , total_loss: 0.8394, data_loss: 0.8394\n",
      "step 220 , total_loss: 0.8333, data_loss: 0.8333\n",
      "step 240 , total_loss: 0.8084, data_loss: 0.8084\n",
      "step 260 , total_loss: 0.8469, data_loss: 0.8469\n",
      "step 280 , total_loss: 0.8219, data_loss: 0.8219\n",
      "step 300 , total_loss: 0.8247, data_loss: 0.8247\n",
      "step 320 , total_loss: 0.7860, data_loss: 0.7860\n",
      "step 340 , total_loss: 0.7790, data_loss: 0.7790\n",
      "step 360 , total_loss: 0.7581, data_loss: 0.7581\n",
      "step 380 , total_loss: 0.7873, data_loss: 0.7873\n",
      "step 400 , total_loss: 0.7570, data_loss: 0.7570\n",
      "step 420 , total_loss: 0.8633, data_loss: 0.8633\n",
      "step 440 , total_loss: 0.8133, data_loss: 0.8133\n",
      "step 460 , total_loss: 0.8306, data_loss: 0.8306\n",
      "step 480 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 500 , total_loss: 0.7596, data_loss: 0.7596\n",
      "step 520 , total_loss: 0.8037, data_loss: 0.8037\n",
      "step 540 , total_loss: 0.8221, data_loss: 0.8221\n",
      "step 560 , total_loss: 0.7906, data_loss: 0.7906\n",
      "step 580 , total_loss: 0.8009, data_loss: 0.8009\n",
      "step 600 , total_loss: 0.8698, data_loss: 0.8698\n",
      "step 620 , total_loss: 0.7572, data_loss: 0.7572\n",
      "step 640 , total_loss: 0.8334, data_loss: 0.8334\n",
      "step 660 , total_loss: 0.7592, data_loss: 0.7592\n",
      "step 680 , total_loss: 0.8796, data_loss: 0.8796\n",
      "step 700 , total_loss: 0.8908, data_loss: 0.8908\n",
      "step 720 , total_loss: 0.8911, data_loss: 0.8911\n",
      "step 740 , total_loss: 0.9150, data_loss: 0.9150\n",
      "step 760 , total_loss: 0.7542, data_loss: 0.7542\n",
      "step 780 , total_loss: 0.7980, data_loss: 0.7980\n",
      "step 800 , total_loss: 0.9259, data_loss: 0.9259\n",
      "step 820 , total_loss: 0.8494, data_loss: 0.8494\n",
      "step 840 , total_loss: 0.8061, data_loss: 0.8061\n",
      "step 860 , total_loss: 0.9085, data_loss: 0.9085\n",
      "step 880 , total_loss: 0.8253, data_loss: 0.8253\n",
      "step 900 , total_loss: 0.8763, data_loss: 0.8763\n",
      "step 920 , total_loss: 0.7879, data_loss: 0.7879\n",
      "step 940 , total_loss: 0.7752, data_loss: 0.7752\n",
      "step 960 , total_loss: 0.7120, data_loss: 0.7120\n",
      "step 980 , total_loss: 0.8163, data_loss: 0.8163\n",
      "step 1000 , total_loss: 0.8496, data_loss: 0.8496\n",
      "step 1020 , total_loss: 0.7589, data_loss: 0.7589\n",
      "step 1040 , total_loss: 0.8427, data_loss: 0.8427\n",
      "step 1060 , total_loss: 0.7975, data_loss: 0.7975\n",
      "step 1080 , total_loss: 0.7850, data_loss: 0.7850\n",
      "step 1100 , total_loss: 0.8483, data_loss: 0.8483\n",
      "step 1120 , total_loss: 0.7340, data_loss: 0.7340\n",
      "step 1140 , total_loss: 0.9013, data_loss: 0.9013\n",
      "step 1160 , total_loss: 0.8817, data_loss: 0.8817\n",
      "step 1180 , total_loss: 0.8326, data_loss: 0.8326\n",
      "step 1200 , total_loss: 0.7957, data_loss: 0.7957\n",
      "step 1220 , total_loss: 0.7556, data_loss: 0.7556\n",
      "step 1240 , total_loss: 0.8764, data_loss: 0.8764\n",
      "step 1260 , total_loss: 0.7516, data_loss: 0.7516\n",
      "step 1280 , total_loss: 0.8302, data_loss: 0.8302\n",
      "step 1300 , total_loss: 0.7579, data_loss: 0.7579\n",
      "step 1320 , total_loss: 0.8248, data_loss: 0.8248\n",
      "step 1340 , total_loss: 0.7671, data_loss: 0.7671\n",
      "step 1360 , total_loss: 0.8130, data_loss: 0.8130\n",
      "step 1380 , total_loss: 0.7887, data_loss: 0.7887\n",
      "step 1400 , total_loss: 0.7321, data_loss: 0.7321\n",
      "step 1420 , total_loss: 0.8227, data_loss: 0.8227\n",
      "step 1440 , total_loss: 0.7693, data_loss: 0.7693\n",
      "step 1460 , total_loss: 0.8275, data_loss: 0.8275\n",
      "step 1480 , total_loss: 0.8364, data_loss: 0.8364\n",
      "step 1500 , total_loss: 0.7733, data_loss: 0.7733\n",
      "step 1520 , total_loss: 0.8377, data_loss: 0.8377\n",
      "step 1540 , total_loss: 0.7721, data_loss: 0.7721\n",
      "step 1560 , total_loss: 0.7637, data_loss: 0.7637\n",
      "step 1580 , total_loss: 0.8557, data_loss: 0.8557\n",
      "step 1600 , total_loss: 0.8047, data_loss: 0.8047\n",
      "step 1620 , total_loss: 0.9531, data_loss: 0.9531\n",
      "step 1640 , total_loss: 0.8319, data_loss: 0.8319\n",
      "step 1660 , total_loss: 0.7638, data_loss: 0.7638\n",
      "step 1680 , total_loss: 0.8538, data_loss: 0.8538\n",
      "step 1700 , total_loss: 0.8263, data_loss: 0.8263\n",
      "step 1720 , total_loss: 0.7998, data_loss: 0.7998\n",
      "step 1740 , total_loss: 0.9231, data_loss: 0.9231\n",
      "step 1760 , total_loss: 0.9107, data_loss: 0.9107\n",
      "step 1780 , total_loss: 0.7907, data_loss: 0.7907\n",
      "step 1800 , total_loss: 0.7363, data_loss: 0.7363\n",
      "step 1820 , total_loss: 0.9185, data_loss: 0.9185\n",
      "step 1840 , total_loss: 0.7938, data_loss: 0.7938\n",
      "step 1860 , total_loss: 0.8164, data_loss: 0.8164\n",
      "step 1880 , total_loss: 0.8813, data_loss: 0.8813\n",
      "step 1900 , total_loss: 0.9108, data_loss: 0.9108\n",
      "step 1920 , total_loss: 0.7590, data_loss: 0.7590\n",
      "step 1940 , total_loss: 0.7876, data_loss: 0.7876\n",
      "step 1960 , total_loss: 0.7435, data_loss: 0.7435\n",
      "step 1980 , total_loss: 0.7654, data_loss: 0.7654\n",
      "step 2000 , total_loss: 0.8081, data_loss: 0.8081\n",
      "step 2020 , total_loss: 0.8732, data_loss: 0.8732\n",
      "step 2040 , total_loss: 0.8342, data_loss: 0.8342\n",
      "step 2060 , total_loss: 0.7865, data_loss: 0.7865\n",
      "step 2080 , total_loss: 0.8157, data_loss: 0.8157\n",
      "step 2100 , total_loss: 0.8437, data_loss: 0.8437\n",
      "step 2120 , total_loss: 0.8693, data_loss: 0.8693\n",
      "step 2140 , total_loss: 0.7960, data_loss: 0.7960\n",
      "step 2160 , total_loss: 0.8944, data_loss: 0.8944\n",
      "step 2180 , total_loss: 0.8065, data_loss: 0.8065\n",
      "step 2200 , total_loss: 0.8963, data_loss: 0.8963\n",
      "step 2220 , total_loss: 0.8153, data_loss: 0.8153\n",
      "step 2240 , total_loss: 0.8187, data_loss: 0.8187\n",
      "step 2260 , total_loss: 0.8851, data_loss: 0.8851\n",
      "step 2280 , total_loss: 0.8471, data_loss: 0.8471\n",
      "step 2300 , total_loss: 0.9028, data_loss: 0.9028\n",
      "step 2320 , total_loss: 0.7455, data_loss: 0.7455\n",
      "step 2340 , total_loss: 0.8827, data_loss: 0.8827\n",
      "step 2360 , total_loss: 0.7958, data_loss: 0.7958\n",
      "step 2380 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 2400 , total_loss: 0.9047, data_loss: 0.9047\n",
      "step 2420 , total_loss: 0.8009, data_loss: 0.8009\n",
      "step 2440 , total_loss: 0.7898, data_loss: 0.7898\n",
      "step 2460 , total_loss: 0.8012, data_loss: 0.8012\n",
      "step 2480 , total_loss: 0.8708, data_loss: 0.8708\n",
      "step 2500 , total_loss: 0.8570, data_loss: 0.8570\n",
      "step 2520 , total_loss: 0.8908, data_loss: 0.8908\n",
      "step 2540 , total_loss: 0.8468, data_loss: 0.8468\n",
      "step 2560 , total_loss: 0.8083, data_loss: 0.8083\n",
      "step 2580 , total_loss: 0.7786, data_loss: 0.7786\n",
      "step 2600 , total_loss: 0.8252, data_loss: 0.8252\n",
      "step 2620 , total_loss: 0.8729, data_loss: 0.8729\n",
      "step 2640 , total_loss: 0.7511, data_loss: 0.7511\n",
      "step 2660 , total_loss: 0.8581, data_loss: 0.8581\n",
      "step 2680 , total_loss: 0.7597, data_loss: 0.7597\n",
      "step 2700 , total_loss: 0.7970, data_loss: 0.7970\n",
      "step 2720 , total_loss: 0.8020, data_loss: 0.8020\n",
      "step 2740 , total_loss: 0.8553, data_loss: 0.8553\n",
      "step 2760 , total_loss: 0.8742, data_loss: 0.8742\n",
      "step 2780 , total_loss: 0.8587, data_loss: 0.8587\n",
      "step 2800 , total_loss: 0.8386, data_loss: 0.8386\n",
      "step 2820 , total_loss: 0.9036, data_loss: 0.9036\n",
      "step 2840 , total_loss: 0.9075, data_loss: 0.9075\n",
      "step 2860 , total_loss: 0.8487, data_loss: 0.8487\n",
      "step 2880 , total_loss: 0.8564, data_loss: 0.8564\n",
      "step 2900 , total_loss: 0.8533, data_loss: 0.8533\n",
      "step 2920 , total_loss: 0.7495, data_loss: 0.7495\n",
      "step 2940 , total_loss: 0.9420, data_loss: 0.9420\n",
      "step 2960 , total_loss: 0.7896, data_loss: 0.7896\n",
      "step 2980 , total_loss: 0.8910, data_loss: 0.8910\n",
      "step 3000 , total_loss: 0.8874, data_loss: 0.8874\n",
      "step 3020 , total_loss: 0.9742, data_loss: 0.9742\n",
      "step 3040 , total_loss: 0.8459, data_loss: 0.8459\n",
      "step 3060 , total_loss: 0.8287, data_loss: 0.8287\n",
      "step 3080 , total_loss: 0.8432, data_loss: 0.8432\n",
      "step 3100 , total_loss: 0.7760, data_loss: 0.7760\n",
      "step 3120 , total_loss: 0.7540, data_loss: 0.7540\n",
      "step 3140 , total_loss: 0.8663, data_loss: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3160 , total_loss: 0.8248, data_loss: 0.8248\n",
      "step 3180 , total_loss: 0.9180, data_loss: 0.9180\n",
      "step 3200 , total_loss: 0.8468, data_loss: 0.8468\n",
      "step 3220 , total_loss: 0.8541, data_loss: 0.8541\n",
      "step 3240 , total_loss: 0.8176, data_loss: 0.8176\n",
      "step 3260 , total_loss: 0.7283, data_loss: 0.7283\n",
      "step 3280 , total_loss: 0.9002, data_loss: 0.9002\n",
      "step 3300 , total_loss: 0.8975, data_loss: 0.8975\n",
      "eval valid at epoch 5: auc:0.8404,logloss:1.0122,mean_mrr:0.7849,ndcg@2:0.7582,ndcg@4:0.8272,ndcg@6:0.839,group_auc:0.8389\n",
      "step 20 , total_loss: 0.8164, data_loss: 0.8164\n",
      "step 40 , total_loss: 0.7103, data_loss: 0.7103\n",
      "step 60 , total_loss: 0.8200, data_loss: 0.8200\n",
      "step 80 , total_loss: 0.7990, data_loss: 0.7990\n",
      "step 100 , total_loss: 0.8179, data_loss: 0.8179\n",
      "step 120 , total_loss: 0.7839, data_loss: 0.7839\n",
      "step 140 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 160 , total_loss: 0.7524, data_loss: 0.7524\n",
      "step 180 , total_loss: 0.8643, data_loss: 0.8643\n",
      "step 200 , total_loss: 0.7769, data_loss: 0.7769\n",
      "step 220 , total_loss: 0.7860, data_loss: 0.7860\n",
      "step 240 , total_loss: 0.8333, data_loss: 0.8333\n",
      "step 260 , total_loss: 0.8568, data_loss: 0.8568\n",
      "step 280 , total_loss: 0.6951, data_loss: 0.6951\n",
      "step 300 , total_loss: 0.8787, data_loss: 0.8787\n",
      "step 320 , total_loss: 0.8111, data_loss: 0.8111\n",
      "step 340 , total_loss: 0.7668, data_loss: 0.7668\n",
      "step 360 , total_loss: 0.7147, data_loss: 0.7147\n",
      "step 380 , total_loss: 0.7149, data_loss: 0.7149\n",
      "step 400 , total_loss: 0.7887, data_loss: 0.7887\n",
      "step 420 , total_loss: 0.6866, data_loss: 0.6866\n",
      "step 440 , total_loss: 0.7874, data_loss: 0.7874\n",
      "step 460 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 480 , total_loss: 0.6753, data_loss: 0.6753\n",
      "step 500 , total_loss: 0.7843, data_loss: 0.7843\n",
      "step 520 , total_loss: 0.7593, data_loss: 0.7593\n",
      "step 540 , total_loss: 0.7870, data_loss: 0.7870\n",
      "step 560 , total_loss: 0.7737, data_loss: 0.7737\n",
      "step 580 , total_loss: 0.7170, data_loss: 0.7170\n",
      "step 600 , total_loss: 0.7607, data_loss: 0.7607\n",
      "step 620 , total_loss: 0.7715, data_loss: 0.7715\n",
      "step 640 , total_loss: 0.7850, data_loss: 0.7850\n",
      "step 660 , total_loss: 0.7769, data_loss: 0.7769\n",
      "step 680 , total_loss: 0.7985, data_loss: 0.7985\n",
      "step 700 , total_loss: 0.8016, data_loss: 0.8016\n",
      "step 720 , total_loss: 0.8248, data_loss: 0.8248\n",
      "step 740 , total_loss: 0.7745, data_loss: 0.7745\n",
      "step 760 , total_loss: 0.6820, data_loss: 0.6820\n",
      "step 780 , total_loss: 0.8081, data_loss: 0.8081\n",
      "step 800 , total_loss: 0.8399, data_loss: 0.8399\n",
      "step 820 , total_loss: 0.8104, data_loss: 0.8104\n",
      "step 840 , total_loss: 0.8290, data_loss: 0.8290\n",
      "step 860 , total_loss: 0.7551, data_loss: 0.7551\n",
      "step 880 , total_loss: 0.7497, data_loss: 0.7497\n",
      "step 900 , total_loss: 0.7370, data_loss: 0.7370\n",
      "step 920 , total_loss: 0.8118, data_loss: 0.8118\n",
      "step 940 , total_loss: 0.7574, data_loss: 0.7574\n",
      "step 960 , total_loss: 0.7816, data_loss: 0.7816\n",
      "step 980 , total_loss: 0.8886, data_loss: 0.8886\n",
      "step 1000 , total_loss: 0.8856, data_loss: 0.8856\n",
      "step 1020 , total_loss: 0.8114, data_loss: 0.8114\n",
      "step 1040 , total_loss: 0.8216, data_loss: 0.8216\n",
      "step 1060 , total_loss: 0.7763, data_loss: 0.7763\n",
      "step 1080 , total_loss: 0.7894, data_loss: 0.7894\n",
      "step 1100 , total_loss: 0.7331, data_loss: 0.7331\n",
      "step 1120 , total_loss: 0.7726, data_loss: 0.7726\n",
      "step 1140 , total_loss: 0.7974, data_loss: 0.7974\n",
      "step 1160 , total_loss: 0.7576, data_loss: 0.7576\n",
      "step 1180 , total_loss: 0.7830, data_loss: 0.7830\n",
      "step 1200 , total_loss: 0.7817, data_loss: 0.7817\n",
      "step 1220 , total_loss: 0.8314, data_loss: 0.8314\n",
      "step 1240 , total_loss: 0.7475, data_loss: 0.7475\n",
      "step 1260 , total_loss: 0.8087, data_loss: 0.8087\n",
      "step 1280 , total_loss: 0.7189, data_loss: 0.7189\n",
      "step 1300 , total_loss: 0.7692, data_loss: 0.7692\n",
      "step 1320 , total_loss: 0.7775, data_loss: 0.7775\n",
      "step 1340 , total_loss: 0.8009, data_loss: 0.8009\n",
      "step 1360 , total_loss: 0.7520, data_loss: 0.7520\n",
      "step 1380 , total_loss: 0.8454, data_loss: 0.8454\n",
      "step 1400 , total_loss: 0.8094, data_loss: 0.8094\n",
      "step 1420 , total_loss: 0.8203, data_loss: 0.8203\n",
      "step 1440 , total_loss: 0.8180, data_loss: 0.8180\n",
      "step 1460 , total_loss: 0.8069, data_loss: 0.8069\n",
      "step 1480 , total_loss: 0.7464, data_loss: 0.7464\n",
      "step 1500 , total_loss: 0.8142, data_loss: 0.8142\n",
      "step 1520 , total_loss: 0.7604, data_loss: 0.7604\n",
      "step 1540 , total_loss: 0.8376, data_loss: 0.8376\n",
      "step 1560 , total_loss: 0.7732, data_loss: 0.7732\n",
      "step 1580 , total_loss: 0.9042, data_loss: 0.9042\n",
      "step 1600 , total_loss: 0.8529, data_loss: 0.8529\n",
      "step 1620 , total_loss: 0.8358, data_loss: 0.8358\n",
      "step 1640 , total_loss: 0.7885, data_loss: 0.7885\n",
      "step 1660 , total_loss: 0.8051, data_loss: 0.8051\n",
      "step 1680 , total_loss: 0.8623, data_loss: 0.8623\n",
      "step 1700 , total_loss: 0.7978, data_loss: 0.7978\n",
      "step 1720 , total_loss: 0.8385, data_loss: 0.8385\n",
      "step 1740 , total_loss: 0.7769, data_loss: 0.7769\n",
      "step 1760 , total_loss: 0.7720, data_loss: 0.7720\n",
      "step 1780 , total_loss: 0.8083, data_loss: 0.8083\n",
      "step 1800 , total_loss: 0.8116, data_loss: 0.8116\n",
      "step 1820 , total_loss: 0.8085, data_loss: 0.8085\n",
      "step 1840 , total_loss: 0.8366, data_loss: 0.8366\n",
      "step 1860 , total_loss: 0.7708, data_loss: 0.7708\n",
      "step 1880 , total_loss: 0.7679, data_loss: 0.7679\n",
      "step 1900 , total_loss: 0.7751, data_loss: 0.7751\n",
      "step 1920 , total_loss: 0.8155, data_loss: 0.8155\n",
      "step 1940 , total_loss: 0.8426, data_loss: 0.8426\n",
      "step 1960 , total_loss: 0.8914, data_loss: 0.8914\n",
      "step 1980 , total_loss: 0.7784, data_loss: 0.7784\n",
      "step 2000 , total_loss: 0.8150, data_loss: 0.8150\n",
      "step 2020 , total_loss: 0.7527, data_loss: 0.7527\n",
      "step 2040 , total_loss: 0.8197, data_loss: 0.8197\n",
      "step 2060 , total_loss: 0.8469, data_loss: 0.8469\n",
      "step 2080 , total_loss: 0.8118, data_loss: 0.8118\n",
      "step 2100 , total_loss: 0.8350, data_loss: 0.8350\n",
      "step 2120 , total_loss: 0.7912, data_loss: 0.7912\n",
      "step 2140 , total_loss: 0.7272, data_loss: 0.7272\n",
      "step 2160 , total_loss: 0.8496, data_loss: 0.8496\n",
      "step 2180 , total_loss: 0.8566, data_loss: 0.8566\n",
      "step 2200 , total_loss: 0.8327, data_loss: 0.8327\n",
      "step 2220 , total_loss: 0.7414, data_loss: 0.7414\n",
      "step 2240 , total_loss: 0.7839, data_loss: 0.7839\n",
      "step 2260 , total_loss: 0.7521, data_loss: 0.7521\n",
      "step 2280 , total_loss: 0.7551, data_loss: 0.7551\n",
      "step 2300 , total_loss: 0.8724, data_loss: 0.8724\n",
      "step 2320 , total_loss: 0.8218, data_loss: 0.8218\n",
      "step 2340 , total_loss: 0.9166, data_loss: 0.9166\n",
      "step 2360 , total_loss: 0.8343, data_loss: 0.8343\n",
      "step 2380 , total_loss: 0.8746, data_loss: 0.8746\n",
      "step 2400 , total_loss: 0.8235, data_loss: 0.8235\n",
      "step 2420 , total_loss: 0.7807, data_loss: 0.7807\n",
      "step 2440 , total_loss: 0.8495, data_loss: 0.8495\n",
      "step 2460 , total_loss: 0.7972, data_loss: 0.7972\n",
      "step 2480 , total_loss: 0.7424, data_loss: 0.7424\n",
      "step 2500 , total_loss: 0.7990, data_loss: 0.7990\n",
      "step 2520 , total_loss: 0.7651, data_loss: 0.7651\n",
      "step 2540 , total_loss: 0.8774, data_loss: 0.8774\n",
      "step 2560 , total_loss: 0.7328, data_loss: 0.7328\n",
      "step 2580 , total_loss: 0.8222, data_loss: 0.8222\n",
      "step 2600 , total_loss: 0.7543, data_loss: 0.7543\n",
      "step 2620 , total_loss: 0.8443, data_loss: 0.8443\n",
      "step 2640 , total_loss: 0.8681, data_loss: 0.8681\n",
      "step 2660 , total_loss: 0.8637, data_loss: 0.8637\n",
      "step 2680 , total_loss: 0.9573, data_loss: 0.9573\n",
      "step 2700 , total_loss: 0.7971, data_loss: 0.7971\n",
      "step 2720 , total_loss: 0.7939, data_loss: 0.7939\n",
      "step 2740 , total_loss: 0.8242, data_loss: 0.8242\n",
      "step 2760 , total_loss: 0.8506, data_loss: 0.8506\n",
      "step 2780 , total_loss: 0.8755, data_loss: 0.8755\n",
      "step 2800 , total_loss: 0.8193, data_loss: 0.8193\n",
      "step 2820 , total_loss: 0.8821, data_loss: 0.8821\n",
      "step 2840 , total_loss: 0.8133, data_loss: 0.8133\n",
      "step 2860 , total_loss: 0.7874, data_loss: 0.7874\n",
      "step 2880 , total_loss: 0.7577, data_loss: 0.7577\n",
      "step 2900 , total_loss: 0.7535, data_loss: 0.7535\n",
      "step 2920 , total_loss: 0.7994, data_loss: 0.7994\n",
      "step 2940 , total_loss: 0.7075, data_loss: 0.7075\n",
      "step 2960 , total_loss: 0.8461, data_loss: 0.8461\n",
      "step 2980 , total_loss: 0.8331, data_loss: 0.8331\n",
      "step 3000 , total_loss: 0.7911, data_loss: 0.7911\n",
      "step 3020 , total_loss: 0.8204, data_loss: 0.8204\n",
      "step 3040 , total_loss: 0.8529, data_loss: 0.8529\n",
      "step 3060 , total_loss: 0.7864, data_loss: 0.7864\n",
      "step 3080 , total_loss: 0.8302, data_loss: 0.8302\n",
      "step 3100 , total_loss: 0.8178, data_loss: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3120 , total_loss: 0.8093, data_loss: 0.8093\n",
      "step 3140 , total_loss: 0.7917, data_loss: 0.7917\n",
      "step 3160 , total_loss: 0.7843, data_loss: 0.7843\n",
      "step 3180 , total_loss: 0.9064, data_loss: 0.9064\n",
      "step 3200 , total_loss: 0.8207, data_loss: 0.8207\n",
      "step 3220 , total_loss: 0.8258, data_loss: 0.8258\n",
      "step 3240 , total_loss: 0.6979, data_loss: 0.6979\n",
      "step 3260 , total_loss: 0.7859, data_loss: 0.7859\n",
      "step 3280 , total_loss: 0.8908, data_loss: 0.8908\n",
      "step 3300 , total_loss: 0.7399, data_loss: 0.7399\n",
      "eval valid at epoch 6: auc:0.8408,logloss:1.1115,mean_mrr:0.7862,ndcg@2:0.7602,ndcg@4:0.8282,ndcg@6:0.8399,group_auc:0.8398\n",
      "step 20 , total_loss: 0.7533, data_loss: 0.7533\n",
      "step 40 , total_loss: 0.6925, data_loss: 0.6925\n",
      "step 60 , total_loss: 0.7580, data_loss: 0.7580\n",
      "step 80 , total_loss: 0.7481, data_loss: 0.7481\n",
      "step 100 , total_loss: 0.8131, data_loss: 0.8131\n",
      "step 120 , total_loss: 0.8613, data_loss: 0.8613\n",
      "step 140 , total_loss: 0.6903, data_loss: 0.6903\n",
      "step 160 , total_loss: 0.7156, data_loss: 0.7156\n",
      "step 180 , total_loss: 0.7252, data_loss: 0.7252\n",
      "step 200 , total_loss: 0.7962, data_loss: 0.7962\n",
      "step 220 , total_loss: 0.7349, data_loss: 0.7349\n",
      "step 240 , total_loss: 0.7801, data_loss: 0.7801\n",
      "step 260 , total_loss: 0.7210, data_loss: 0.7210\n",
      "step 280 , total_loss: 0.7723, data_loss: 0.7723\n",
      "step 300 , total_loss: 0.7766, data_loss: 0.7766\n",
      "step 320 , total_loss: 0.7214, data_loss: 0.7214\n",
      "step 340 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 360 , total_loss: 0.7723, data_loss: 0.7723\n",
      "step 380 , total_loss: 0.7655, data_loss: 0.7655\n",
      "step 400 , total_loss: 0.7224, data_loss: 0.7224\n",
      "step 420 , total_loss: 0.8160, data_loss: 0.8160\n",
      "step 440 , total_loss: 0.7758, data_loss: 0.7758\n",
      "step 460 , total_loss: 0.6866, data_loss: 0.6866\n",
      "step 480 , total_loss: 0.7305, data_loss: 0.7305\n",
      "step 500 , total_loss: 0.8798, data_loss: 0.8798\n",
      "step 520 , total_loss: 0.7561, data_loss: 0.7561\n",
      "step 540 , total_loss: 0.8363, data_loss: 0.8363\n",
      "step 560 , total_loss: 0.7323, data_loss: 0.7323\n",
      "step 580 , total_loss: 0.7427, data_loss: 0.7427\n",
      "step 600 , total_loss: 0.7139, data_loss: 0.7139\n",
      "step 620 , total_loss: 0.7607, data_loss: 0.7607\n",
      "step 640 , total_loss: 0.7993, data_loss: 0.7993\n",
      "step 660 , total_loss: 0.7854, data_loss: 0.7854\n",
      "step 680 , total_loss: 0.7857, data_loss: 0.7857\n",
      "step 700 , total_loss: 0.8434, data_loss: 0.8434\n",
      "step 720 , total_loss: 0.8209, data_loss: 0.8209\n",
      "step 740 , total_loss: 0.6854, data_loss: 0.6854\n",
      "step 760 , total_loss: 0.7658, data_loss: 0.7658\n",
      "step 780 , total_loss: 0.8151, data_loss: 0.8151\n",
      "step 800 , total_loss: 0.7442, data_loss: 0.7442\n",
      "step 820 , total_loss: 0.8378, data_loss: 0.8378\n",
      "step 840 , total_loss: 0.7614, data_loss: 0.7614\n",
      "step 860 , total_loss: 0.7410, data_loss: 0.7410\n",
      "step 880 , total_loss: 0.8114, data_loss: 0.8114\n",
      "step 900 , total_loss: 0.6931, data_loss: 0.6931\n",
      "step 920 , total_loss: 0.7859, data_loss: 0.7859\n",
      "step 940 , total_loss: 0.7920, data_loss: 0.7920\n",
      "step 960 , total_loss: 0.8168, data_loss: 0.8168\n",
      "step 980 , total_loss: 0.7359, data_loss: 0.7359\n",
      "step 1000 , total_loss: 0.8110, data_loss: 0.8110\n",
      "step 1020 , total_loss: 0.8855, data_loss: 0.8855\n",
      "step 1040 , total_loss: 0.8260, data_loss: 0.8260\n",
      "step 1060 , total_loss: 0.7641, data_loss: 0.7641\n",
      "step 1080 , total_loss: 0.7610, data_loss: 0.7610\n",
      "step 1100 , total_loss: 0.7161, data_loss: 0.7161\n",
      "step 1120 , total_loss: 0.8353, data_loss: 0.8353\n",
      "step 1140 , total_loss: 0.7225, data_loss: 0.7225\n",
      "step 1160 , total_loss: 0.6842, data_loss: 0.6842\n",
      "step 1180 , total_loss: 0.7325, data_loss: 0.7325\n",
      "step 1200 , total_loss: 0.8406, data_loss: 0.8406\n",
      "step 1220 , total_loss: 0.7315, data_loss: 0.7315\n",
      "step 1240 , total_loss: 0.7660, data_loss: 0.7660\n",
      "step 1260 , total_loss: 0.8500, data_loss: 0.8500\n",
      "step 1280 , total_loss: 0.7723, data_loss: 0.7723\n",
      "step 1300 , total_loss: 0.7661, data_loss: 0.7661\n",
      "step 1320 , total_loss: 0.7762, data_loss: 0.7762\n",
      "step 1340 , total_loss: 0.7594, data_loss: 0.7594\n",
      "step 1360 , total_loss: 0.8820, data_loss: 0.8820\n",
      "step 1380 , total_loss: 0.7759, data_loss: 0.7759\n",
      "step 1400 , total_loss: 0.7179, data_loss: 0.7179\n",
      "step 1420 , total_loss: 0.6576, data_loss: 0.6576\n",
      "step 1440 , total_loss: 0.8515, data_loss: 0.8515\n",
      "step 1460 , total_loss: 0.7222, data_loss: 0.7222\n",
      "step 1480 , total_loss: 0.7409, data_loss: 0.7409\n",
      "step 1500 , total_loss: 0.8155, data_loss: 0.8155\n",
      "step 1520 , total_loss: 0.6859, data_loss: 0.6859\n",
      "step 1540 , total_loss: 0.7633, data_loss: 0.7633\n",
      "step 1560 , total_loss: 0.8388, data_loss: 0.8388\n",
      "step 1580 , total_loss: 0.6941, data_loss: 0.6941\n",
      "step 1600 , total_loss: 0.7844, data_loss: 0.7844\n",
      "step 1620 , total_loss: 0.8511, data_loss: 0.8511\n",
      "step 1640 , total_loss: 0.8070, data_loss: 0.8070\n",
      "step 1660 , total_loss: 0.8057, data_loss: 0.8057\n",
      "step 1680 , total_loss: 0.8232, data_loss: 0.8232\n",
      "step 1700 , total_loss: 0.8719, data_loss: 0.8719\n",
      "step 1720 , total_loss: 0.8278, data_loss: 0.8278\n",
      "step 1740 , total_loss: 0.8127, data_loss: 0.8127\n",
      "step 1760 , total_loss: 0.8177, data_loss: 0.8177\n",
      "step 1780 , total_loss: 0.7420, data_loss: 0.7420\n",
      "step 1800 , total_loss: 0.7390, data_loss: 0.7390\n",
      "step 1820 , total_loss: 0.8245, data_loss: 0.8245\n",
      "step 1840 , total_loss: 0.7751, data_loss: 0.7751\n",
      "step 1860 , total_loss: 0.7984, data_loss: 0.7984\n",
      "step 1880 , total_loss: 0.7730, data_loss: 0.7730\n",
      "step 1900 , total_loss: 0.7332, data_loss: 0.7332\n",
      "step 1920 , total_loss: 0.8335, data_loss: 0.8335\n",
      "step 1940 , total_loss: 0.7947, data_loss: 0.7947\n",
      "step 1960 , total_loss: 0.7140, data_loss: 0.7140\n",
      "step 1980 , total_loss: 0.8423, data_loss: 0.8423\n",
      "step 2000 , total_loss: 0.7507, data_loss: 0.7507\n",
      "step 2020 , total_loss: 0.7952, data_loss: 0.7952\n",
      "step 2040 , total_loss: 0.8118, data_loss: 0.8118\n",
      "step 2060 , total_loss: 0.8285, data_loss: 0.8285\n",
      "step 2080 , total_loss: 0.7627, data_loss: 0.7627\n",
      "step 2100 , total_loss: 0.7945, data_loss: 0.7945\n",
      "step 2120 , total_loss: 0.7054, data_loss: 0.7054\n",
      "step 2140 , total_loss: 0.8281, data_loss: 0.8281\n",
      "step 2160 , total_loss: 0.7374, data_loss: 0.7374\n",
      "step 2180 , total_loss: 0.8390, data_loss: 0.8390\n",
      "step 2200 , total_loss: 0.8767, data_loss: 0.8767\n",
      "step 2220 , total_loss: 0.7869, data_loss: 0.7869\n",
      "step 2240 , total_loss: 0.8047, data_loss: 0.8047\n",
      "step 2260 , total_loss: 0.7936, data_loss: 0.7936\n",
      "step 2280 , total_loss: 0.7332, data_loss: 0.7332\n",
      "step 2300 , total_loss: 0.7711, data_loss: 0.7711\n",
      "step 2320 , total_loss: 0.6885, data_loss: 0.6885\n",
      "step 2340 , total_loss: 0.8014, data_loss: 0.8014\n",
      "step 2360 , total_loss: 0.7606, data_loss: 0.7606\n",
      "step 2380 , total_loss: 0.7064, data_loss: 0.7064\n",
      "step 2400 , total_loss: 0.6822, data_loss: 0.6822\n",
      "step 2420 , total_loss: 0.7190, data_loss: 0.7190\n",
      "step 2440 , total_loss: 0.7946, data_loss: 0.7946\n",
      "step 2460 , total_loss: 0.6706, data_loss: 0.6706\n",
      "step 2480 , total_loss: 0.8048, data_loss: 0.8048\n",
      "step 2500 , total_loss: 0.7163, data_loss: 0.7163\n",
      "step 2520 , total_loss: 0.7341, data_loss: 0.7341\n",
      "step 2540 , total_loss: 0.9189, data_loss: 0.9189\n",
      "step 2560 , total_loss: 0.7159, data_loss: 0.7159\n",
      "step 2580 , total_loss: 0.8081, data_loss: 0.8081\n",
      "step 2600 , total_loss: 0.7670, data_loss: 0.7670\n",
      "step 2620 , total_loss: 0.7541, data_loss: 0.7541\n",
      "step 2640 , total_loss: 0.7343, data_loss: 0.7343\n",
      "step 2660 , total_loss: 0.8472, data_loss: 0.8472\n",
      "step 2680 , total_loss: 0.7853, data_loss: 0.7853\n",
      "step 2700 , total_loss: 0.9320, data_loss: 0.9320\n",
      "step 2720 , total_loss: 0.8742, data_loss: 0.8742\n",
      "step 2740 , total_loss: 0.8623, data_loss: 0.8623\n",
      "step 2760 , total_loss: 0.7701, data_loss: 0.7701\n",
      "step 2780 , total_loss: 0.8339, data_loss: 0.8339\n",
      "step 2800 , total_loss: 0.8066, data_loss: 0.8066\n",
      "step 2820 , total_loss: 0.7874, data_loss: 0.7874\n",
      "step 2840 , total_loss: 0.7606, data_loss: 0.7606\n",
      "step 2860 , total_loss: 0.8495, data_loss: 0.8495\n",
      "step 2880 , total_loss: 0.8065, data_loss: 0.8065\n",
      "step 2900 , total_loss: 0.7071, data_loss: 0.7071\n",
      "step 2920 , total_loss: 0.7601, data_loss: 0.7601\n",
      "step 2940 , total_loss: 0.7614, data_loss: 0.7614\n",
      "step 2960 , total_loss: 0.7928, data_loss: 0.7928\n",
      "step 2980 , total_loss: 0.6968, data_loss: 0.6968\n",
      "step 3000 , total_loss: 0.8236, data_loss: 0.8236\n",
      "step 3020 , total_loss: 0.7481, data_loss: 0.7481\n",
      "step 3040 , total_loss: 0.8118, data_loss: 0.8118\n",
      "step 3060 , total_loss: 0.8006, data_loss: 0.8006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3080 , total_loss: 0.7875, data_loss: 0.7875\n",
      "step 3100 , total_loss: 0.8060, data_loss: 0.8060\n",
      "step 3120 , total_loss: 0.7813, data_loss: 0.7813\n",
      "step 3140 , total_loss: 0.7712, data_loss: 0.7712\n",
      "step 3160 , total_loss: 0.8539, data_loss: 0.8539\n",
      "step 3180 , total_loss: 0.7701, data_loss: 0.7701\n",
      "step 3200 , total_loss: 0.7948, data_loss: 0.7948\n",
      "step 3220 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 3240 , total_loss: 0.7728, data_loss: 0.7728\n",
      "step 3260 , total_loss: 0.7915, data_loss: 0.7915\n",
      "step 3280 , total_loss: 0.8011, data_loss: 0.8011\n",
      "step 3300 , total_loss: 0.7517, data_loss: 0.7517\n",
      "eval valid at epoch 7: auc:0.8426,logloss:1.1948,mean_mrr:0.7879,ndcg@2:0.7621,ndcg@4:0.8299,ndcg@6:0.8412,group_auc:0.8414\n",
      "step 20 , total_loss: 0.7236, data_loss: 0.7236\n",
      "step 40 , total_loss: 0.7164, data_loss: 0.7164\n",
      "step 60 , total_loss: 0.6860, data_loss: 0.6860\n",
      "step 80 , total_loss: 0.6937, data_loss: 0.6937\n",
      "step 100 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 120 , total_loss: 0.7088, data_loss: 0.7088\n",
      "step 140 , total_loss: 0.7401, data_loss: 0.7401\n",
      "step 160 , total_loss: 0.6818, data_loss: 0.6818\n",
      "step 180 , total_loss: 0.7727, data_loss: 0.7727\n",
      "step 200 , total_loss: 0.7013, data_loss: 0.7013\n",
      "step 220 , total_loss: 0.6421, data_loss: 0.6421\n",
      "step 240 , total_loss: 0.6963, data_loss: 0.6963\n",
      "step 260 , total_loss: 0.6894, data_loss: 0.6894\n",
      "step 280 , total_loss: 0.7253, data_loss: 0.7253\n",
      "step 300 , total_loss: 0.8065, data_loss: 0.8065\n",
      "step 320 , total_loss: 0.7761, data_loss: 0.7761\n",
      "step 340 , total_loss: 0.7440, data_loss: 0.7440\n",
      "step 360 , total_loss: 0.7050, data_loss: 0.7050\n",
      "step 380 , total_loss: 0.7184, data_loss: 0.7184\n",
      "step 400 , total_loss: 0.7149, data_loss: 0.7149\n",
      "step 420 , total_loss: 0.7928, data_loss: 0.7928\n",
      "step 440 , total_loss: 0.8214, data_loss: 0.8214\n",
      "step 460 , total_loss: 0.6567, data_loss: 0.6567\n",
      "step 480 , total_loss: 0.7191, data_loss: 0.7191\n",
      "step 500 , total_loss: 0.7077, data_loss: 0.7077\n",
      "step 520 , total_loss: 0.7485, data_loss: 0.7485\n",
      "step 540 , total_loss: 0.7919, data_loss: 0.7919\n",
      "step 560 , total_loss: 0.7022, data_loss: 0.7022\n",
      "step 580 , total_loss: 0.7338, data_loss: 0.7338\n",
      "step 600 , total_loss: 0.6978, data_loss: 0.6978\n",
      "step 620 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 640 , total_loss: 0.6427, data_loss: 0.6427\n",
      "step 660 , total_loss: 0.6589, data_loss: 0.6589\n",
      "step 680 , total_loss: 0.8001, data_loss: 0.8001\n",
      "step 700 , total_loss: 0.7350, data_loss: 0.7350\n",
      "step 720 , total_loss: 0.7634, data_loss: 0.7634\n",
      "step 740 , total_loss: 0.7255, data_loss: 0.7255\n",
      "step 760 , total_loss: 0.7118, data_loss: 0.7118\n",
      "step 780 , total_loss: 0.7149, data_loss: 0.7149\n",
      "step 800 , total_loss: 0.8106, data_loss: 0.8106\n",
      "step 820 , total_loss: 0.7978, data_loss: 0.7978\n",
      "step 840 , total_loss: 0.8006, data_loss: 0.8006\n",
      "step 860 , total_loss: 0.7951, data_loss: 0.7951\n",
      "step 880 , total_loss: 0.8191, data_loss: 0.8191\n",
      "step 900 , total_loss: 0.7102, data_loss: 0.7102\n",
      "step 920 , total_loss: 0.7743, data_loss: 0.7743\n",
      "step 940 , total_loss: 0.7503, data_loss: 0.7503\n",
      "step 960 , total_loss: 0.7544, data_loss: 0.7544\n",
      "step 980 , total_loss: 0.7149, data_loss: 0.7149\n",
      "step 1000 , total_loss: 0.7254, data_loss: 0.7254\n",
      "step 1020 , total_loss: 0.7361, data_loss: 0.7361\n",
      "step 1040 , total_loss: 0.8078, data_loss: 0.8078\n",
      "step 1060 , total_loss: 0.7672, data_loss: 0.7672\n",
      "step 1080 , total_loss: 0.6682, data_loss: 0.6682\n",
      "step 1100 , total_loss: 0.7684, data_loss: 0.7684\n",
      "step 1120 , total_loss: 0.8379, data_loss: 0.8379\n",
      "step 1140 , total_loss: 0.7165, data_loss: 0.7165\n",
      "step 1160 , total_loss: 0.6752, data_loss: 0.6752\n",
      "step 1180 , total_loss: 0.7458, data_loss: 0.7458\n",
      "step 1200 , total_loss: 0.8057, data_loss: 0.8057\n",
      "step 1220 , total_loss: 0.7167, data_loss: 0.7167\n",
      "step 1240 , total_loss: 0.7782, data_loss: 0.7782\n",
      "step 1260 , total_loss: 0.7623, data_loss: 0.7623\n",
      "step 1280 , total_loss: 0.7142, data_loss: 0.7142\n",
      "step 1300 , total_loss: 0.7124, data_loss: 0.7124\n",
      "step 1320 , total_loss: 0.6508, data_loss: 0.6508\n",
      "step 1340 , total_loss: 0.8322, data_loss: 0.8322\n",
      "step 1360 , total_loss: 0.8315, data_loss: 0.8315\n",
      "step 1380 , total_loss: 0.7590, data_loss: 0.7590\n",
      "step 1400 , total_loss: 0.8094, data_loss: 0.8094\n",
      "step 1420 , total_loss: 0.7334, data_loss: 0.7334\n",
      "step 1440 , total_loss: 0.8153, data_loss: 0.8153\n",
      "step 1460 , total_loss: 0.8028, data_loss: 0.8028\n",
      "step 1480 , total_loss: 0.7976, data_loss: 0.7976\n",
      "step 1500 , total_loss: 0.7546, data_loss: 0.7546\n",
      "step 1520 , total_loss: 0.7450, data_loss: 0.7450\n",
      "step 1540 , total_loss: 0.7966, data_loss: 0.7966\n",
      "step 1560 , total_loss: 0.7826, data_loss: 0.7826\n",
      "step 1580 , total_loss: 0.8833, data_loss: 0.8833\n",
      "step 1600 , total_loss: 0.7671, data_loss: 0.7671\n",
      "step 1620 , total_loss: 0.6937, data_loss: 0.6937\n",
      "step 1640 , total_loss: 0.8007, data_loss: 0.8007\n",
      "step 1660 , total_loss: 0.7740, data_loss: 0.7740\n",
      "step 1680 , total_loss: 0.6994, data_loss: 0.6994\n",
      "step 1700 , total_loss: 0.7233, data_loss: 0.7233\n",
      "step 1720 , total_loss: 0.8105, data_loss: 0.8105\n",
      "step 1740 , total_loss: 0.7248, data_loss: 0.7248\n",
      "step 1760 , total_loss: 0.7993, data_loss: 0.7993\n",
      "step 1780 , total_loss: 0.7568, data_loss: 0.7568\n",
      "step 1800 , total_loss: 0.8319, data_loss: 0.8319\n",
      "step 1820 , total_loss: 0.7243, data_loss: 0.7243\n",
      "step 1840 , total_loss: 0.7512, data_loss: 0.7512\n",
      "step 1860 , total_loss: 0.7101, data_loss: 0.7101\n",
      "step 1880 , total_loss: 0.7126, data_loss: 0.7126\n",
      "step 1900 , total_loss: 0.7355, data_loss: 0.7355\n",
      "step 1920 , total_loss: 0.8316, data_loss: 0.8316\n",
      "step 1940 , total_loss: 0.7387, data_loss: 0.7387\n",
      "step 1960 , total_loss: 0.7936, data_loss: 0.7936\n",
      "step 1980 , total_loss: 0.7806, data_loss: 0.7806\n",
      "step 2000 , total_loss: 0.8165, data_loss: 0.8165\n",
      "step 2020 , total_loss: 0.6774, data_loss: 0.6774\n",
      "step 2040 , total_loss: 0.6900, data_loss: 0.6900\n",
      "step 2060 , total_loss: 0.7230, data_loss: 0.7230\n",
      "step 2080 , total_loss: 0.8077, data_loss: 0.8077\n",
      "step 2100 , total_loss: 0.7546, data_loss: 0.7546\n",
      "step 2120 , total_loss: 0.7548, data_loss: 0.7548\n",
      "step 2140 , total_loss: 0.7854, data_loss: 0.7854\n",
      "step 2160 , total_loss: 0.7051, data_loss: 0.7051\n",
      "step 2180 , total_loss: 0.7556, data_loss: 0.7556\n",
      "step 2200 , total_loss: 0.6934, data_loss: 0.6934\n",
      "step 2220 , total_loss: 0.7296, data_loss: 0.7296\n",
      "step 2240 , total_loss: 0.7587, data_loss: 0.7587\n",
      "step 2260 , total_loss: 0.8122, data_loss: 0.8122\n",
      "step 2280 , total_loss: 0.7782, data_loss: 0.7782\n",
      "step 2300 , total_loss: 0.7976, data_loss: 0.7976\n",
      "step 2320 , total_loss: 0.8030, data_loss: 0.8030\n",
      "step 2340 , total_loss: 0.7843, data_loss: 0.7843\n",
      "step 2360 , total_loss: 0.7077, data_loss: 0.7077\n",
      "step 2380 , total_loss: 0.7742, data_loss: 0.7742\n",
      "step 2400 , total_loss: 0.8470, data_loss: 0.8470\n",
      "step 2420 , total_loss: 0.8163, data_loss: 0.8163\n",
      "step 2440 , total_loss: 0.8199, data_loss: 0.8199\n",
      "step 2460 , total_loss: 0.8049, data_loss: 0.8049\n",
      "step 2480 , total_loss: 0.7492, data_loss: 0.7492\n",
      "step 2500 , total_loss: 0.7588, data_loss: 0.7588\n",
      "step 2520 , total_loss: 0.7776, data_loss: 0.7776\n",
      "step 2540 , total_loss: 0.7068, data_loss: 0.7068\n",
      "step 2560 , total_loss: 0.7629, data_loss: 0.7629\n",
      "step 2580 , total_loss: 0.7368, data_loss: 0.7368\n",
      "step 2600 , total_loss: 0.7985, data_loss: 0.7985\n",
      "step 2620 , total_loss: 0.7644, data_loss: 0.7644\n",
      "step 2640 , total_loss: 0.8063, data_loss: 0.8063\n",
      "step 2660 , total_loss: 0.7727, data_loss: 0.7727\n",
      "step 2680 , total_loss: 0.7128, data_loss: 0.7128\n",
      "step 2700 , total_loss: 0.7725, data_loss: 0.7725\n",
      "step 2720 , total_loss: 0.7060, data_loss: 0.7060\n",
      "step 2740 , total_loss: 0.7504, data_loss: 0.7504\n",
      "step 2760 , total_loss: 0.7432, data_loss: 0.7432\n",
      "step 2780 , total_loss: 0.7962, data_loss: 0.7962\n",
      "step 2800 , total_loss: 0.7537, data_loss: 0.7537\n",
      "step 2820 , total_loss: 0.7586, data_loss: 0.7586\n",
      "step 2840 , total_loss: 0.7612, data_loss: 0.7612\n",
      "step 2860 , total_loss: 0.8340, data_loss: 0.8340\n",
      "step 2880 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 2900 , total_loss: 0.7941, data_loss: 0.7941\n",
      "step 2920 , total_loss: 0.7238, data_loss: 0.7238\n",
      "step 2940 , total_loss: 0.7256, data_loss: 0.7256\n",
      "step 2960 , total_loss: 0.7093, data_loss: 0.7093\n",
      "step 2980 , total_loss: 0.6876, data_loss: 0.6876\n",
      "step 3000 , total_loss: 0.8489, data_loss: 0.8489\n",
      "step 3020 , total_loss: 0.7260, data_loss: 0.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3040 , total_loss: 0.8062, data_loss: 0.8062\n",
      "step 3060 , total_loss: 0.7633, data_loss: 0.7633\n",
      "step 3080 , total_loss: 0.8233, data_loss: 0.8233\n",
      "step 3100 , total_loss: 0.7654, data_loss: 0.7654\n",
      "step 3120 , total_loss: 0.7174, data_loss: 0.7174\n",
      "step 3140 , total_loss: 0.7693, data_loss: 0.7693\n",
      "step 3160 , total_loss: 0.7891, data_loss: 0.7891\n",
      "step 3180 , total_loss: 0.8283, data_loss: 0.8283\n",
      "step 3200 , total_loss: 0.7937, data_loss: 0.7937\n",
      "step 3220 , total_loss: 0.7701, data_loss: 0.7701\n",
      "step 3240 , total_loss: 0.7877, data_loss: 0.7877\n",
      "step 3260 , total_loss: 0.7510, data_loss: 0.7510\n",
      "step 3280 , total_loss: 0.8133, data_loss: 0.8133\n",
      "step 3300 , total_loss: 0.8501, data_loss: 0.8501\n",
      "eval valid at epoch 8: auc:0.8418,logloss:1.2995,mean_mrr:0.7886,ndcg@2:0.7627,ndcg@4:0.83,ndcg@6:0.8417,group_auc:0.8413\n",
      "step 20 , total_loss: 0.7587, data_loss: 0.7587\n",
      "step 40 , total_loss: 0.6555, data_loss: 0.6555\n",
      "step 60 , total_loss: 0.7051, data_loss: 0.7051\n",
      "step 80 , total_loss: 0.7870, data_loss: 0.7870\n",
      "step 100 , total_loss: 0.7058, data_loss: 0.7058\n",
      "step 120 , total_loss: 0.6931, data_loss: 0.6931\n",
      "step 140 , total_loss: 0.7717, data_loss: 0.7717\n",
      "step 160 , total_loss: 0.7251, data_loss: 0.7251\n",
      "step 180 , total_loss: 0.8117, data_loss: 0.8117\n",
      "step 200 , total_loss: 0.7502, data_loss: 0.7502\n",
      "step 220 , total_loss: 0.6944, data_loss: 0.6944\n",
      "step 240 , total_loss: 0.7033, data_loss: 0.7033\n",
      "step 260 , total_loss: 0.6987, data_loss: 0.6987\n",
      "step 280 , total_loss: 0.7873, data_loss: 0.7873\n",
      "step 300 , total_loss: 0.7475, data_loss: 0.7475\n",
      "step 320 , total_loss: 0.6648, data_loss: 0.6648\n",
      "step 340 , total_loss: 0.7192, data_loss: 0.7192\n",
      "step 360 , total_loss: 0.6947, data_loss: 0.6947\n",
      "step 380 , total_loss: 0.6599, data_loss: 0.6599\n",
      "step 400 , total_loss: 0.7032, data_loss: 0.7032\n",
      "step 420 , total_loss: 0.6833, data_loss: 0.6833\n",
      "step 440 , total_loss: 0.6556, data_loss: 0.6556\n",
      "step 460 , total_loss: 0.7516, data_loss: 0.7516\n",
      "step 480 , total_loss: 0.6582, data_loss: 0.6582\n",
      "step 500 , total_loss: 0.7897, data_loss: 0.7897\n",
      "step 520 , total_loss: 0.7400, data_loss: 0.7400\n",
      "step 540 , total_loss: 0.7062, data_loss: 0.7062\n",
      "step 560 , total_loss: 0.7064, data_loss: 0.7064\n",
      "step 580 , total_loss: 0.7816, data_loss: 0.7816\n",
      "step 600 , total_loss: 0.8407, data_loss: 0.8407\n",
      "step 620 , total_loss: 0.7222, data_loss: 0.7222\n",
      "step 640 , total_loss: 0.7523, data_loss: 0.7523\n",
      "step 660 , total_loss: 0.7102, data_loss: 0.7102\n",
      "step 680 , total_loss: 0.7494, data_loss: 0.7494\n",
      "step 700 , total_loss: 0.7813, data_loss: 0.7813\n",
      "step 720 , total_loss: 0.6698, data_loss: 0.6698\n",
      "step 740 , total_loss: 0.6835, data_loss: 0.6835\n",
      "step 760 , total_loss: 0.6802, data_loss: 0.6802\n",
      "step 780 , total_loss: 0.7601, data_loss: 0.7601\n",
      "step 800 , total_loss: 0.7092, data_loss: 0.7092\n",
      "step 820 , total_loss: 0.6987, data_loss: 0.6987\n",
      "step 840 , total_loss: 0.7655, data_loss: 0.7655\n",
      "step 860 , total_loss: 0.7243, data_loss: 0.7243\n",
      "step 880 , total_loss: 0.7558, data_loss: 0.7558\n",
      "step 900 , total_loss: 0.8262, data_loss: 0.8262\n",
      "step 920 , total_loss: 0.7150, data_loss: 0.7150\n",
      "step 940 , total_loss: 0.6698, data_loss: 0.6698\n",
      "step 960 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 980 , total_loss: 0.7344, data_loss: 0.7344\n",
      "step 1000 , total_loss: 0.6828, data_loss: 0.6828\n",
      "step 1020 , total_loss: 0.7008, data_loss: 0.7008\n",
      "step 1040 , total_loss: 0.6579, data_loss: 0.6579\n",
      "step 1060 , total_loss: 0.7476, data_loss: 0.7476\n",
      "step 1080 , total_loss: 0.8632, data_loss: 0.8632\n",
      "step 1100 , total_loss: 0.7004, data_loss: 0.7004\n",
      "step 1120 , total_loss: 0.7575, data_loss: 0.7575\n",
      "step 1140 , total_loss: 0.7459, data_loss: 0.7459\n",
      "step 1160 , total_loss: 0.7453, data_loss: 0.7453\n",
      "step 1180 , total_loss: 0.6741, data_loss: 0.6741\n",
      "step 1200 , total_loss: 0.6564, data_loss: 0.6564\n",
      "step 1220 , total_loss: 0.6839, data_loss: 0.6839\n",
      "step 1240 , total_loss: 0.7204, data_loss: 0.7204\n",
      "step 1260 , total_loss: 0.7010, data_loss: 0.7010\n",
      "step 1280 , total_loss: 0.7817, data_loss: 0.7817\n",
      "step 1300 , total_loss: 0.7166, data_loss: 0.7166\n",
      "step 1320 , total_loss: 0.7239, data_loss: 0.7239\n",
      "step 1340 , total_loss: 0.7278, data_loss: 0.7278\n",
      "step 1360 , total_loss: 0.7853, data_loss: 0.7853\n",
      "step 1380 , total_loss: 0.7300, data_loss: 0.7300\n",
      "step 1400 , total_loss: 0.6614, data_loss: 0.6614\n",
      "step 1420 , total_loss: 0.8054, data_loss: 0.8054\n",
      "step 1440 , total_loss: 0.7671, data_loss: 0.7671\n",
      "step 1460 , total_loss: 0.7344, data_loss: 0.7344\n",
      "step 1480 , total_loss: 0.6895, data_loss: 0.6895\n",
      "step 1500 , total_loss: 0.7418, data_loss: 0.7418\n",
      "step 1520 , total_loss: 0.8056, data_loss: 0.8056\n",
      "step 1540 , total_loss: 0.7307, data_loss: 0.7307\n",
      "step 1560 , total_loss: 0.7474, data_loss: 0.7474\n",
      "step 1580 , total_loss: 0.7229, data_loss: 0.7229\n",
      "step 1600 , total_loss: 0.8605, data_loss: 0.8605\n",
      "step 1620 , total_loss: 0.6988, data_loss: 0.6988\n",
      "step 1640 , total_loss: 0.8030, data_loss: 0.8030\n",
      "step 1660 , total_loss: 0.7505, data_loss: 0.7505\n",
      "step 1680 , total_loss: 0.8298, data_loss: 0.8298\n",
      "step 1700 , total_loss: 0.7474, data_loss: 0.7474\n",
      "step 1720 , total_loss: 0.7143, data_loss: 0.7143\n",
      "step 1740 , total_loss: 0.7124, data_loss: 0.7124\n",
      "step 1760 , total_loss: 0.7491, data_loss: 0.7491\n",
      "step 1780 , total_loss: 0.7555, data_loss: 0.7555\n",
      "step 1800 , total_loss: 0.7142, data_loss: 0.7142\n",
      "step 1820 , total_loss: 0.7085, data_loss: 0.7085\n",
      "step 1840 , total_loss: 0.7693, data_loss: 0.7693\n",
      "step 1860 , total_loss: 0.8120, data_loss: 0.8120\n",
      "step 1880 , total_loss: 0.7729, data_loss: 0.7729\n",
      "step 1900 , total_loss: 0.7169, data_loss: 0.7169\n",
      "step 1920 , total_loss: 0.8293, data_loss: 0.8293\n",
      "step 1940 , total_loss: 0.6955, data_loss: 0.6955\n",
      "step 1960 , total_loss: 0.7436, data_loss: 0.7436\n",
      "step 1980 , total_loss: 0.7898, data_loss: 0.7898\n",
      "step 2000 , total_loss: 0.7288, data_loss: 0.7288\n",
      "step 2020 , total_loss: 0.7839, data_loss: 0.7839\n",
      "step 2040 , total_loss: 0.7197, data_loss: 0.7197\n",
      "step 2060 , total_loss: 0.7443, data_loss: 0.7443\n",
      "step 2080 , total_loss: 0.8099, data_loss: 0.8099\n",
      "step 2100 , total_loss: 0.8107, data_loss: 0.8107\n",
      "step 2120 , total_loss: 0.6752, data_loss: 0.6752\n",
      "step 2140 , total_loss: 0.7539, data_loss: 0.7539\n",
      "step 2160 , total_loss: 0.7244, data_loss: 0.7244\n",
      "step 2180 , total_loss: 0.7369, data_loss: 0.7369\n",
      "step 2200 , total_loss: 0.8378, data_loss: 0.8378\n",
      "step 2220 , total_loss: 0.7237, data_loss: 0.7237\n",
      "step 2240 , total_loss: 0.7938, data_loss: 0.7938\n",
      "step 2260 , total_loss: 0.7399, data_loss: 0.7399\n",
      "step 2280 , total_loss: 0.7824, data_loss: 0.7824\n",
      "step 2300 , total_loss: 0.6325, data_loss: 0.6325\n",
      "step 2320 , total_loss: 0.8291, data_loss: 0.8291\n",
      "step 2340 , total_loss: 0.7661, data_loss: 0.7661\n",
      "step 2360 , total_loss: 0.6991, data_loss: 0.6991\n",
      "step 2380 , total_loss: 0.7906, data_loss: 0.7906\n",
      "step 2400 , total_loss: 0.7086, data_loss: 0.7086\n",
      "step 2420 , total_loss: 0.8559, data_loss: 0.8559\n",
      "step 2440 , total_loss: 0.6542, data_loss: 0.6542\n",
      "step 2460 , total_loss: 0.7562, data_loss: 0.7562\n",
      "step 2480 , total_loss: 0.7329, data_loss: 0.7329\n",
      "step 2500 , total_loss: 0.7550, data_loss: 0.7550\n",
      "step 2520 , total_loss: 0.7331, data_loss: 0.7331\n",
      "step 2540 , total_loss: 0.7188, data_loss: 0.7188\n",
      "step 2560 , total_loss: 0.7711, data_loss: 0.7711\n",
      "step 2580 , total_loss: 0.7409, data_loss: 0.7409\n",
      "step 2600 , total_loss: 0.6749, data_loss: 0.6749\n",
      "step 2620 , total_loss: 0.7276, data_loss: 0.7276\n",
      "step 2640 , total_loss: 0.7281, data_loss: 0.7281\n",
      "step 2660 , total_loss: 0.8119, data_loss: 0.8119\n",
      "step 2680 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 2700 , total_loss: 0.7767, data_loss: 0.7767\n",
      "step 2720 , total_loss: 0.7528, data_loss: 0.7528\n",
      "step 2740 , total_loss: 0.7516, data_loss: 0.7516\n",
      "step 2760 , total_loss: 0.8041, data_loss: 0.8041\n",
      "step 2780 , total_loss: 0.7585, data_loss: 0.7585\n",
      "step 2800 , total_loss: 0.6930, data_loss: 0.6930\n",
      "step 2820 , total_loss: 0.7506, data_loss: 0.7506\n",
      "step 2840 , total_loss: 0.7581, data_loss: 0.7581\n",
      "step 2860 , total_loss: 0.8298, data_loss: 0.8298\n",
      "step 2880 , total_loss: 0.7473, data_loss: 0.7473\n",
      "step 2900 , total_loss: 0.7336, data_loss: 0.7336\n",
      "step 2920 , total_loss: 0.7776, data_loss: 0.7776\n",
      "step 2940 , total_loss: 0.7827, data_loss: 0.7827\n",
      "step 2960 , total_loss: 0.7695, data_loss: 0.7695\n",
      "step 2980 , total_loss: 0.7331, data_loss: 0.7331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000 , total_loss: 0.7020, data_loss: 0.7020\n",
      "step 3020 , total_loss: 0.7673, data_loss: 0.7673\n",
      "step 3040 , total_loss: 0.8051, data_loss: 0.8051\n",
      "step 3060 , total_loss: 0.7736, data_loss: 0.7736\n",
      "step 3080 , total_loss: 0.6747, data_loss: 0.6747\n",
      "step 3100 , total_loss: 0.8295, data_loss: 0.8295\n",
      "step 3120 , total_loss: 0.7831, data_loss: 0.7831\n",
      "step 3140 , total_loss: 0.6773, data_loss: 0.6773\n",
      "step 3160 , total_loss: 0.7131, data_loss: 0.7131\n",
      "step 3180 , total_loss: 0.7478, data_loss: 0.7478\n",
      "step 3200 , total_loss: 0.7321, data_loss: 0.7321\n",
      "step 3220 , total_loss: 0.8114, data_loss: 0.8114\n",
      "step 3240 , total_loss: 0.7735, data_loss: 0.7735\n",
      "step 3260 , total_loss: 0.7445, data_loss: 0.7445\n",
      "step 3280 , total_loss: 0.7527, data_loss: 0.7527\n",
      "step 3300 , total_loss: 0.8443, data_loss: 0.8443\n",
      "eval valid at epoch 9: auc:0.842,logloss:1.3998,mean_mrr:0.7898,ndcg@2:0.764,ndcg@4:0.8309,ndcg@6:0.8426,group_auc:0.8421\n",
      "step 20 , total_loss: 0.6450, data_loss: 0.6450\n",
      "step 40 , total_loss: 0.7445, data_loss: 0.7445\n",
      "step 60 , total_loss: 0.6985, data_loss: 0.6985\n",
      "step 80 , total_loss: 0.6828, data_loss: 0.6828\n",
      "step 100 , total_loss: 0.7513, data_loss: 0.7513\n",
      "step 120 , total_loss: 0.7408, data_loss: 0.7408\n",
      "step 140 , total_loss: 0.6329, data_loss: 0.6329\n",
      "step 160 , total_loss: 0.7090, data_loss: 0.7090\n",
      "step 180 , total_loss: 0.7708, data_loss: 0.7708\n",
      "step 200 , total_loss: 0.6923, data_loss: 0.6923\n",
      "step 220 , total_loss: 0.7365, data_loss: 0.7365\n",
      "step 240 , total_loss: 0.7040, data_loss: 0.7040\n",
      "step 260 , total_loss: 0.6118, data_loss: 0.6118\n",
      "step 280 , total_loss: 0.6833, data_loss: 0.6833\n",
      "step 300 , total_loss: 0.7232, data_loss: 0.7232\n",
      "step 320 , total_loss: 0.7001, data_loss: 0.7001\n",
      "step 340 , total_loss: 0.6963, data_loss: 0.6963\n",
      "step 360 , total_loss: 0.7054, data_loss: 0.7054\n",
      "step 380 , total_loss: 0.7601, data_loss: 0.7601\n",
      "step 400 , total_loss: 0.7974, data_loss: 0.7974\n",
      "step 420 , total_loss: 0.6975, data_loss: 0.6975\n",
      "step 440 , total_loss: 0.6973, data_loss: 0.6973\n",
      "step 460 , total_loss: 0.7462, data_loss: 0.7462\n",
      "step 480 , total_loss: 0.6102, data_loss: 0.6102\n",
      "step 500 , total_loss: 0.7497, data_loss: 0.7497\n",
      "step 520 , total_loss: 0.7051, data_loss: 0.7051\n",
      "step 540 , total_loss: 0.6391, data_loss: 0.6391\n",
      "step 560 , total_loss: 0.6318, data_loss: 0.6318\n",
      "step 580 , total_loss: 0.6617, data_loss: 0.6617\n",
      "step 600 , total_loss: 0.6961, data_loss: 0.6961\n",
      "step 620 , total_loss: 0.6657, data_loss: 0.6657\n",
      "step 640 , total_loss: 0.6968, data_loss: 0.6968\n",
      "step 660 , total_loss: 0.7286, data_loss: 0.7286\n",
      "step 680 , total_loss: 0.7377, data_loss: 0.7377\n",
      "step 700 , total_loss: 0.6553, data_loss: 0.6553\n",
      "step 720 , total_loss: 0.6975, data_loss: 0.6975\n",
      "step 740 , total_loss: 0.7176, data_loss: 0.7176\n",
      "step 760 , total_loss: 0.7113, data_loss: 0.7113\n",
      "step 780 , total_loss: 0.7191, data_loss: 0.7191\n",
      "step 800 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 820 , total_loss: 0.7465, data_loss: 0.7465\n",
      "step 840 , total_loss: 0.7589, data_loss: 0.7589\n",
      "step 860 , total_loss: 0.7199, data_loss: 0.7199\n",
      "step 880 , total_loss: 0.7383, data_loss: 0.7383\n",
      "step 900 , total_loss: 0.6957, data_loss: 0.6957\n",
      "step 920 , total_loss: 0.6606, data_loss: 0.6606\n",
      "step 940 , total_loss: 0.6860, data_loss: 0.6860\n",
      "step 960 , total_loss: 0.7828, data_loss: 0.7828\n",
      "step 980 , total_loss: 0.7019, data_loss: 0.7019\n",
      "step 1000 , total_loss: 0.6333, data_loss: 0.6333\n",
      "step 1020 , total_loss: 0.7210, data_loss: 0.7210\n",
      "step 1040 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 1060 , total_loss: 0.7371, data_loss: 0.7371\n",
      "step 1080 , total_loss: 0.7138, data_loss: 0.7138\n",
      "step 1100 , total_loss: 0.7876, data_loss: 0.7876\n",
      "step 1120 , total_loss: 0.7800, data_loss: 0.7800\n",
      "step 1140 , total_loss: 0.6916, data_loss: 0.6916\n",
      "step 1160 , total_loss: 0.7407, data_loss: 0.7407\n",
      "step 1180 , total_loss: 0.6899, data_loss: 0.6899\n",
      "step 1200 , total_loss: 0.6798, data_loss: 0.6798\n",
      "step 1220 , total_loss: 0.7712, data_loss: 0.7712\n",
      "step 1240 , total_loss: 0.7213, data_loss: 0.7213\n",
      "step 1260 , total_loss: 0.6257, data_loss: 0.6257\n",
      "step 1280 , total_loss: 0.7954, data_loss: 0.7954\n",
      "step 1300 , total_loss: 0.7682, data_loss: 0.7682\n",
      "step 1320 , total_loss: 0.7086, data_loss: 0.7086\n",
      "step 1340 , total_loss: 0.7821, data_loss: 0.7821\n",
      "step 1360 , total_loss: 0.7301, data_loss: 0.7301\n",
      "step 1380 , total_loss: 0.7799, data_loss: 0.7799\n",
      "step 1400 , total_loss: 0.7396, data_loss: 0.7396\n",
      "step 1420 , total_loss: 0.7386, data_loss: 0.7386\n",
      "step 1440 , total_loss: 0.7553, data_loss: 0.7553\n",
      "step 1460 , total_loss: 0.7432, data_loss: 0.7432\n",
      "step 1480 , total_loss: 0.7554, data_loss: 0.7554\n",
      "step 1500 , total_loss: 0.7075, data_loss: 0.7075\n",
      "step 1520 , total_loss: 0.8036, data_loss: 0.8036\n",
      "step 1540 , total_loss: 0.6981, data_loss: 0.6981\n",
      "step 1560 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 1580 , total_loss: 0.7457, data_loss: 0.7457\n",
      "step 1600 , total_loss: 0.7340, data_loss: 0.7340\n",
      "step 1620 , total_loss: 0.7202, data_loss: 0.7202\n",
      "step 1640 , total_loss: 0.7273, data_loss: 0.7273\n",
      "step 1660 , total_loss: 0.8096, data_loss: 0.8096\n",
      "step 1680 , total_loss: 0.6813, data_loss: 0.6813\n",
      "step 1700 , total_loss: 0.6979, data_loss: 0.6979\n",
      "step 1720 , total_loss: 0.7384, data_loss: 0.7384\n",
      "step 1740 , total_loss: 0.7021, data_loss: 0.7021\n",
      "step 1760 , total_loss: 0.7150, data_loss: 0.7150\n",
      "step 1780 , total_loss: 0.6682, data_loss: 0.6682\n",
      "step 1800 , total_loss: 0.7230, data_loss: 0.7230\n",
      "step 1820 , total_loss: 0.6690, data_loss: 0.6690\n",
      "step 1840 , total_loss: 0.6867, data_loss: 0.6867\n",
      "step 1860 , total_loss: 0.7550, data_loss: 0.7550\n",
      "step 1880 , total_loss: 0.6793, data_loss: 0.6793\n",
      "step 1900 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 1920 , total_loss: 0.7046, data_loss: 0.7046\n",
      "step 1940 , total_loss: 0.7595, data_loss: 0.7595\n",
      "step 1960 , total_loss: 0.7625, data_loss: 0.7625\n",
      "step 1980 , total_loss: 0.8106, data_loss: 0.8106\n",
      "step 2000 , total_loss: 0.6979, data_loss: 0.6979\n",
      "step 2020 , total_loss: 0.7339, data_loss: 0.7339\n",
      "step 2040 , total_loss: 0.7481, data_loss: 0.7481\n",
      "step 2060 , total_loss: 0.8206, data_loss: 0.8206\n",
      "step 2080 , total_loss: 0.7381, data_loss: 0.7381\n",
      "step 2100 , total_loss: 0.7238, data_loss: 0.7238\n",
      "step 2120 , total_loss: 0.7116, data_loss: 0.7116\n",
      "step 2140 , total_loss: 0.6516, data_loss: 0.6516\n",
      "step 2160 , total_loss: 0.7779, data_loss: 0.7779\n",
      "step 2180 , total_loss: 0.7376, data_loss: 0.7376\n",
      "step 2200 , total_loss: 0.7029, data_loss: 0.7029\n",
      "step 2220 , total_loss: 0.7308, data_loss: 0.7308\n",
      "step 2240 , total_loss: 0.7431, data_loss: 0.7431\n",
      "step 2260 , total_loss: 0.7212, data_loss: 0.7212\n",
      "step 2280 , total_loss: 0.7999, data_loss: 0.7999\n",
      "step 2300 , total_loss: 0.7329, data_loss: 0.7329\n",
      "step 2320 , total_loss: 0.7756, data_loss: 0.7756\n",
      "step 2340 , total_loss: 0.8063, data_loss: 0.8063\n",
      "step 2360 , total_loss: 0.7404, data_loss: 0.7404\n",
      "step 2380 , total_loss: 0.7085, data_loss: 0.7085\n",
      "step 2400 , total_loss: 0.7583, data_loss: 0.7583\n",
      "step 2420 , total_loss: 0.7809, data_loss: 0.7809\n",
      "step 2440 , total_loss: 0.7692, data_loss: 0.7692\n",
      "step 2460 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 2480 , total_loss: 0.7543, data_loss: 0.7543\n",
      "step 2500 , total_loss: 0.7417, data_loss: 0.7417\n",
      "step 2520 , total_loss: 0.7542, data_loss: 0.7542\n",
      "step 2540 , total_loss: 0.7380, data_loss: 0.7380\n",
      "step 2560 , total_loss: 0.7240, data_loss: 0.7240\n",
      "step 2580 , total_loss: 0.7362, data_loss: 0.7362\n",
      "step 2600 , total_loss: 0.7113, data_loss: 0.7113\n",
      "step 2620 , total_loss: 0.7470, data_loss: 0.7470\n",
      "step 2640 , total_loss: 0.7699, data_loss: 0.7699\n",
      "step 2660 , total_loss: 0.7188, data_loss: 0.7188\n",
      "step 2680 , total_loss: 0.7420, data_loss: 0.7420\n",
      "step 2700 , total_loss: 0.6909, data_loss: 0.6909\n",
      "step 2720 , total_loss: 0.6858, data_loss: 0.6858\n",
      "step 2740 , total_loss: 0.7710, data_loss: 0.7710\n",
      "step 2760 , total_loss: 0.7246, data_loss: 0.7246\n",
      "step 2780 , total_loss: 0.6849, data_loss: 0.6849\n",
      "step 2800 , total_loss: 0.7009, data_loss: 0.7009\n",
      "step 2820 , total_loss: 0.7792, data_loss: 0.7792\n",
      "step 2840 , total_loss: 0.7921, data_loss: 0.7921\n",
      "step 2860 , total_loss: 0.7812, data_loss: 0.7812\n",
      "step 2880 , total_loss: 0.7575, data_loss: 0.7575\n",
      "step 2900 , total_loss: 0.7524, data_loss: 0.7524\n",
      "step 2920 , total_loss: 0.7132, data_loss: 0.7132\n",
      "step 2940 , total_loss: 0.7507, data_loss: 0.7507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2960 , total_loss: 0.6766, data_loss: 0.6766\n",
      "step 2980 , total_loss: 0.7158, data_loss: 0.7158\n",
      "step 3000 , total_loss: 0.7296, data_loss: 0.7296\n",
      "step 3020 , total_loss: 0.7687, data_loss: 0.7687\n",
      "step 3040 , total_loss: 0.8041, data_loss: 0.8041\n",
      "step 3060 , total_loss: 0.7245, data_loss: 0.7245\n",
      "step 3080 , total_loss: 0.7861, data_loss: 0.7861\n",
      "step 3100 , total_loss: 0.8063, data_loss: 0.8063\n",
      "step 3120 , total_loss: 0.7044, data_loss: 0.7044\n",
      "step 3140 , total_loss: 0.7131, data_loss: 0.7131\n",
      "step 3160 , total_loss: 0.8323, data_loss: 0.8323\n",
      "step 3180 , total_loss: 0.7047, data_loss: 0.7047\n",
      "step 3200 , total_loss: 0.6762, data_loss: 0.6762\n",
      "step 3220 , total_loss: 0.6924, data_loss: 0.6924\n",
      "step 3240 , total_loss: 0.7174, data_loss: 0.7174\n",
      "step 3260 , total_loss: 0.7716, data_loss: 0.7716\n",
      "step 3280 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 3300 , total_loss: 0.6975, data_loss: 0.6975\n",
      "eval valid at epoch 10: auc:0.841,logloss:1.4935,mean_mrr:0.7885,ndcg@2:0.7628,ndcg@4:0.8297,ndcg@6:0.8416,group_auc:0.8409\n",
      "[(1, {'auc': 0.799, 'logloss': 0.708, 'mean_mrr': 0.7345, 'ndcg@2': 0.6963, 'ndcg@4': 0.7837, 'ndcg@6': 0.801, 'group_auc': 0.7948}), (2, {'auc': 0.8207, 'logloss': 0.7483, 'mean_mrr': 0.7616, 'ndcg@2': 0.7298, 'ndcg@4': 0.807, 'ndcg@6': 0.8214, 'group_auc': 0.8185}), (3, {'auc': 0.8307, 'logloss': 0.8383, 'mean_mrr': 0.7732, 'ndcg@2': 0.7445, 'ndcg@4': 0.8173, 'ndcg@6': 0.8301, 'group_auc': 0.829}), (4, {'auc': 0.8365, 'logloss': 0.9544, 'mean_mrr': 0.781, 'ndcg@2': 0.754, 'ndcg@4': 0.8239, 'ndcg@6': 0.836, 'group_auc': 0.8357}), (5, {'auc': 0.8404, 'logloss': 1.0122, 'mean_mrr': 0.7849, 'ndcg@2': 0.7582, 'ndcg@4': 0.8272, 'ndcg@6': 0.839, 'group_auc': 0.8389}), (6, {'auc': 0.8408, 'logloss': 1.1115, 'mean_mrr': 0.7862, 'ndcg@2': 0.7602, 'ndcg@4': 0.8282, 'ndcg@6': 0.8399, 'group_auc': 0.8398}), (7, {'auc': 0.8426, 'logloss': 1.1948, 'mean_mrr': 0.7879, 'ndcg@2': 0.7621, 'ndcg@4': 0.8299, 'ndcg@6': 0.8412, 'group_auc': 0.8414}), (8, {'auc': 0.8418, 'logloss': 1.2995, 'mean_mrr': 0.7886, 'ndcg@2': 0.7627, 'ndcg@4': 0.83, 'ndcg@6': 0.8417, 'group_auc': 0.8413}), (9, {'auc': 0.842, 'logloss': 1.3998, 'mean_mrr': 0.7898, 'ndcg@2': 0.764, 'ndcg@4': 0.8309, 'ndcg@6': 0.8426, 'group_auc': 0.8421}), (10, {'auc': 0.841, 'logloss': 1.4935, 'mean_mrr': 0.7885, 'ndcg@2': 0.7628, 'ndcg@4': 0.8297, 'ndcg@6': 0.8416, 'group_auc': 0.8409})]\n",
      "best epoch: 9\n",
      "Time cost for training is 66.40 mins\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e41391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.8333, 'logloss': 1.6876, 'mean_mrr': 0.6691, 'ndcg@2': 0.6184, 'ndcg@4': 0.6914, 'ndcg@6': 0.7216, 'group_auc': 0.8318}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved model in ../../tests/resources/deeprec/slirec/model/din_op/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 13:21:11.750419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 13:21:11.750616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 13:21:11.750757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 13:21:11.750917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 13:21:11.751057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 13:21:11.751148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8844 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.834,\n",
       " 'logloss': 1.5783,\n",
       " 'mean_mrr': 0.6692,\n",
       " 'ndcg@2': 0.6185,\n",
       " 'ndcg@4': 0.6916,\n",
       " 'ndcg@6': 0.7221,\n",
       " 'group_auc': 0.8321}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
