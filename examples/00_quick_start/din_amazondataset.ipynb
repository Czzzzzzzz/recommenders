{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6d328",
   "metadata": {},
   "source": [
    "# To analyze the dataset of Amazon\n",
    "\n",
    "number of users: 123,960\n",
    "\n",
    "number of movies: 50,024\n",
    "\n",
    "number of categories: 164\n",
    "\n",
    "number of historical actions for each user:\n",
    "\n",
    "number of train samples: 1,325,653\n",
    "\n",
    "number of test samples: 1,239,600\n",
    "\n",
    "number of pos samples: \n",
    "\n",
    "number of neg samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['relu', 'relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6a1733",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-03 00:31:18.987810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:19.043838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:19.044024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:19.583591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-03 00:31:19.584655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:19.584818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:19.584926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:20.057401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:20.057559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:20.057668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 00:31:20.057759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-03 00:31:20.068528: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-06-03 00:33:07.471466: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.5715, data_loss: 1.5715\n",
      "step 40 , total_loss: 1.5763, data_loss: 1.5763\n",
      "step 60 , total_loss: 1.5559, data_loss: 1.5559\n",
      "step 80 , total_loss: 1.5045, data_loss: 1.5045\n",
      "step 100 , total_loss: 1.5237, data_loss: 1.5237\n",
      "step 120 , total_loss: 1.5685, data_loss: 1.5685\n",
      "step 140 , total_loss: 1.5238, data_loss: 1.5238\n",
      "step 160 , total_loss: 1.5029, data_loss: 1.5029\n",
      "step 180 , total_loss: 1.5502, data_loss: 1.5502\n",
      "step 200 , total_loss: 1.5264, data_loss: 1.5264\n",
      "step 220 , total_loss: 1.5038, data_loss: 1.5038\n",
      "step 240 , total_loss: 1.5354, data_loss: 1.5354\n",
      "step 260 , total_loss: 1.5132, data_loss: 1.5132\n",
      "step 280 , total_loss: 1.5112, data_loss: 1.5112\n",
      "step 300 , total_loss: 1.4766, data_loss: 1.4766\n",
      "step 320 , total_loss: 1.4373, data_loss: 1.4373\n",
      "step 340 , total_loss: 1.4966, data_loss: 1.4966\n",
      "step 360 , total_loss: 1.4150, data_loss: 1.4150\n",
      "step 380 , total_loss: 1.4382, data_loss: 1.4382\n",
      "step 400 , total_loss: 1.4698, data_loss: 1.4698\n",
      "step 420 , total_loss: 1.5032, data_loss: 1.5032\n",
      "step 440 , total_loss: 1.4902, data_loss: 1.4902\n",
      "step 460 , total_loss: 1.3818, data_loss: 1.3818\n",
      "step 480 , total_loss: 1.4283, data_loss: 1.4283\n",
      "step 500 , total_loss: 1.4001, data_loss: 1.4001\n",
      "step 520 , total_loss: 1.3741, data_loss: 1.3741\n",
      "step 540 , total_loss: 1.3756, data_loss: 1.3756\n",
      "step 560 , total_loss: 1.3800, data_loss: 1.3800\n",
      "step 580 , total_loss: 1.4554, data_loss: 1.4554\n",
      "step 600 , total_loss: 1.3997, data_loss: 1.3997\n",
      "step 620 , total_loss: 1.4276, data_loss: 1.4276\n",
      "step 640 , total_loss: 1.3939, data_loss: 1.3939\n",
      "step 660 , total_loss: 1.3707, data_loss: 1.3707\n",
      "step 680 , total_loss: 1.3613, data_loss: 1.3613\n",
      "step 700 , total_loss: 1.3216, data_loss: 1.3216\n",
      "step 720 , total_loss: 1.3657, data_loss: 1.3657\n",
      "step 740 , total_loss: 1.3818, data_loss: 1.3818\n",
      "step 760 , total_loss: 1.4009, data_loss: 1.4009\n",
      "step 780 , total_loss: 1.3801, data_loss: 1.3801\n",
      "step 800 , total_loss: 1.3288, data_loss: 1.3288\n",
      "step 820 , total_loss: 1.3344, data_loss: 1.3344\n",
      "step 840 , total_loss: 1.3652, data_loss: 1.3652\n",
      "step 860 , total_loss: 1.3145, data_loss: 1.3145\n",
      "step 880 , total_loss: 1.3323, data_loss: 1.3323\n",
      "step 900 , total_loss: 1.3050, data_loss: 1.3050\n",
      "step 920 , total_loss: 1.3718, data_loss: 1.3718\n",
      "step 940 , total_loss: 1.2318, data_loss: 1.2318\n",
      "step 960 , total_loss: 1.3494, data_loss: 1.3494\n",
      "step 980 , total_loss: 1.2658, data_loss: 1.2658\n",
      "step 1000 , total_loss: 1.3609, data_loss: 1.3609\n",
      "step 1020 , total_loss: 1.2989, data_loss: 1.2989\n",
      "step 1040 , total_loss: 1.3230, data_loss: 1.3230\n",
      "step 1060 , total_loss: 1.3056, data_loss: 1.3056\n",
      "step 1080 , total_loss: 1.3015, data_loss: 1.3015\n",
      "step 1100 , total_loss: 1.3406, data_loss: 1.3406\n",
      "step 1120 , total_loss: 1.2716, data_loss: 1.2716\n",
      "step 1140 , total_loss: 1.2682, data_loss: 1.2682\n",
      "step 1160 , total_loss: 1.2942, data_loss: 1.2942\n",
      "step 1180 , total_loss: 1.2715, data_loss: 1.2715\n",
      "step 1200 , total_loss: 1.3009, data_loss: 1.3009\n",
      "step 1220 , total_loss: 1.3320, data_loss: 1.3320\n",
      "step 1240 , total_loss: 1.2564, data_loss: 1.2564\n",
      "step 1260 , total_loss: 1.3360, data_loss: 1.3360\n",
      "step 1280 , total_loss: 1.2623, data_loss: 1.2623\n",
      "step 1300 , total_loss: 1.2687, data_loss: 1.2687\n",
      "step 1320 , total_loss: 1.2618, data_loss: 1.2618\n",
      "step 1340 , total_loss: 1.2522, data_loss: 1.2522\n",
      "step 1360 , total_loss: 1.2595, data_loss: 1.2595\n",
      "step 1380 , total_loss: 1.2907, data_loss: 1.2907\n",
      "step 1400 , total_loss: 1.2450, data_loss: 1.2450\n",
      "step 1420 , total_loss: 1.2790, data_loss: 1.2790\n",
      "step 1440 , total_loss: 1.1848, data_loss: 1.1848\n",
      "step 1460 , total_loss: 1.2390, data_loss: 1.2390\n",
      "step 1480 , total_loss: 1.1671, data_loss: 1.1671\n",
      "step 1500 , total_loss: 1.2526, data_loss: 1.2526\n",
      "step 1520 , total_loss: 1.2673, data_loss: 1.2673\n",
      "step 1540 , total_loss: 1.2483, data_loss: 1.2483\n",
      "step 1560 , total_loss: 1.2477, data_loss: 1.2477\n",
      "step 1580 , total_loss: 1.3044, data_loss: 1.3044\n",
      "step 1600 , total_loss: 1.2912, data_loss: 1.2912\n",
      "step 1620 , total_loss: 1.2948, data_loss: 1.2948\n",
      "step 1640 , total_loss: 1.1878, data_loss: 1.1878\n",
      "step 1660 , total_loss: 1.2424, data_loss: 1.2424\n",
      "step 1680 , total_loss: 1.2881, data_loss: 1.2881\n",
      "step 1700 , total_loss: 1.2389, data_loss: 1.2389\n",
      "step 1720 , total_loss: 1.2646, data_loss: 1.2646\n",
      "step 1740 , total_loss: 1.2309, data_loss: 1.2309\n",
      "step 1760 , total_loss: 1.2208, data_loss: 1.2208\n",
      "step 1780 , total_loss: 1.2826, data_loss: 1.2826\n",
      "step 1800 , total_loss: 1.2159, data_loss: 1.2159\n",
      "step 1820 , total_loss: 1.1944, data_loss: 1.1944\n",
      "step 1840 , total_loss: 1.2441, data_loss: 1.2441\n",
      "step 1860 , total_loss: 1.2229, data_loss: 1.2229\n",
      "step 1880 , total_loss: 1.2219, data_loss: 1.2219\n",
      "step 1900 , total_loss: 1.2544, data_loss: 1.2544\n",
      "step 1920 , total_loss: 1.1594, data_loss: 1.1594\n",
      "step 1940 , total_loss: 1.2628, data_loss: 1.2628\n",
      "step 1960 , total_loss: 1.2028, data_loss: 1.2028\n",
      "step 1980 , total_loss: 1.1964, data_loss: 1.1964\n",
      "step 2000 , total_loss: 1.2846, data_loss: 1.2846\n",
      "step 2020 , total_loss: 1.1843, data_loss: 1.1843\n",
      "step 2040 , total_loss: 1.1974, data_loss: 1.1974\n",
      "step 2060 , total_loss: 1.1680, data_loss: 1.1680\n",
      "step 2080 , total_loss: 1.1856, data_loss: 1.1856\n",
      "step 2100 , total_loss: 1.1495, data_loss: 1.1495\n",
      "step 2120 , total_loss: 1.1868, data_loss: 1.1868\n",
      "step 2140 , total_loss: 1.1630, data_loss: 1.1630\n",
      "step 2160 , total_loss: 1.1600, data_loss: 1.1600\n",
      "step 2180 , total_loss: 1.1032, data_loss: 1.1032\n",
      "step 2200 , total_loss: 1.1986, data_loss: 1.1986\n",
      "step 2220 , total_loss: 1.1525, data_loss: 1.1525\n",
      "step 2240 , total_loss: 1.1570, data_loss: 1.1570\n",
      "step 2260 , total_loss: 1.1787, data_loss: 1.1787\n",
      "step 2280 , total_loss: 1.1987, data_loss: 1.1987\n",
      "step 2300 , total_loss: 1.2402, data_loss: 1.2402\n",
      "step 2320 , total_loss: 1.1658, data_loss: 1.1658\n",
      "step 2340 , total_loss: 1.1474, data_loss: 1.1474\n",
      "step 2360 , total_loss: 1.1306, data_loss: 1.1306\n",
      "step 2380 , total_loss: 1.1790, data_loss: 1.1790\n",
      "step 2400 , total_loss: 1.2643, data_loss: 1.2643\n",
      "step 2420 , total_loss: 1.2183, data_loss: 1.2183\n",
      "step 2440 , total_loss: 1.1803, data_loss: 1.1803\n",
      "step 2460 , total_loss: 1.1499, data_loss: 1.1499\n",
      "step 2480 , total_loss: 1.0494, data_loss: 1.0494\n",
      "step 2500 , total_loss: 1.1754, data_loss: 1.1754\n",
      "step 2520 , total_loss: 1.1383, data_loss: 1.1383\n",
      "step 2540 , total_loss: 1.1781, data_loss: 1.1781\n",
      "step 2560 , total_loss: 1.2174, data_loss: 1.2174\n",
      "step 2580 , total_loss: 1.1313, data_loss: 1.1313\n",
      "step 2600 , total_loss: 1.0826, data_loss: 1.0826\n",
      "step 2620 , total_loss: 1.1567, data_loss: 1.1567\n",
      "step 2640 , total_loss: 1.1447, data_loss: 1.1447\n",
      "step 2660 , total_loss: 1.1583, data_loss: 1.1583\n",
      "step 2680 , total_loss: 1.1405, data_loss: 1.1405\n",
      "step 2700 , total_loss: 1.1459, data_loss: 1.1459\n",
      "step 2720 , total_loss: 1.1548, data_loss: 1.1548\n",
      "step 2740 , total_loss: 1.2274, data_loss: 1.2274\n",
      "step 2760 , total_loss: 1.1387, data_loss: 1.1387\n",
      "step 2780 , total_loss: 1.1947, data_loss: 1.1947\n",
      "step 2800 , total_loss: 1.1418, data_loss: 1.1418\n",
      "step 2820 , total_loss: 1.1596, data_loss: 1.1596\n",
      "step 2840 , total_loss: 1.2526, data_loss: 1.2526\n",
      "step 2860 , total_loss: 1.1721, data_loss: 1.1721\n",
      "step 2880 , total_loss: 1.1129, data_loss: 1.1129\n",
      "step 2900 , total_loss: 1.2013, data_loss: 1.2013\n",
      "step 2920 , total_loss: 1.1122, data_loss: 1.1122\n",
      "step 2940 , total_loss: 1.2307, data_loss: 1.2307\n",
      "step 2960 , total_loss: 1.1493, data_loss: 1.1493\n",
      "step 2980 , total_loss: 1.1646, data_loss: 1.1646\n",
      "step 3000 , total_loss: 1.1602, data_loss: 1.1602\n",
      "step 3020 , total_loss: 1.2300, data_loss: 1.2300\n",
      "step 3040 , total_loss: 1.1046, data_loss: 1.1046\n",
      "step 3060 , total_loss: 1.1993, data_loss: 1.1993\n",
      "step 3080 , total_loss: 1.0689, data_loss: 1.0689\n",
      "step 3100 , total_loss: 1.1406, data_loss: 1.1406\n",
      "step 3120 , total_loss: 1.1458, data_loss: 1.1458\n",
      "step 3140 , total_loss: 1.1859, data_loss: 1.1859\n",
      "step 3160 , total_loss: 1.1250, data_loss: 1.1250\n",
      "step 3180 , total_loss: 1.1561, data_loss: 1.1561\n",
      "step 3200 , total_loss: 1.1143, data_loss: 1.1143\n",
      "step 3220 , total_loss: 1.0758, data_loss: 1.0758\n",
      "step 3240 , total_loss: 1.0790, data_loss: 1.0790\n",
      "step 3260 , total_loss: 1.1252, data_loss: 1.1252\n",
      "step 3280 , total_loss: 1.1105, data_loss: 1.1105\n",
      "step 3300 , total_loss: 1.1378, data_loss: 1.1378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval valid at epoch 1: auc:0.7983,logloss:0.6678,mean_mrr:0.7267,ndcg@2:0.689,ndcg@4:0.7783,ndcg@6:0.7952,group_auc:0.7905\n",
      "step 20 , total_loss: 1.0315, data_loss: 1.0315\n",
      "step 40 , total_loss: 0.9942, data_loss: 0.9942\n",
      "step 60 , total_loss: 1.0724, data_loss: 1.0724\n",
      "step 80 , total_loss: 1.0451, data_loss: 1.0451\n",
      "step 100 , total_loss: 1.0991, data_loss: 1.0991\n",
      "step 120 , total_loss: 1.0817, data_loss: 1.0817\n",
      "step 140 , total_loss: 1.0639, data_loss: 1.0639\n",
      "step 160 , total_loss: 1.0790, data_loss: 1.0790\n",
      "step 180 , total_loss: 1.0825, data_loss: 1.0825\n",
      "step 200 , total_loss: 1.1222, data_loss: 1.1222\n",
      "step 220 , total_loss: 1.0632, data_loss: 1.0632\n",
      "step 240 , total_loss: 1.0763, data_loss: 1.0763\n",
      "step 260 , total_loss: 1.0823, data_loss: 1.0823\n",
      "step 280 , total_loss: 1.0155, data_loss: 1.0155\n",
      "step 300 , total_loss: 1.0050, data_loss: 1.0050\n",
      "step 320 , total_loss: 1.0620, data_loss: 1.0620\n",
      "step 340 , total_loss: 1.0218, data_loss: 1.0218\n",
      "step 360 , total_loss: 1.1594, data_loss: 1.1594\n",
      "step 380 , total_loss: 1.0421, data_loss: 1.0421\n",
      "step 400 , total_loss: 1.1040, data_loss: 1.1040\n",
      "step 420 , total_loss: 1.2172, data_loss: 1.2172\n",
      "step 440 , total_loss: 1.0923, data_loss: 1.0923\n",
      "step 460 , total_loss: 1.0226, data_loss: 1.0226\n",
      "step 480 , total_loss: 1.1151, data_loss: 1.1151\n",
      "step 500 , total_loss: 1.0847, data_loss: 1.0847\n",
      "step 520 , total_loss: 1.0687, data_loss: 1.0687\n",
      "step 540 , total_loss: 0.9792, data_loss: 0.9792\n",
      "step 560 , total_loss: 1.1046, data_loss: 1.1046\n",
      "step 580 , total_loss: 1.1095, data_loss: 1.1095\n",
      "step 600 , total_loss: 1.1547, data_loss: 1.1547\n",
      "step 620 , total_loss: 1.0997, data_loss: 1.0997\n",
      "step 640 , total_loss: 1.1183, data_loss: 1.1183\n",
      "step 660 , total_loss: 1.0654, data_loss: 1.0654\n",
      "step 680 , total_loss: 0.9812, data_loss: 0.9812\n",
      "step 700 , total_loss: 1.0447, data_loss: 1.0447\n",
      "step 720 , total_loss: 1.0261, data_loss: 1.0261\n",
      "step 740 , total_loss: 1.1054, data_loss: 1.1054\n",
      "step 760 , total_loss: 1.0642, data_loss: 1.0642\n",
      "step 780 , total_loss: 1.0955, data_loss: 1.0955\n",
      "step 800 , total_loss: 1.0244, data_loss: 1.0244\n",
      "step 820 , total_loss: 1.0530, data_loss: 1.0530\n",
      "step 840 , total_loss: 1.0376, data_loss: 1.0376\n",
      "step 860 , total_loss: 1.0470, data_loss: 1.0470\n",
      "step 880 , total_loss: 1.0930, data_loss: 1.0930\n",
      "step 900 , total_loss: 1.0458, data_loss: 1.0458\n",
      "step 920 , total_loss: 1.0986, data_loss: 1.0986\n",
      "step 940 , total_loss: 1.0529, data_loss: 1.0529\n",
      "step 960 , total_loss: 1.0339, data_loss: 1.0339\n",
      "step 980 , total_loss: 1.0158, data_loss: 1.0158\n",
      "step 1000 , total_loss: 1.0883, data_loss: 1.0883\n",
      "step 1020 , total_loss: 1.1345, data_loss: 1.1345\n",
      "step 1040 , total_loss: 1.0525, data_loss: 1.0525\n",
      "step 1060 , total_loss: 1.0690, data_loss: 1.0690\n",
      "step 1080 , total_loss: 1.2080, data_loss: 1.2080\n",
      "step 1100 , total_loss: 1.0359, data_loss: 1.0359\n",
      "step 1120 , total_loss: 0.9843, data_loss: 0.9843\n",
      "step 1140 , total_loss: 1.0355, data_loss: 1.0355\n",
      "step 1160 , total_loss: 1.0733, data_loss: 1.0733\n",
      "step 1180 , total_loss: 1.0744, data_loss: 1.0744\n",
      "step 1200 , total_loss: 1.1064, data_loss: 1.1064\n",
      "step 1220 , total_loss: 1.0379, data_loss: 1.0379\n",
      "step 1240 , total_loss: 1.0907, data_loss: 1.0907\n",
      "step 1260 , total_loss: 1.0487, data_loss: 1.0487\n",
      "step 1280 , total_loss: 1.0005, data_loss: 1.0005\n",
      "step 1300 , total_loss: 1.0740, data_loss: 1.0740\n",
      "step 1320 , total_loss: 1.0754, data_loss: 1.0754\n",
      "step 1340 , total_loss: 0.9928, data_loss: 0.9928\n",
      "step 1360 , total_loss: 0.9813, data_loss: 0.9813\n",
      "step 1380 , total_loss: 1.0757, data_loss: 1.0757\n",
      "step 1400 , total_loss: 1.0692, data_loss: 1.0692\n",
      "step 1420 , total_loss: 0.9979, data_loss: 0.9979\n",
      "step 1440 , total_loss: 0.9719, data_loss: 0.9719\n",
      "step 1460 , total_loss: 1.0976, data_loss: 1.0976\n",
      "step 1480 , total_loss: 1.0137, data_loss: 1.0137\n",
      "step 1500 , total_loss: 1.0576, data_loss: 1.0576\n",
      "step 1520 , total_loss: 1.0153, data_loss: 1.0153\n",
      "step 1540 , total_loss: 1.0608, data_loss: 1.0608\n",
      "step 1560 , total_loss: 1.1621, data_loss: 1.1621\n",
      "step 1580 , total_loss: 1.0799, data_loss: 1.0799\n",
      "step 1600 , total_loss: 1.0554, data_loss: 1.0554\n",
      "step 1620 , total_loss: 1.0380, data_loss: 1.0380\n",
      "step 1640 , total_loss: 1.1377, data_loss: 1.1377\n",
      "step 1660 , total_loss: 1.0150, data_loss: 1.0150\n",
      "step 1680 , total_loss: 0.9728, data_loss: 0.9728\n",
      "step 1700 , total_loss: 1.0713, data_loss: 1.0713\n",
      "step 1720 , total_loss: 1.0477, data_loss: 1.0477\n",
      "step 1740 , total_loss: 1.0319, data_loss: 1.0319\n",
      "step 1760 , total_loss: 0.9830, data_loss: 0.9830\n",
      "step 1780 , total_loss: 1.0470, data_loss: 1.0470\n",
      "step 1800 , total_loss: 1.0116, data_loss: 1.0116\n",
      "step 1820 , total_loss: 1.0311, data_loss: 1.0311\n",
      "step 1840 , total_loss: 1.0482, data_loss: 1.0482\n",
      "step 1860 , total_loss: 0.9400, data_loss: 0.9400\n",
      "step 1880 , total_loss: 1.0951, data_loss: 1.0951\n",
      "step 1900 , total_loss: 1.0543, data_loss: 1.0543\n",
      "step 1920 , total_loss: 1.0266, data_loss: 1.0266\n",
      "step 1940 , total_loss: 1.1016, data_loss: 1.1016\n",
      "step 1960 , total_loss: 1.0128, data_loss: 1.0128\n",
      "step 1980 , total_loss: 1.0713, data_loss: 1.0713\n",
      "step 2000 , total_loss: 1.0207, data_loss: 1.0207\n",
      "step 2020 , total_loss: 1.0563, data_loss: 1.0563\n",
      "step 2040 , total_loss: 1.0450, data_loss: 1.0450\n",
      "step 2060 , total_loss: 1.0959, data_loss: 1.0959\n",
      "step 2080 , total_loss: 0.9749, data_loss: 0.9749\n",
      "step 2100 , total_loss: 1.0128, data_loss: 1.0128\n",
      "step 2120 , total_loss: 1.0232, data_loss: 1.0232\n",
      "step 2140 , total_loss: 1.0316, data_loss: 1.0316\n",
      "step 2160 , total_loss: 1.0320, data_loss: 1.0320\n",
      "step 2180 , total_loss: 1.0679, data_loss: 1.0679\n",
      "step 2200 , total_loss: 1.0874, data_loss: 1.0874\n",
      "step 2220 , total_loss: 1.0157, data_loss: 1.0157\n",
      "step 2240 , total_loss: 0.9812, data_loss: 0.9812\n",
      "step 2260 , total_loss: 1.0184, data_loss: 1.0184\n",
      "step 2280 , total_loss: 1.0063, data_loss: 1.0063\n",
      "step 2300 , total_loss: 1.0536, data_loss: 1.0536\n",
      "step 2320 , total_loss: 0.9998, data_loss: 0.9998\n",
      "step 2340 , total_loss: 1.0411, data_loss: 1.0411\n",
      "step 2360 , total_loss: 1.0857, data_loss: 1.0857\n",
      "step 2380 , total_loss: 0.9514, data_loss: 0.9514\n",
      "step 2400 , total_loss: 1.0413, data_loss: 1.0413\n",
      "step 2420 , total_loss: 0.9666, data_loss: 0.9666\n",
      "step 2440 , total_loss: 1.0160, data_loss: 1.0160\n",
      "step 2460 , total_loss: 1.0142, data_loss: 1.0142\n",
      "step 2480 , total_loss: 0.9936, data_loss: 0.9936\n",
      "step 2500 , total_loss: 1.0862, data_loss: 1.0862\n",
      "step 2520 , total_loss: 0.9860, data_loss: 0.9860\n",
      "step 2540 , total_loss: 1.0464, data_loss: 1.0464\n",
      "step 2560 , total_loss: 0.9869, data_loss: 0.9869\n",
      "step 2580 , total_loss: 1.0923, data_loss: 1.0923\n",
      "step 2600 , total_loss: 1.0205, data_loss: 1.0205\n",
      "step 2620 , total_loss: 1.0634, data_loss: 1.0634\n",
      "step 2640 , total_loss: 1.0270, data_loss: 1.0270\n",
      "step 2660 , total_loss: 1.1057, data_loss: 1.1057\n",
      "step 2680 , total_loss: 1.0126, data_loss: 1.0126\n",
      "step 2700 , total_loss: 0.9863, data_loss: 0.9863\n",
      "step 2720 , total_loss: 1.0921, data_loss: 1.0921\n",
      "step 2740 , total_loss: 1.0307, data_loss: 1.0307\n",
      "step 2760 , total_loss: 0.9976, data_loss: 0.9976\n",
      "step 2780 , total_loss: 1.0495, data_loss: 1.0495\n",
      "step 2800 , total_loss: 1.0882, data_loss: 1.0882\n",
      "step 2820 , total_loss: 1.0333, data_loss: 1.0333\n",
      "step 2840 , total_loss: 1.0630, data_loss: 1.0630\n",
      "step 2860 , total_loss: 0.9865, data_loss: 0.9865\n",
      "step 2880 , total_loss: 1.0310, data_loss: 1.0310\n",
      "step 2900 , total_loss: 1.0344, data_loss: 1.0344\n",
      "step 2920 , total_loss: 1.0759, data_loss: 1.0759\n",
      "step 2940 , total_loss: 1.0574, data_loss: 1.0574\n",
      "step 2960 , total_loss: 0.9981, data_loss: 0.9981\n",
      "step 2980 , total_loss: 1.0175, data_loss: 1.0175\n",
      "step 3000 , total_loss: 0.9990, data_loss: 0.9990\n",
      "step 3020 , total_loss: 0.9831, data_loss: 0.9831\n",
      "step 3040 , total_loss: 0.9982, data_loss: 0.9982\n",
      "step 3060 , total_loss: 1.0027, data_loss: 1.0027\n",
      "step 3080 , total_loss: 1.0276, data_loss: 1.0276\n",
      "step 3100 , total_loss: 1.0384, data_loss: 1.0384\n",
      "step 3120 , total_loss: 0.9846, data_loss: 0.9846\n",
      "step 3140 , total_loss: 1.0308, data_loss: 1.0308\n",
      "step 3160 , total_loss: 1.0424, data_loss: 1.0424\n",
      "step 3180 , total_loss: 1.0354, data_loss: 1.0354\n",
      "step 3200 , total_loss: 1.0535, data_loss: 1.0535\n",
      "step 3220 , total_loss: 1.0018, data_loss: 1.0018\n",
      "step 3240 , total_loss: 0.9500, data_loss: 0.9500\n",
      "step 3260 , total_loss: 1.0643, data_loss: 1.0643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3280 , total_loss: 0.9914, data_loss: 0.9914\n",
      "step 3300 , total_loss: 1.0637, data_loss: 1.0637\n",
      "eval valid at epoch 2: auc:0.8228,logloss:0.7184,mean_mrr:0.757,ndcg@2:0.7264,ndcg@4:0.8045,ndcg@6:0.8181,group_auc:0.8174\n",
      "step 20 , total_loss: 1.0124, data_loss: 1.0124\n",
      "step 40 , total_loss: 0.9652, data_loss: 0.9652\n",
      "step 60 , total_loss: 0.9943, data_loss: 0.9943\n",
      "step 80 , total_loss: 0.9296, data_loss: 0.9296\n",
      "step 100 , total_loss: 0.9176, data_loss: 0.9176\n",
      "step 120 , total_loss: 0.9301, data_loss: 0.9301\n",
      "step 140 , total_loss: 0.9425, data_loss: 0.9425\n",
      "step 160 , total_loss: 0.9963, data_loss: 0.9963\n",
      "step 180 , total_loss: 1.0203, data_loss: 1.0203\n",
      "step 200 , total_loss: 0.9664, data_loss: 0.9664\n",
      "step 220 , total_loss: 0.9034, data_loss: 0.9034\n",
      "step 240 , total_loss: 0.9897, data_loss: 0.9897\n",
      "step 260 , total_loss: 0.8319, data_loss: 0.8319\n",
      "step 280 , total_loss: 0.9449, data_loss: 0.9449\n",
      "step 300 , total_loss: 0.9869, data_loss: 0.9869\n",
      "step 320 , total_loss: 0.8724, data_loss: 0.8724\n",
      "step 340 , total_loss: 0.8960, data_loss: 0.8960\n",
      "step 360 , total_loss: 0.9319, data_loss: 0.9319\n",
      "step 380 , total_loss: 0.9818, data_loss: 0.9818\n",
      "step 400 , total_loss: 0.9099, data_loss: 0.9099\n",
      "step 420 , total_loss: 0.9569, data_loss: 0.9569\n",
      "step 440 , total_loss: 0.9174, data_loss: 0.9174\n",
      "step 460 , total_loss: 1.1065, data_loss: 1.1065\n",
      "step 480 , total_loss: 0.9444, data_loss: 0.9444\n",
      "step 500 , total_loss: 0.9529, data_loss: 0.9529\n",
      "step 520 , total_loss: 0.9514, data_loss: 0.9514\n",
      "step 540 , total_loss: 0.9656, data_loss: 0.9656\n",
      "step 560 , total_loss: 0.9188, data_loss: 0.9188\n",
      "step 580 , total_loss: 1.0447, data_loss: 1.0447\n",
      "step 600 , total_loss: 0.9242, data_loss: 0.9242\n",
      "step 620 , total_loss: 0.9676, data_loss: 0.9676\n",
      "step 640 , total_loss: 0.9606, data_loss: 0.9606\n",
      "step 660 , total_loss: 0.9929, data_loss: 0.9929\n",
      "step 680 , total_loss: 0.9386, data_loss: 0.9386\n",
      "step 700 , total_loss: 0.9496, data_loss: 0.9496\n",
      "step 720 , total_loss: 0.9375, data_loss: 0.9375\n",
      "step 740 , total_loss: 1.0254, data_loss: 1.0254\n",
      "step 760 , total_loss: 0.9100, data_loss: 0.9100\n",
      "step 780 , total_loss: 0.9356, data_loss: 0.9356\n",
      "step 800 , total_loss: 0.9461, data_loss: 0.9461\n",
      "step 820 , total_loss: 0.9764, data_loss: 0.9764\n",
      "step 840 , total_loss: 0.9515, data_loss: 0.9515\n",
      "step 860 , total_loss: 0.9322, data_loss: 0.9322\n",
      "step 880 , total_loss: 0.9781, data_loss: 0.9781\n",
      "step 900 , total_loss: 0.9022, data_loss: 0.9022\n",
      "step 920 , total_loss: 0.9638, data_loss: 0.9638\n",
      "step 940 , total_loss: 0.9691, data_loss: 0.9691\n",
      "step 960 , total_loss: 1.0330, data_loss: 1.0330\n",
      "step 980 , total_loss: 0.9638, data_loss: 0.9638\n",
      "step 1000 , total_loss: 0.9217, data_loss: 0.9217\n",
      "step 1020 , total_loss: 0.9151, data_loss: 0.9151\n",
      "step 1040 , total_loss: 0.9384, data_loss: 0.9384\n",
      "step 1060 , total_loss: 0.9203, data_loss: 0.9203\n",
      "step 1080 , total_loss: 0.9481, data_loss: 0.9481\n",
      "step 1100 , total_loss: 0.9078, data_loss: 0.9078\n",
      "step 1120 , total_loss: 0.9397, data_loss: 0.9397\n",
      "step 1140 , total_loss: 0.9713, data_loss: 0.9713\n",
      "step 1160 , total_loss: 0.9521, data_loss: 0.9521\n",
      "step 1180 , total_loss: 0.9681, data_loss: 0.9681\n",
      "step 1200 , total_loss: 1.0291, data_loss: 1.0291\n",
      "step 1220 , total_loss: 0.8899, data_loss: 0.8899\n",
      "step 1240 , total_loss: 0.9765, data_loss: 0.9765\n",
      "step 1260 , total_loss: 0.8778, data_loss: 0.8778\n",
      "step 1280 , total_loss: 1.0305, data_loss: 1.0305\n",
      "step 1300 , total_loss: 1.0029, data_loss: 1.0029\n",
      "step 1320 , total_loss: 0.9893, data_loss: 0.9893\n",
      "step 1340 , total_loss: 0.9443, data_loss: 0.9443\n",
      "step 1360 , total_loss: 0.9888, data_loss: 0.9888\n",
      "step 1380 , total_loss: 0.9686, data_loss: 0.9686\n",
      "step 1400 , total_loss: 0.9695, data_loss: 0.9695\n",
      "step 1420 , total_loss: 0.9837, data_loss: 0.9837\n",
      "step 1440 , total_loss: 0.9432, data_loss: 0.9432\n",
      "step 1460 , total_loss: 0.9975, data_loss: 0.9975\n",
      "step 1480 , total_loss: 0.9631, data_loss: 0.9631\n",
      "step 1500 , total_loss: 0.9521, data_loss: 0.9521\n",
      "step 1520 , total_loss: 0.9177, data_loss: 0.9177\n",
      "step 1540 , total_loss: 1.0313, data_loss: 1.0313\n",
      "step 1560 , total_loss: 0.9310, data_loss: 0.9310\n",
      "step 1580 , total_loss: 0.9435, data_loss: 0.9435\n",
      "step 1600 , total_loss: 0.9209, data_loss: 0.9209\n",
      "step 1620 , total_loss: 0.9595, data_loss: 0.9595\n",
      "step 1640 , total_loss: 0.9790, data_loss: 0.9790\n",
      "step 1660 , total_loss: 0.9557, data_loss: 0.9557\n",
      "step 1680 , total_loss: 0.9705, data_loss: 0.9705\n",
      "step 1700 , total_loss: 0.9844, data_loss: 0.9844\n",
      "step 1720 , total_loss: 0.9294, data_loss: 0.9294\n",
      "step 1740 , total_loss: 0.8570, data_loss: 0.8570\n",
      "step 1760 , total_loss: 0.9493, data_loss: 0.9493\n",
      "step 1780 , total_loss: 0.9673, data_loss: 0.9673\n",
      "step 1800 , total_loss: 0.8614, data_loss: 0.8614\n",
      "step 1820 , total_loss: 0.9489, data_loss: 0.9489\n",
      "step 1840 , total_loss: 1.0054, data_loss: 1.0054\n",
      "step 1860 , total_loss: 1.0023, data_loss: 1.0023\n",
      "step 1880 , total_loss: 0.9639, data_loss: 0.9639\n",
      "step 1900 , total_loss: 0.9612, data_loss: 0.9612\n",
      "step 1920 , total_loss: 0.8827, data_loss: 0.8827\n",
      "step 1940 , total_loss: 0.9745, data_loss: 0.9745\n",
      "step 1960 , total_loss: 1.0272, data_loss: 1.0272\n",
      "step 1980 , total_loss: 0.9397, data_loss: 0.9397\n",
      "step 2000 , total_loss: 0.9866, data_loss: 0.9866\n",
      "step 2020 , total_loss: 0.9821, data_loss: 0.9821\n",
      "step 2040 , total_loss: 0.9712, data_loss: 0.9712\n",
      "step 2060 , total_loss: 0.8762, data_loss: 0.8762\n",
      "step 2080 , total_loss: 0.9911, data_loss: 0.9911\n",
      "step 2100 , total_loss: 0.9807, data_loss: 0.9807\n",
      "step 2120 , total_loss: 0.9029, data_loss: 0.9029\n",
      "step 2140 , total_loss: 1.0095, data_loss: 1.0095\n",
      "step 2160 , total_loss: 0.9619, data_loss: 0.9619\n",
      "step 2180 , total_loss: 0.9640, data_loss: 0.9640\n",
      "step 2200 , total_loss: 0.9916, data_loss: 0.9916\n",
      "step 2220 , total_loss: 0.8736, data_loss: 0.8736\n",
      "step 2240 , total_loss: 1.0184, data_loss: 1.0184\n",
      "step 2260 , total_loss: 0.9501, data_loss: 0.9501\n",
      "step 2280 , total_loss: 0.9773, data_loss: 0.9773\n",
      "step 2300 , total_loss: 0.9919, data_loss: 0.9919\n",
      "step 2320 , total_loss: 0.8970, data_loss: 0.8970\n",
      "step 2340 , total_loss: 0.9095, data_loss: 0.9095\n",
      "step 2360 , total_loss: 1.0038, data_loss: 1.0038\n",
      "step 2380 , total_loss: 0.9648, data_loss: 0.9648\n",
      "step 2400 , total_loss: 0.9692, data_loss: 0.9692\n",
      "step 2420 , total_loss: 1.0599, data_loss: 1.0599\n",
      "step 2440 , total_loss: 0.9514, data_loss: 0.9514\n",
      "step 2460 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 2480 , total_loss: 0.9214, data_loss: 0.9214\n",
      "step 2500 , total_loss: 1.0107, data_loss: 1.0107\n",
      "step 2520 , total_loss: 0.9429, data_loss: 0.9429\n",
      "step 2540 , total_loss: 0.9614, data_loss: 0.9614\n",
      "step 2560 , total_loss: 0.8958, data_loss: 0.8958\n",
      "step 2580 , total_loss: 0.9119, data_loss: 0.9119\n",
      "step 2600 , total_loss: 0.9563, data_loss: 0.9563\n",
      "step 2620 , total_loss: 0.9404, data_loss: 0.9404\n",
      "step 2640 , total_loss: 0.9212, data_loss: 0.9212\n",
      "step 2660 , total_loss: 0.9428, data_loss: 0.9428\n",
      "step 2680 , total_loss: 1.0929, data_loss: 1.0929\n",
      "step 2700 , total_loss: 0.8705, data_loss: 0.8705\n",
      "step 2720 , total_loss: 0.9919, data_loss: 0.9919\n",
      "step 2740 , total_loss: 0.9375, data_loss: 0.9375\n",
      "step 2760 , total_loss: 0.9939, data_loss: 0.9939\n",
      "step 2780 , total_loss: 0.9219, data_loss: 0.9219\n",
      "step 2800 , total_loss: 0.8992, data_loss: 0.8992\n",
      "step 2820 , total_loss: 1.0381, data_loss: 1.0381\n",
      "step 2840 , total_loss: 0.9375, data_loss: 0.9375\n",
      "step 2860 , total_loss: 0.9791, data_loss: 0.9791\n",
      "step 2880 , total_loss: 1.1014, data_loss: 1.1014\n",
      "step 2900 , total_loss: 0.8910, data_loss: 0.8910\n",
      "step 2920 , total_loss: 0.9711, data_loss: 0.9711\n",
      "step 2940 , total_loss: 0.9629, data_loss: 0.9629\n",
      "step 2960 , total_loss: 0.9685, data_loss: 0.9685\n",
      "step 2980 , total_loss: 0.9186, data_loss: 0.9186\n",
      "step 3000 , total_loss: 0.9496, data_loss: 0.9496\n",
      "step 3020 , total_loss: 0.9936, data_loss: 0.9936\n",
      "step 3040 , total_loss: 0.9667, data_loss: 0.9667\n",
      "step 3060 , total_loss: 0.9305, data_loss: 0.9305\n",
      "step 3080 , total_loss: 0.9791, data_loss: 0.9791\n",
      "step 3100 , total_loss: 0.9379, data_loss: 0.9379\n",
      "step 3120 , total_loss: 0.9844, data_loss: 0.9844\n",
      "step 3140 , total_loss: 0.9732, data_loss: 0.9732\n",
      "step 3160 , total_loss: 0.9905, data_loss: 0.9905\n",
      "step 3180 , total_loss: 0.9522, data_loss: 0.9522\n",
      "step 3200 , total_loss: 0.9318, data_loss: 0.9318\n",
      "step 3220 , total_loss: 0.9713, data_loss: 0.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3240 , total_loss: 0.9778, data_loss: 0.9778\n",
      "step 3260 , total_loss: 1.0067, data_loss: 1.0067\n",
      "step 3280 , total_loss: 0.9838, data_loss: 0.9838\n",
      "step 3300 , total_loss: 0.9762, data_loss: 0.9762\n",
      "eval valid at epoch 3: auc:0.8329,logloss:0.7964,mean_mrr:0.7702,ndcg@2:0.7424,ndcg@4:0.8157,ndcg@6:0.828,group_auc:0.8284\n",
      "step 20 , total_loss: 0.8572, data_loss: 0.8572\n",
      "step 40 , total_loss: 0.8675, data_loss: 0.8675\n",
      "step 60 , total_loss: 0.8927, data_loss: 0.8927\n",
      "step 80 , total_loss: 0.9344, data_loss: 0.9344\n",
      "step 100 , total_loss: 0.9494, data_loss: 0.9494\n",
      "step 120 , total_loss: 0.9282, data_loss: 0.9282\n",
      "step 140 , total_loss: 0.8659, data_loss: 0.8659\n",
      "step 160 , total_loss: 0.8119, data_loss: 0.8119\n",
      "step 180 , total_loss: 0.8190, data_loss: 0.8190\n",
      "step 200 , total_loss: 0.8331, data_loss: 0.8331\n",
      "step 220 , total_loss: 0.8972, data_loss: 0.8972\n",
      "step 240 , total_loss: 0.8250, data_loss: 0.8250\n",
      "step 260 , total_loss: 0.8855, data_loss: 0.8855\n",
      "step 280 , total_loss: 0.9122, data_loss: 0.9122\n",
      "step 300 , total_loss: 0.9571, data_loss: 0.9571\n",
      "step 320 , total_loss: 0.9115, data_loss: 0.9115\n",
      "step 340 , total_loss: 0.8655, data_loss: 0.8655\n",
      "step 360 , total_loss: 0.8522, data_loss: 0.8522\n",
      "step 380 , total_loss: 0.7964, data_loss: 0.7964\n",
      "step 400 , total_loss: 0.9855, data_loss: 0.9855\n",
      "step 420 , total_loss: 0.9636, data_loss: 0.9636\n",
      "step 440 , total_loss: 0.8348, data_loss: 0.8348\n",
      "step 460 , total_loss: 0.8999, data_loss: 0.8999\n",
      "step 480 , total_loss: 0.8644, data_loss: 0.8644\n",
      "step 500 , total_loss: 0.9099, data_loss: 0.9099\n",
      "step 520 , total_loss: 0.8846, data_loss: 0.8846\n",
      "step 540 , total_loss: 0.8586, data_loss: 0.8586\n",
      "step 560 , total_loss: 0.8692, data_loss: 0.8692\n",
      "step 580 , total_loss: 0.8725, data_loss: 0.8725\n",
      "step 600 , total_loss: 0.8950, data_loss: 0.8950\n",
      "step 620 , total_loss: 0.9597, data_loss: 0.9597\n",
      "step 640 , total_loss: 0.8757, data_loss: 0.8757\n",
      "step 660 , total_loss: 0.8708, data_loss: 0.8708\n",
      "step 680 , total_loss: 0.8207, data_loss: 0.8207\n",
      "step 700 , total_loss: 0.9300, data_loss: 0.9300\n",
      "step 720 , total_loss: 0.8634, data_loss: 0.8634\n",
      "step 740 , total_loss: 0.8459, data_loss: 0.8459\n",
      "step 760 , total_loss: 0.9240, data_loss: 0.9240\n",
      "step 780 , total_loss: 0.7686, data_loss: 0.7686\n",
      "step 800 , total_loss: 0.8842, data_loss: 0.8842\n",
      "step 820 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 840 , total_loss: 0.9014, data_loss: 0.9014\n",
      "step 860 , total_loss: 0.8789, data_loss: 0.8789\n",
      "step 880 , total_loss: 0.9039, data_loss: 0.9039\n",
      "step 900 , total_loss: 1.0080, data_loss: 1.0080\n",
      "step 920 , total_loss: 0.8609, data_loss: 0.8609\n",
      "step 940 , total_loss: 0.8197, data_loss: 0.8197\n",
      "step 960 , total_loss: 0.9789, data_loss: 0.9789\n",
      "step 980 , total_loss: 0.9848, data_loss: 0.9848\n",
      "step 1000 , total_loss: 0.8906, data_loss: 0.8906\n",
      "step 1020 , total_loss: 0.9700, data_loss: 0.9700\n",
      "step 1040 , total_loss: 0.8620, data_loss: 0.8620\n",
      "step 1060 , total_loss: 0.9324, data_loss: 0.9324\n",
      "step 1080 , total_loss: 0.8994, data_loss: 0.8994\n",
      "step 1100 , total_loss: 0.9234, data_loss: 0.9234\n",
      "step 1120 , total_loss: 0.8790, data_loss: 0.8790\n",
      "step 1140 , total_loss: 0.8471, data_loss: 0.8471\n",
      "step 1160 , total_loss: 0.8373, data_loss: 0.8373\n",
      "step 1180 , total_loss: 0.9939, data_loss: 0.9939\n",
      "step 1200 , total_loss: 0.8151, data_loss: 0.8151\n",
      "step 1220 , total_loss: 0.9485, data_loss: 0.9485\n",
      "step 1240 , total_loss: 0.9449, data_loss: 0.9449\n",
      "step 1260 , total_loss: 0.9147, data_loss: 0.9147\n",
      "step 1280 , total_loss: 0.8712, data_loss: 0.8712\n",
      "step 1300 , total_loss: 0.9606, data_loss: 0.9606\n",
      "step 1320 , total_loss: 0.8451, data_loss: 0.8451\n",
      "step 1340 , total_loss: 0.9164, data_loss: 0.9164\n",
      "step 1360 , total_loss: 0.9175, data_loss: 0.9175\n",
      "step 1380 , total_loss: 0.9008, data_loss: 0.9008\n",
      "step 1400 , total_loss: 0.9213, data_loss: 0.9213\n",
      "step 1420 , total_loss: 0.9046, data_loss: 0.9046\n",
      "step 1440 , total_loss: 0.9164, data_loss: 0.9164\n",
      "step 1460 , total_loss: 0.9191, data_loss: 0.9191\n",
      "step 1480 , total_loss: 0.8472, data_loss: 0.8472\n",
      "step 1500 , total_loss: 0.9445, data_loss: 0.9445\n",
      "step 1520 , total_loss: 0.8546, data_loss: 0.8546\n",
      "step 1540 , total_loss: 0.8522, data_loss: 0.8522\n",
      "step 1560 , total_loss: 0.9422, data_loss: 0.9422\n",
      "step 1580 , total_loss: 0.9568, data_loss: 0.9568\n",
      "step 1600 , total_loss: 0.8321, data_loss: 0.8321\n",
      "step 1620 , total_loss: 0.9139, data_loss: 0.9139\n",
      "step 1640 , total_loss: 0.9647, data_loss: 0.9647\n",
      "step 1660 , total_loss: 0.8484, data_loss: 0.8484\n",
      "step 1680 , total_loss: 0.8623, data_loss: 0.8623\n",
      "step 1700 , total_loss: 0.8903, data_loss: 0.8903\n",
      "step 1720 , total_loss: 0.9726, data_loss: 0.9726\n",
      "step 1740 , total_loss: 0.9157, data_loss: 0.9157\n",
      "step 1760 , total_loss: 0.9112, data_loss: 0.9112\n",
      "step 1780 , total_loss: 0.9099, data_loss: 0.9099\n",
      "step 1800 , total_loss: 0.7565, data_loss: 0.7565\n",
      "step 1820 , total_loss: 0.8562, data_loss: 0.8562\n",
      "step 1840 , total_loss: 0.8599, data_loss: 0.8599\n",
      "step 1860 , total_loss: 0.9195, data_loss: 0.9195\n",
      "step 1880 , total_loss: 0.8644, data_loss: 0.8644\n",
      "step 1900 , total_loss: 0.9236, data_loss: 0.9236\n",
      "step 1920 , total_loss: 0.8821, data_loss: 0.8821\n",
      "step 1940 , total_loss: 0.9677, data_loss: 0.9677\n",
      "step 1960 , total_loss: 0.8750, data_loss: 0.8750\n",
      "step 1980 , total_loss: 0.8559, data_loss: 0.8559\n",
      "step 2000 , total_loss: 0.9420, data_loss: 0.9420\n",
      "step 2020 , total_loss: 0.9770, data_loss: 0.9770\n",
      "step 2040 , total_loss: 0.9722, data_loss: 0.9722\n",
      "step 2060 , total_loss: 0.8611, data_loss: 0.8611\n",
      "step 2080 , total_loss: 0.9483, data_loss: 0.9483\n",
      "step 2100 , total_loss: 0.9032, data_loss: 0.9032\n",
      "step 2120 , total_loss: 0.9274, data_loss: 0.9274\n",
      "step 2140 , total_loss: 0.8407, data_loss: 0.8407\n",
      "step 2160 , total_loss: 0.9149, data_loss: 0.9149\n",
      "step 2180 , total_loss: 0.9388, data_loss: 0.9388\n",
      "step 2200 , total_loss: 0.8845, data_loss: 0.8845\n",
      "step 2220 , total_loss: 0.7829, data_loss: 0.7829\n",
      "step 2240 , total_loss: 0.9811, data_loss: 0.9811\n",
      "step 2260 , total_loss: 0.8585, data_loss: 0.8585\n",
      "step 2280 , total_loss: 0.8755, data_loss: 0.8755\n",
      "step 2300 , total_loss: 0.9507, data_loss: 0.9507\n",
      "step 2320 , total_loss: 0.8055, data_loss: 0.8055\n",
      "step 2340 , total_loss: 0.9689, data_loss: 0.9689\n",
      "step 2360 , total_loss: 0.8904, data_loss: 0.8904\n",
      "step 2380 , total_loss: 0.9668, data_loss: 0.9668\n",
      "step 2400 , total_loss: 0.9571, data_loss: 0.9571\n",
      "step 2420 , total_loss: 1.0194, data_loss: 1.0194\n",
      "step 2440 , total_loss: 0.8340, data_loss: 0.8340\n",
      "step 2460 , total_loss: 0.8975, data_loss: 0.8975\n",
      "step 2480 , total_loss: 0.9897, data_loss: 0.9897\n",
      "step 2500 , total_loss: 0.8453, data_loss: 0.8453\n",
      "step 2520 , total_loss: 0.9196, data_loss: 0.9196\n",
      "step 2540 , total_loss: 0.8985, data_loss: 0.8985\n",
      "step 2560 , total_loss: 0.9856, data_loss: 0.9856\n",
      "step 2580 , total_loss: 0.8932, data_loss: 0.8932\n",
      "step 2600 , total_loss: 0.8810, data_loss: 0.8810\n",
      "step 2620 , total_loss: 0.9840, data_loss: 0.9840\n",
      "step 2640 , total_loss: 0.9365, data_loss: 0.9365\n",
      "step 2660 , total_loss: 0.8232, data_loss: 0.8232\n",
      "step 2680 , total_loss: 0.9026, data_loss: 0.9026\n",
      "step 2700 , total_loss: 0.9030, data_loss: 0.9030\n",
      "step 2720 , total_loss: 0.9506, data_loss: 0.9506\n",
      "step 2740 , total_loss: 1.0504, data_loss: 1.0504\n",
      "step 2760 , total_loss: 0.9203, data_loss: 0.9203\n",
      "step 2780 , total_loss: 0.9235, data_loss: 0.9235\n",
      "step 2800 , total_loss: 0.9026, data_loss: 0.9026\n",
      "step 2820 , total_loss: 0.9200, data_loss: 0.9200\n",
      "step 2840 , total_loss: 0.9490, data_loss: 0.9490\n",
      "step 2860 , total_loss: 0.9135, data_loss: 0.9135\n",
      "step 2880 , total_loss: 0.8744, data_loss: 0.8744\n",
      "step 2900 , total_loss: 0.8396, data_loss: 0.8396\n",
      "step 2920 , total_loss: 0.9412, data_loss: 0.9412\n",
      "step 2940 , total_loss: 0.8801, data_loss: 0.8801\n",
      "step 2960 , total_loss: 0.8422, data_loss: 0.8422\n",
      "step 2980 , total_loss: 0.9517, data_loss: 0.9517\n",
      "step 3000 , total_loss: 0.9432, data_loss: 0.9432\n",
      "step 3020 , total_loss: 0.8765, data_loss: 0.8765\n",
      "step 3040 , total_loss: 0.8551, data_loss: 0.8551\n",
      "step 3060 , total_loss: 0.9103, data_loss: 0.9103\n",
      "step 3080 , total_loss: 0.9583, data_loss: 0.9583\n",
      "step 3100 , total_loss: 0.9012, data_loss: 0.9012\n",
      "step 3120 , total_loss: 0.8694, data_loss: 0.8694\n",
      "step 3140 , total_loss: 0.9475, data_loss: 0.9475\n",
      "step 3160 , total_loss: 0.8832, data_loss: 0.8832\n",
      "step 3180 , total_loss: 0.8547, data_loss: 0.8547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200 , total_loss: 0.9241, data_loss: 0.9241\n",
      "step 3220 , total_loss: 0.8653, data_loss: 0.8653\n",
      "step 3240 , total_loss: 0.8466, data_loss: 0.8466\n",
      "step 3260 , total_loss: 0.9577, data_loss: 0.9577\n",
      "step 3280 , total_loss: 0.8692, data_loss: 0.8692\n",
      "step 3300 , total_loss: 0.8859, data_loss: 0.8859\n",
      "eval valid at epoch 4: auc:0.8396,logloss:0.9247,mean_mrr:0.779,ndcg@2:0.7533,ndcg@4:0.823,ndcg@6:0.8346,group_auc:0.836\n",
      "step 20 , total_loss: 0.8639, data_loss: 0.8639\n",
      "step 40 , total_loss: 0.9479, data_loss: 0.9479\n",
      "step 60 , total_loss: 0.9017, data_loss: 0.9017\n",
      "step 80 , total_loss: 0.8145, data_loss: 0.8145\n",
      "step 100 , total_loss: 0.9217, data_loss: 0.9217\n",
      "step 120 , total_loss: 0.8001, data_loss: 0.8001\n",
      "step 140 , total_loss: 0.8113, data_loss: 0.8113\n",
      "step 160 , total_loss: 0.9017, data_loss: 0.9017\n",
      "step 180 , total_loss: 0.8206, data_loss: 0.8206\n",
      "step 200 , total_loss: 0.8855, data_loss: 0.8855\n",
      "step 220 , total_loss: 0.8813, data_loss: 0.8813\n",
      "step 240 , total_loss: 0.8391, data_loss: 0.8391\n",
      "step 260 , total_loss: 0.9161, data_loss: 0.9161\n",
      "step 280 , total_loss: 0.8235, data_loss: 0.8235\n",
      "step 300 , total_loss: 0.8475, data_loss: 0.8475\n",
      "step 320 , total_loss: 0.8612, data_loss: 0.8612\n",
      "step 340 , total_loss: 0.8689, data_loss: 0.8689\n",
      "step 360 , total_loss: 0.7522, data_loss: 0.7522\n",
      "step 380 , total_loss: 0.8437, data_loss: 0.8437\n",
      "step 400 , total_loss: 0.8565, data_loss: 0.8565\n",
      "step 420 , total_loss: 0.8712, data_loss: 0.8712\n",
      "step 440 , total_loss: 0.8437, data_loss: 0.8437\n",
      "step 460 , total_loss: 0.8202, data_loss: 0.8202\n",
      "step 480 , total_loss: 0.8814, data_loss: 0.8814\n",
      "step 500 , total_loss: 0.8450, data_loss: 0.8450\n",
      "step 520 , total_loss: 0.8205, data_loss: 0.8205\n",
      "step 540 , total_loss: 0.8809, data_loss: 0.8809\n",
      "step 560 , total_loss: 0.8205, data_loss: 0.8205\n",
      "step 580 , total_loss: 0.8163, data_loss: 0.8163\n",
      "step 600 , total_loss: 0.8756, data_loss: 0.8756\n",
      "step 620 , total_loss: 0.7597, data_loss: 0.7597\n",
      "step 640 , total_loss: 0.8274, data_loss: 0.8274\n",
      "step 660 , total_loss: 0.8064, data_loss: 0.8064\n",
      "step 680 , total_loss: 0.9299, data_loss: 0.9299\n",
      "step 700 , total_loss: 0.8707, data_loss: 0.8707\n",
      "step 720 , total_loss: 0.9094, data_loss: 0.9094\n",
      "step 740 , total_loss: 0.9333, data_loss: 0.9333\n",
      "step 760 , total_loss: 0.7850, data_loss: 0.7850\n",
      "step 780 , total_loss: 0.8653, data_loss: 0.8653\n",
      "step 800 , total_loss: 0.8990, data_loss: 0.8990\n",
      "step 820 , total_loss: 0.8259, data_loss: 0.8259\n",
      "step 840 , total_loss: 0.8588, data_loss: 0.8588\n",
      "step 860 , total_loss: 0.9188, data_loss: 0.9188\n",
      "step 880 , total_loss: 0.8507, data_loss: 0.8507\n",
      "step 900 , total_loss: 0.9250, data_loss: 0.9250\n",
      "step 920 , total_loss: 0.7678, data_loss: 0.7678\n",
      "step 940 , total_loss: 0.8221, data_loss: 0.8221\n",
      "step 960 , total_loss: 0.7784, data_loss: 0.7784\n",
      "step 980 , total_loss: 0.8347, data_loss: 0.8347\n",
      "step 1000 , total_loss: 0.8930, data_loss: 0.8930\n",
      "step 1020 , total_loss: 0.8093, data_loss: 0.8093\n",
      "step 1040 , total_loss: 0.9050, data_loss: 0.9050\n",
      "step 1060 , total_loss: 0.8565, data_loss: 0.8565\n",
      "step 1080 , total_loss: 0.8513, data_loss: 0.8513\n",
      "step 1100 , total_loss: 0.9069, data_loss: 0.9069\n",
      "step 1120 , total_loss: 0.7821, data_loss: 0.7821\n",
      "step 1140 , total_loss: 0.9248, data_loss: 0.9248\n",
      "step 1160 , total_loss: 0.8850, data_loss: 0.8850\n",
      "step 1180 , total_loss: 0.8243, data_loss: 0.8243\n",
      "step 1200 , total_loss: 0.8754, data_loss: 0.8754\n",
      "step 1220 , total_loss: 0.7830, data_loss: 0.7830\n",
      "step 1240 , total_loss: 0.9200, data_loss: 0.9200\n",
      "step 1260 , total_loss: 0.8013, data_loss: 0.8013\n",
      "step 1280 , total_loss: 0.8562, data_loss: 0.8562\n",
      "step 1300 , total_loss: 0.7828, data_loss: 0.7828\n",
      "step 1320 , total_loss: 0.8409, data_loss: 0.8409\n",
      "step 1340 , total_loss: 0.8330, data_loss: 0.8330\n",
      "step 1360 , total_loss: 0.9176, data_loss: 0.9176\n",
      "step 1380 , total_loss: 0.8849, data_loss: 0.8849\n",
      "step 1400 , total_loss: 0.8266, data_loss: 0.8266\n",
      "step 1420 , total_loss: 0.8957, data_loss: 0.8957\n",
      "step 1440 , total_loss: 0.8415, data_loss: 0.8415\n",
      "step 1460 , total_loss: 0.8000, data_loss: 0.8000\n",
      "step 1480 , total_loss: 0.9255, data_loss: 0.9255\n",
      "step 1500 , total_loss: 0.7916, data_loss: 0.7916\n",
      "step 1520 , total_loss: 0.8132, data_loss: 0.8132\n",
      "step 1540 , total_loss: 0.7644, data_loss: 0.7644\n",
      "step 1560 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 1580 , total_loss: 0.8966, data_loss: 0.8966\n",
      "step 1600 , total_loss: 0.8669, data_loss: 0.8669\n",
      "step 1620 , total_loss: 0.9817, data_loss: 0.9817\n",
      "step 1640 , total_loss: 0.8524, data_loss: 0.8524\n",
      "step 1660 , total_loss: 0.8431, data_loss: 0.8431\n",
      "step 1680 , total_loss: 0.9218, data_loss: 0.9218\n",
      "step 1700 , total_loss: 0.8272, data_loss: 0.8272\n",
      "step 1720 , total_loss: 0.8372, data_loss: 0.8372\n",
      "step 1740 , total_loss: 0.9090, data_loss: 0.9090\n",
      "step 1760 , total_loss: 0.9083, data_loss: 0.9083\n",
      "step 1780 , total_loss: 0.8465, data_loss: 0.8465\n",
      "step 1800 , total_loss: 0.8560, data_loss: 0.8560\n",
      "step 1820 , total_loss: 0.9846, data_loss: 0.9846\n",
      "step 1840 , total_loss: 0.8285, data_loss: 0.8285\n",
      "step 1860 , total_loss: 0.8149, data_loss: 0.8149\n",
      "step 1880 , total_loss: 0.8578, data_loss: 0.8578\n",
      "step 1900 , total_loss: 0.8484, data_loss: 0.8484\n",
      "step 1920 , total_loss: 0.8415, data_loss: 0.8415\n",
      "step 1940 , total_loss: 0.8607, data_loss: 0.8607\n",
      "step 1960 , total_loss: 0.7871, data_loss: 0.7871\n",
      "step 1980 , total_loss: 0.8113, data_loss: 0.8113\n",
      "step 2000 , total_loss: 0.8475, data_loss: 0.8475\n",
      "step 2020 , total_loss: 0.8657, data_loss: 0.8657\n",
      "step 2040 , total_loss: 0.8369, data_loss: 0.8369\n",
      "step 2060 , total_loss: 0.8483, data_loss: 0.8483\n",
      "step 2080 , total_loss: 0.8110, data_loss: 0.8110\n",
      "step 2100 , total_loss: 0.8357, data_loss: 0.8357\n",
      "step 2120 , total_loss: 0.8821, data_loss: 0.8821\n",
      "step 2140 , total_loss: 0.7680, data_loss: 0.7680\n",
      "step 2160 , total_loss: 0.8988, data_loss: 0.8988\n",
      "step 2180 , total_loss: 0.8657, data_loss: 0.8657\n",
      "step 2200 , total_loss: 0.9148, data_loss: 0.9148\n",
      "step 2220 , total_loss: 0.8622, data_loss: 0.8622\n",
      "step 2240 , total_loss: 0.9468, data_loss: 0.9468\n",
      "step 2260 , total_loss: 0.8537, data_loss: 0.8537\n",
      "step 2280 , total_loss: 0.8722, data_loss: 0.8722\n",
      "step 2300 , total_loss: 0.9416, data_loss: 0.9416\n",
      "step 2320 , total_loss: 0.7874, data_loss: 0.7874\n",
      "step 2340 , total_loss: 0.8719, data_loss: 0.8719\n",
      "step 2360 , total_loss: 0.8681, data_loss: 0.8681\n",
      "step 2380 , total_loss: 0.7990, data_loss: 0.7990\n",
      "step 2400 , total_loss: 0.9026, data_loss: 0.9026\n",
      "step 2420 , total_loss: 0.8031, data_loss: 0.8031\n",
      "step 2440 , total_loss: 0.8287, data_loss: 0.8287\n",
      "step 2460 , total_loss: 0.8343, data_loss: 0.8343\n",
      "step 2480 , total_loss: 0.8244, data_loss: 0.8244\n",
      "step 2500 , total_loss: 0.8890, data_loss: 0.8890\n",
      "step 2520 , total_loss: 0.8897, data_loss: 0.8897\n",
      "step 2540 , total_loss: 0.8398, data_loss: 0.8398\n",
      "step 2560 , total_loss: 0.8917, data_loss: 0.8917\n",
      "step 2580 , total_loss: 0.7989, data_loss: 0.7989\n",
      "step 2600 , total_loss: 0.8575, data_loss: 0.8575\n",
      "step 2620 , total_loss: 0.8809, data_loss: 0.8809\n",
      "step 2640 , total_loss: 0.8514, data_loss: 0.8514\n",
      "step 2660 , total_loss: 0.8826, data_loss: 0.8826\n",
      "step 2680 , total_loss: 0.8247, data_loss: 0.8247\n",
      "step 2700 , total_loss: 0.8601, data_loss: 0.8601\n",
      "step 2720 , total_loss: 0.8510, data_loss: 0.8510\n",
      "step 2740 , total_loss: 0.9012, data_loss: 0.9012\n",
      "step 2760 , total_loss: 0.8829, data_loss: 0.8829\n",
      "step 2780 , total_loss: 0.8601, data_loss: 0.8601\n",
      "step 2800 , total_loss: 0.8926, data_loss: 0.8926\n",
      "step 2820 , total_loss: 0.9069, data_loss: 0.9069\n",
      "step 2840 , total_loss: 0.8573, data_loss: 0.8573\n",
      "step 2860 , total_loss: 0.8666, data_loss: 0.8666\n",
      "step 2880 , total_loss: 0.8459, data_loss: 0.8459\n",
      "step 2900 , total_loss: 0.8743, data_loss: 0.8743\n",
      "step 2920 , total_loss: 0.8051, data_loss: 0.8051\n",
      "step 2940 , total_loss: 0.8877, data_loss: 0.8877\n",
      "step 2960 , total_loss: 0.8739, data_loss: 0.8739\n",
      "step 2980 , total_loss: 0.9031, data_loss: 0.9031\n",
      "step 3000 , total_loss: 0.9232, data_loss: 0.9232\n",
      "step 3020 , total_loss: 0.9479, data_loss: 0.9479\n",
      "step 3040 , total_loss: 0.9060, data_loss: 0.9060\n",
      "step 3060 , total_loss: 0.8503, data_loss: 0.8503\n",
      "step 3080 , total_loss: 0.9209, data_loss: 0.9209\n",
      "step 3100 , total_loss: 0.8946, data_loss: 0.8946\n",
      "step 3120 , total_loss: 0.7808, data_loss: 0.7808\n",
      "step 3140 , total_loss: 0.8893, data_loss: 0.8893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3160 , total_loss: 0.8941, data_loss: 0.8941\n",
      "step 3180 , total_loss: 0.9281, data_loss: 0.9281\n",
      "step 3200 , total_loss: 0.8230, data_loss: 0.8230\n",
      "step 3220 , total_loss: 0.8721, data_loss: 0.8721\n",
      "step 3240 , total_loss: 0.8728, data_loss: 0.8728\n",
      "step 3260 , total_loss: 0.7441, data_loss: 0.7441\n",
      "step 3280 , total_loss: 0.8747, data_loss: 0.8747\n",
      "step 3300 , total_loss: 0.8898, data_loss: 0.8898\n",
      "eval valid at epoch 5: auc:0.8429,logloss:0.9989,mean_mrr:0.7835,ndcg@2:0.7592,ndcg@4:0.8269,ndcg@6:0.838,group_auc:0.84\n",
      "step 20 , total_loss: 0.8156, data_loss: 0.8156\n",
      "step 40 , total_loss: 0.7655, data_loss: 0.7655\n",
      "step 60 , total_loss: 0.8024, data_loss: 0.8024\n",
      "step 80 , total_loss: 0.8035, data_loss: 0.8035\n",
      "step 100 , total_loss: 0.8036, data_loss: 0.8036\n",
      "step 120 , total_loss: 0.8275, data_loss: 0.8275\n",
      "step 140 , total_loss: 0.8370, data_loss: 0.8370\n",
      "step 160 , total_loss: 0.8431, data_loss: 0.8431\n",
      "step 180 , total_loss: 0.8205, data_loss: 0.8205\n",
      "step 200 , total_loss: 0.7937, data_loss: 0.7937\n",
      "step 220 , total_loss: 0.8075, data_loss: 0.8075\n",
      "step 240 , total_loss: 0.8690, data_loss: 0.8690\n",
      "step 260 , total_loss: 0.8521, data_loss: 0.8521\n",
      "step 280 , total_loss: 0.7297, data_loss: 0.7297\n",
      "step 300 , total_loss: 0.8978, data_loss: 0.8978\n",
      "step 320 , total_loss: 0.8461, data_loss: 0.8461\n",
      "step 340 , total_loss: 0.8276, data_loss: 0.8276\n",
      "step 360 , total_loss: 0.7503, data_loss: 0.7503\n",
      "step 380 , total_loss: 0.7078, data_loss: 0.7078\n",
      "step 400 , total_loss: 0.8361, data_loss: 0.8361\n",
      "step 420 , total_loss: 0.7319, data_loss: 0.7319\n",
      "step 440 , total_loss: 0.8528, data_loss: 0.8528\n",
      "step 460 , total_loss: 0.7760, data_loss: 0.7760\n",
      "step 480 , total_loss: 0.7120, data_loss: 0.7120\n",
      "step 500 , total_loss: 0.7508, data_loss: 0.7508\n",
      "step 520 , total_loss: 0.8179, data_loss: 0.8179\n",
      "step 540 , total_loss: 0.7809, data_loss: 0.7809\n",
      "step 560 , total_loss: 0.7535, data_loss: 0.7535\n",
      "step 580 , total_loss: 0.7718, data_loss: 0.7718\n",
      "step 600 , total_loss: 0.8444, data_loss: 0.8444\n",
      "step 620 , total_loss: 0.7921, data_loss: 0.7921\n",
      "step 640 , total_loss: 0.8874, data_loss: 0.8874\n",
      "step 660 , total_loss: 0.8593, data_loss: 0.8593\n",
      "step 680 , total_loss: 0.8085, data_loss: 0.8085\n",
      "step 700 , total_loss: 0.8573, data_loss: 0.8573\n",
      "step 720 , total_loss: 0.8215, data_loss: 0.8215\n",
      "step 740 , total_loss: 0.8096, data_loss: 0.8096\n",
      "step 760 , total_loss: 0.7417, data_loss: 0.7417\n",
      "step 780 , total_loss: 0.8113, data_loss: 0.8113\n",
      "step 800 , total_loss: 0.8099, data_loss: 0.8099\n",
      "step 820 , total_loss: 0.8497, data_loss: 0.8497\n",
      "step 840 , total_loss: 0.8555, data_loss: 0.8555\n",
      "step 860 , total_loss: 0.8260, data_loss: 0.8260\n",
      "step 880 , total_loss: 0.7700, data_loss: 0.7700\n",
      "step 900 , total_loss: 0.7519, data_loss: 0.7519\n",
      "step 920 , total_loss: 0.8289, data_loss: 0.8289\n",
      "step 940 , total_loss: 0.8216, data_loss: 0.8216\n",
      "step 960 , total_loss: 0.8262, data_loss: 0.8262\n",
      "step 980 , total_loss: 0.9107, data_loss: 0.9107\n",
      "step 1000 , total_loss: 0.8217, data_loss: 0.8217\n",
      "step 1020 , total_loss: 0.8945, data_loss: 0.8945\n",
      "step 1040 , total_loss: 0.8983, data_loss: 0.8983\n",
      "step 1060 , total_loss: 0.8098, data_loss: 0.8098\n",
      "step 1080 , total_loss: 0.8697, data_loss: 0.8697\n",
      "step 1100 , total_loss: 0.7983, data_loss: 0.7983\n",
      "step 1120 , total_loss: 0.8163, data_loss: 0.8163\n",
      "step 1140 , total_loss: 0.7790, data_loss: 0.7790\n",
      "step 1160 , total_loss: 0.7737, data_loss: 0.7737\n",
      "step 1180 , total_loss: 0.8377, data_loss: 0.8377\n",
      "step 1200 , total_loss: 0.7650, data_loss: 0.7650\n",
      "step 1220 , total_loss: 0.8696, data_loss: 0.8696\n",
      "step 1240 , total_loss: 0.7496, data_loss: 0.7496\n",
      "step 1260 , total_loss: 0.8184, data_loss: 0.8184\n",
      "step 1280 , total_loss: 0.7911, data_loss: 0.7911\n",
      "step 1300 , total_loss: 0.8245, data_loss: 0.8245\n",
      "step 1320 , total_loss: 0.8026, data_loss: 0.8026\n",
      "step 1340 , total_loss: 0.8121, data_loss: 0.8121\n",
      "step 1360 , total_loss: 0.7798, data_loss: 0.7798\n",
      "step 1380 , total_loss: 0.8789, data_loss: 0.8789\n",
      "step 1400 , total_loss: 0.8189, data_loss: 0.8189\n",
      "step 1420 , total_loss: 0.8519, data_loss: 0.8519\n",
      "step 1440 , total_loss: 0.8322, data_loss: 0.8322\n",
      "step 1460 , total_loss: 0.8198, data_loss: 0.8198\n",
      "step 1480 , total_loss: 0.8020, data_loss: 0.8020\n",
      "step 1500 , total_loss: 0.8703, data_loss: 0.8703\n",
      "step 1520 , total_loss: 0.8060, data_loss: 0.8060\n",
      "step 1540 , total_loss: 0.9201, data_loss: 0.9201\n",
      "step 1560 , total_loss: 0.7522, data_loss: 0.7522\n",
      "step 1580 , total_loss: 0.9184, data_loss: 0.9184\n",
      "step 1600 , total_loss: 0.8227, data_loss: 0.8227\n",
      "step 1620 , total_loss: 0.8592, data_loss: 0.8592\n",
      "step 1640 , total_loss: 0.8161, data_loss: 0.8161\n",
      "step 1660 , total_loss: 0.8831, data_loss: 0.8831\n",
      "step 1680 , total_loss: 0.9173, data_loss: 0.9173\n",
      "step 1700 , total_loss: 0.8000, data_loss: 0.8000\n",
      "step 1720 , total_loss: 0.8427, data_loss: 0.8427\n",
      "step 1740 , total_loss: 0.8264, data_loss: 0.8264\n",
      "step 1760 , total_loss: 0.7651, data_loss: 0.7651\n",
      "step 1780 , total_loss: 0.8120, data_loss: 0.8120\n",
      "step 1800 , total_loss: 0.8646, data_loss: 0.8646\n",
      "step 1820 , total_loss: 0.8740, data_loss: 0.8740\n",
      "step 1840 , total_loss: 0.8212, data_loss: 0.8212\n",
      "step 1860 , total_loss: 0.8254, data_loss: 0.8254\n",
      "step 1880 , total_loss: 0.8031, data_loss: 0.8031\n",
      "step 1900 , total_loss: 0.7772, data_loss: 0.7772\n",
      "step 1920 , total_loss: 0.9091, data_loss: 0.9091\n",
      "step 1940 , total_loss: 0.8445, data_loss: 0.8445\n",
      "step 1960 , total_loss: 0.9061, data_loss: 0.9061\n",
      "step 1980 , total_loss: 0.7640, data_loss: 0.7640\n",
      "step 2000 , total_loss: 0.8205, data_loss: 0.8205\n",
      "step 2020 , total_loss: 0.8122, data_loss: 0.8122\n",
      "step 2040 , total_loss: 0.8320, data_loss: 0.8320\n",
      "step 2060 , total_loss: 0.8420, data_loss: 0.8420\n",
      "step 2080 , total_loss: 0.8030, data_loss: 0.8030\n",
      "step 2100 , total_loss: 0.8551, data_loss: 0.8551\n",
      "step 2120 , total_loss: 0.8217, data_loss: 0.8217\n",
      "step 2140 , total_loss: 0.7656, data_loss: 0.7656\n",
      "step 2160 , total_loss: 0.8896, data_loss: 0.8896\n",
      "step 2180 , total_loss: 0.8951, data_loss: 0.8951\n",
      "step 2200 , total_loss: 0.8106, data_loss: 0.8106\n",
      "step 2220 , total_loss: 0.8037, data_loss: 0.8037\n",
      "step 2240 , total_loss: 0.8199, data_loss: 0.8199\n",
      "step 2260 , total_loss: 0.8376, data_loss: 0.8376\n",
      "step 2280 , total_loss: 0.8027, data_loss: 0.8027\n",
      "step 2300 , total_loss: 0.8819, data_loss: 0.8819\n",
      "step 2320 , total_loss: 0.7928, data_loss: 0.7928\n",
      "step 2340 , total_loss: 0.9207, data_loss: 0.9207\n",
      "step 2360 , total_loss: 0.8240, data_loss: 0.8240\n",
      "step 2380 , total_loss: 0.9183, data_loss: 0.9183\n",
      "step 2400 , total_loss: 0.8799, data_loss: 0.8799\n",
      "step 2420 , total_loss: 0.8845, data_loss: 0.8845\n",
      "step 2440 , total_loss: 0.8191, data_loss: 0.8191\n",
      "step 2460 , total_loss: 0.7887, data_loss: 0.7887\n",
      "step 2480 , total_loss: 0.8172, data_loss: 0.8172\n",
      "step 2500 , total_loss: 0.7818, data_loss: 0.7818\n",
      "step 2520 , total_loss: 0.7994, data_loss: 0.7994\n",
      "step 2540 , total_loss: 0.9055, data_loss: 0.9055\n",
      "step 2560 , total_loss: 0.8139, data_loss: 0.8139\n",
      "step 2580 , total_loss: 0.8655, data_loss: 0.8655\n",
      "step 2600 , total_loss: 0.8481, data_loss: 0.8481\n",
      "step 2620 , total_loss: 0.9047, data_loss: 0.9047\n",
      "step 2640 , total_loss: 0.8992, data_loss: 0.8992\n",
      "step 2660 , total_loss: 0.9076, data_loss: 0.9076\n",
      "step 2680 , total_loss: 0.9545, data_loss: 0.9545\n",
      "step 2700 , total_loss: 0.8161, data_loss: 0.8161\n",
      "step 2720 , total_loss: 0.8598, data_loss: 0.8598\n",
      "step 2740 , total_loss: 0.8460, data_loss: 0.8460\n",
      "step 2760 , total_loss: 0.8355, data_loss: 0.8355\n",
      "step 2780 , total_loss: 0.8828, data_loss: 0.8828\n",
      "step 2800 , total_loss: 0.7780, data_loss: 0.7780\n",
      "step 2820 , total_loss: 0.9329, data_loss: 0.9329\n",
      "step 2840 , total_loss: 0.7701, data_loss: 0.7701\n",
      "step 2860 , total_loss: 0.7834, data_loss: 0.7834\n",
      "step 2880 , total_loss: 0.7900, data_loss: 0.7900\n",
      "step 2900 , total_loss: 0.8150, data_loss: 0.8150\n",
      "step 2920 , total_loss: 0.8317, data_loss: 0.8317\n",
      "step 2940 , total_loss: 0.7893, data_loss: 0.7893\n",
      "step 2960 , total_loss: 0.8248, data_loss: 0.8248\n",
      "step 2980 , total_loss: 0.8704, data_loss: 0.8704\n",
      "step 3000 , total_loss: 0.8250, data_loss: 0.8250\n",
      "step 3020 , total_loss: 0.8631, data_loss: 0.8631\n",
      "step 3040 , total_loss: 0.9049, data_loss: 0.9049\n",
      "step 3060 , total_loss: 0.8352, data_loss: 0.8352\n",
      "step 3080 , total_loss: 0.8432, data_loss: 0.8432\n",
      "step 3100 , total_loss: 0.8589, data_loss: 0.8589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3120 , total_loss: 0.8602, data_loss: 0.8602\n",
      "step 3140 , total_loss: 0.7936, data_loss: 0.7936\n",
      "step 3160 , total_loss: 0.8340, data_loss: 0.8340\n",
      "step 3180 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 3200 , total_loss: 0.9294, data_loss: 0.9294\n",
      "step 3220 , total_loss: 0.8486, data_loss: 0.8486\n",
      "step 3240 , total_loss: 0.7522, data_loss: 0.7522\n",
      "step 3260 , total_loss: 0.8190, data_loss: 0.8190\n",
      "step 3280 , total_loss: 0.8799, data_loss: 0.8799\n",
      "step 3300 , total_loss: 0.7053, data_loss: 0.7053\n",
      "eval valid at epoch 6: auc:0.8448,logloss:1.0981,mean_mrr:0.7864,ndcg@2:0.7617,ndcg@4:0.8291,ndcg@6:0.8402,group_auc:0.8418\n",
      "step 20 , total_loss: 0.7282, data_loss: 0.7282\n",
      "step 40 , total_loss: 0.6814, data_loss: 0.6814\n",
      "step 60 , total_loss: 0.7712, data_loss: 0.7712\n",
      "step 80 , total_loss: 0.7693, data_loss: 0.7693\n",
      "step 100 , total_loss: 0.9051, data_loss: 0.9051\n",
      "step 120 , total_loss: 0.8627, data_loss: 0.8627\n",
      "step 140 , total_loss: 0.7383, data_loss: 0.7383\n",
      "step 160 , total_loss: 0.7199, data_loss: 0.7199\n",
      "step 180 , total_loss: 0.8712, data_loss: 0.8712\n",
      "step 200 , total_loss: 0.8080, data_loss: 0.8080\n",
      "step 220 , total_loss: 0.7899, data_loss: 0.7899\n",
      "step 240 , total_loss: 0.8296, data_loss: 0.8296\n",
      "step 260 , total_loss: 0.7331, data_loss: 0.7331\n",
      "step 280 , total_loss: 0.8326, data_loss: 0.8326\n",
      "step 300 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 320 , total_loss: 0.7808, data_loss: 0.7808\n",
      "step 340 , total_loss: 0.7846, data_loss: 0.7846\n",
      "step 360 , total_loss: 0.7419, data_loss: 0.7419\n",
      "step 380 , total_loss: 0.7738, data_loss: 0.7738\n",
      "step 400 , total_loss: 0.7566, data_loss: 0.7566\n",
      "step 420 , total_loss: 0.8129, data_loss: 0.8129\n",
      "step 440 , total_loss: 0.7874, data_loss: 0.7874\n",
      "step 460 , total_loss: 0.7150, data_loss: 0.7150\n",
      "step 480 , total_loss: 0.8362, data_loss: 0.8362\n",
      "step 500 , total_loss: 0.8933, data_loss: 0.8933\n",
      "step 520 , total_loss: 0.8049, data_loss: 0.8049\n",
      "step 540 , total_loss: 0.8066, data_loss: 0.8066\n",
      "step 560 , total_loss: 0.7667, data_loss: 0.7667\n",
      "step 580 , total_loss: 0.8073, data_loss: 0.8073\n",
      "step 600 , total_loss: 0.7366, data_loss: 0.7366\n",
      "step 620 , total_loss: 0.7950, data_loss: 0.7950\n",
      "step 640 , total_loss: 0.8019, data_loss: 0.8019\n",
      "step 660 , total_loss: 0.7807, data_loss: 0.7807\n",
      "step 680 , total_loss: 0.8071, data_loss: 0.8071\n",
      "step 700 , total_loss: 0.8847, data_loss: 0.8847\n",
      "step 720 , total_loss: 0.8170, data_loss: 0.8170\n",
      "step 740 , total_loss: 0.7291, data_loss: 0.7291\n",
      "step 760 , total_loss: 0.7727, data_loss: 0.7727\n",
      "step 780 , total_loss: 0.7674, data_loss: 0.7674\n",
      "step 800 , total_loss: 0.7911, data_loss: 0.7911\n",
      "step 820 , total_loss: 0.8851, data_loss: 0.8851\n",
      "step 840 , total_loss: 0.7861, data_loss: 0.7861\n",
      "step 860 , total_loss: 0.7573, data_loss: 0.7573\n",
      "step 880 , total_loss: 0.7908, data_loss: 0.7908\n",
      "step 900 , total_loss: 0.6883, data_loss: 0.6883\n",
      "step 920 , total_loss: 0.7708, data_loss: 0.7708\n",
      "step 940 , total_loss: 0.8496, data_loss: 0.8496\n",
      "step 960 , total_loss: 0.8493, data_loss: 0.8493\n",
      "step 980 , total_loss: 0.7910, data_loss: 0.7910\n",
      "step 1000 , total_loss: 0.8446, data_loss: 0.8446\n",
      "step 1020 , total_loss: 0.9206, data_loss: 0.9206\n",
      "step 1040 , total_loss: 0.7727, data_loss: 0.7727\n",
      "step 1060 , total_loss: 0.8147, data_loss: 0.8147\n",
      "step 1080 , total_loss: 0.7397, data_loss: 0.7397\n",
      "step 1100 , total_loss: 0.7980, data_loss: 0.7980\n",
      "step 1120 , total_loss: 0.8107, data_loss: 0.8107\n",
      "step 1140 , total_loss: 0.7401, data_loss: 0.7401\n",
      "step 1160 , total_loss: 0.7290, data_loss: 0.7290\n",
      "step 1180 , total_loss: 0.7773, data_loss: 0.7773\n",
      "step 1200 , total_loss: 0.8800, data_loss: 0.8800\n",
      "step 1220 , total_loss: 0.8245, data_loss: 0.8245\n",
      "step 1240 , total_loss: 0.8493, data_loss: 0.8493\n",
      "step 1260 , total_loss: 0.8545, data_loss: 0.8545\n",
      "step 1280 , total_loss: 0.8009, data_loss: 0.8009\n",
      "step 1300 , total_loss: 0.7778, data_loss: 0.7778\n",
      "step 1320 , total_loss: 0.8268, data_loss: 0.8268\n",
      "step 1340 , total_loss: 0.8066, data_loss: 0.8066\n",
      "step 1360 , total_loss: 0.8353, data_loss: 0.8353\n",
      "step 1380 , total_loss: 0.8248, data_loss: 0.8248\n",
      "step 1400 , total_loss: 0.6998, data_loss: 0.6998\n",
      "step 1420 , total_loss: 0.7407, data_loss: 0.7407\n",
      "step 1440 , total_loss: 0.8661, data_loss: 0.8661\n",
      "step 1460 , total_loss: 0.7702, data_loss: 0.7702\n",
      "step 1480 , total_loss: 0.7540, data_loss: 0.7540\n",
      "step 1500 , total_loss: 0.8527, data_loss: 0.8527\n",
      "step 1520 , total_loss: 0.7608, data_loss: 0.7608\n",
      "step 1540 , total_loss: 0.8035, data_loss: 0.8035\n",
      "step 1560 , total_loss: 0.8936, data_loss: 0.8936\n",
      "step 1580 , total_loss: 0.7168, data_loss: 0.7168\n",
      "step 1600 , total_loss: 0.8239, data_loss: 0.8239\n",
      "step 1620 , total_loss: 0.8454, data_loss: 0.8454\n",
      "step 1640 , total_loss: 0.8096, data_loss: 0.8096\n",
      "step 1660 , total_loss: 0.8522, data_loss: 0.8522\n",
      "step 1680 , total_loss: 0.8184, data_loss: 0.8184\n",
      "step 1700 , total_loss: 0.8252, data_loss: 0.8252\n",
      "step 1720 , total_loss: 0.8057, data_loss: 0.8057\n",
      "step 1740 , total_loss: 0.7361, data_loss: 0.7361\n",
      "step 1760 , total_loss: 0.7635, data_loss: 0.7635\n",
      "step 1780 , total_loss: 0.7598, data_loss: 0.7598\n",
      "step 1800 , total_loss: 0.7809, data_loss: 0.7809\n",
      "step 1820 , total_loss: 0.8881, data_loss: 0.8881\n",
      "step 1840 , total_loss: 0.8336, data_loss: 0.8336\n",
      "step 1860 , total_loss: 0.7678, data_loss: 0.7678\n",
      "step 1880 , total_loss: 0.7967, data_loss: 0.7967\n",
      "step 1900 , total_loss: 0.7543, data_loss: 0.7543\n",
      "step 1920 , total_loss: 0.8356, data_loss: 0.8356\n",
      "step 1940 , total_loss: 0.8243, data_loss: 0.8243\n",
      "step 1960 , total_loss: 0.6801, data_loss: 0.6801\n",
      "step 1980 , total_loss: 0.8635, data_loss: 0.8635\n",
      "step 2000 , total_loss: 0.7862, data_loss: 0.7862\n",
      "step 2020 , total_loss: 0.8206, data_loss: 0.8206\n",
      "step 2040 , total_loss: 0.8645, data_loss: 0.8645\n",
      "step 2060 , total_loss: 0.8612, data_loss: 0.8612\n",
      "step 2080 , total_loss: 0.7660, data_loss: 0.7660\n",
      "step 2100 , total_loss: 0.7831, data_loss: 0.7831\n",
      "step 2120 , total_loss: 0.7823, data_loss: 0.7823\n",
      "step 2140 , total_loss: 0.8502, data_loss: 0.8502\n",
      "step 2160 , total_loss: 0.7149, data_loss: 0.7149\n",
      "step 2180 , total_loss: 0.9100, data_loss: 0.9100\n",
      "step 2200 , total_loss: 0.8619, data_loss: 0.8619\n",
      "step 2220 , total_loss: 0.8634, data_loss: 0.8634\n",
      "step 2240 , total_loss: 0.7811, data_loss: 0.7811\n",
      "step 2260 , total_loss: 0.8242, data_loss: 0.8242\n",
      "step 2280 , total_loss: 0.7561, data_loss: 0.7561\n",
      "step 2300 , total_loss: 0.7792, data_loss: 0.7792\n",
      "step 2320 , total_loss: 0.7455, data_loss: 0.7455\n",
      "step 2340 , total_loss: 0.7635, data_loss: 0.7635\n",
      "step 2360 , total_loss: 0.8039, data_loss: 0.8039\n",
      "step 2380 , total_loss: 0.7246, data_loss: 0.7246\n",
      "step 2400 , total_loss: 0.7365, data_loss: 0.7365\n",
      "step 2420 , total_loss: 0.8082, data_loss: 0.8082\n",
      "step 2440 , total_loss: 0.8153, data_loss: 0.8153\n",
      "step 2460 , total_loss: 0.7051, data_loss: 0.7051\n",
      "step 2480 , total_loss: 0.8273, data_loss: 0.8273\n",
      "step 2500 , total_loss: 0.7543, data_loss: 0.7543\n",
      "step 2520 , total_loss: 0.7956, data_loss: 0.7956\n",
      "step 2540 , total_loss: 0.9038, data_loss: 0.9038\n",
      "step 2560 , total_loss: 0.7945, data_loss: 0.7945\n",
      "step 2580 , total_loss: 0.8679, data_loss: 0.8679\n",
      "step 2600 , total_loss: 0.7773, data_loss: 0.7773\n",
      "step 2620 , total_loss: 0.8071, data_loss: 0.8071\n",
      "step 2640 , total_loss: 0.7969, data_loss: 0.7969\n",
      "step 2660 , total_loss: 0.8408, data_loss: 0.8408\n",
      "step 2680 , total_loss: 0.7976, data_loss: 0.7976\n",
      "step 2700 , total_loss: 0.9382, data_loss: 0.9382\n",
      "step 2720 , total_loss: 0.8116, data_loss: 0.8116\n",
      "step 2740 , total_loss: 0.8927, data_loss: 0.8927\n",
      "step 2760 , total_loss: 0.8579, data_loss: 0.8579\n",
      "step 2780 , total_loss: 0.8935, data_loss: 0.8935\n",
      "step 2800 , total_loss: 0.8661, data_loss: 0.8661\n",
      "step 2820 , total_loss: 0.7811, data_loss: 0.7811\n",
      "step 2840 , total_loss: 0.7792, data_loss: 0.7792\n",
      "step 2860 , total_loss: 0.8827, data_loss: 0.8827\n",
      "step 2880 , total_loss: 0.8270, data_loss: 0.8270\n",
      "step 2900 , total_loss: 0.7893, data_loss: 0.7893\n",
      "step 2920 , total_loss: 0.8100, data_loss: 0.8100\n",
      "step 2940 , total_loss: 0.8038, data_loss: 0.8038\n",
      "step 2960 , total_loss: 0.8225, data_loss: 0.8225\n",
      "step 2980 , total_loss: 0.7543, data_loss: 0.7543\n",
      "step 3000 , total_loss: 0.8606, data_loss: 0.8606\n",
      "step 3020 , total_loss: 0.7770, data_loss: 0.7770\n",
      "step 3040 , total_loss: 0.8422, data_loss: 0.8422\n",
      "step 3060 , total_loss: 0.8366, data_loss: 0.8366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3080 , total_loss: 0.8273, data_loss: 0.8273\n",
      "step 3100 , total_loss: 0.8022, data_loss: 0.8022\n",
      "step 3120 , total_loss: 0.8809, data_loss: 0.8809\n",
      "step 3140 , total_loss: 0.7719, data_loss: 0.7719\n",
      "step 3160 , total_loss: 0.8705, data_loss: 0.8705\n",
      "step 3180 , total_loss: 0.7603, data_loss: 0.7603\n",
      "step 3200 , total_loss: 0.8438, data_loss: 0.8438\n",
      "step 3220 , total_loss: 0.7939, data_loss: 0.7939\n",
      "step 3240 , total_loss: 0.8383, data_loss: 0.8383\n",
      "step 3260 , total_loss: 0.8704, data_loss: 0.8704\n",
      "step 3280 , total_loss: 0.7547, data_loss: 0.7547\n",
      "step 3300 , total_loss: 0.8852, data_loss: 0.8852\n",
      "eval valid at epoch 7: auc:0.8468,logloss:1.2043,mean_mrr:0.7895,ndcg@2:0.7658,ndcg@4:0.8322,ndcg@6:0.8425,group_auc:0.8449\n",
      "step 20 , total_loss: 0.7283, data_loss: 0.7283\n",
      "step 40 , total_loss: 0.7179, data_loss: 0.7179\n",
      "step 60 , total_loss: 0.7420, data_loss: 0.7420\n",
      "step 80 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 100 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 120 , total_loss: 0.7573, data_loss: 0.7573\n",
      "step 140 , total_loss: 0.7236, data_loss: 0.7236\n",
      "step 160 , total_loss: 0.6855, data_loss: 0.6855\n",
      "step 180 , total_loss: 0.7434, data_loss: 0.7434\n",
      "step 200 , total_loss: 0.7445, data_loss: 0.7445\n",
      "step 220 , total_loss: 0.7054, data_loss: 0.7054\n",
      "step 240 , total_loss: 0.6966, data_loss: 0.6966\n",
      "step 260 , total_loss: 0.7095, data_loss: 0.7095\n",
      "step 280 , total_loss: 0.7348, data_loss: 0.7348\n",
      "step 300 , total_loss: 0.8141, data_loss: 0.8141\n",
      "step 320 , total_loss: 0.7770, data_loss: 0.7770\n",
      "step 340 , total_loss: 0.7408, data_loss: 0.7408\n",
      "step 360 , total_loss: 0.7698, data_loss: 0.7698\n",
      "step 380 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 400 , total_loss: 0.7573, data_loss: 0.7573\n",
      "step 420 , total_loss: 0.8121, data_loss: 0.8121\n",
      "step 440 , total_loss: 0.8120, data_loss: 0.8120\n",
      "step 460 , total_loss: 0.6466, data_loss: 0.6466\n",
      "step 480 , total_loss: 0.7379, data_loss: 0.7379\n",
      "step 500 , total_loss: 0.8038, data_loss: 0.8038\n",
      "step 520 , total_loss: 0.7378, data_loss: 0.7378\n",
      "step 540 , total_loss: 0.8134, data_loss: 0.8134\n",
      "step 560 , total_loss: 0.7704, data_loss: 0.7704\n",
      "step 580 , total_loss: 0.7588, data_loss: 0.7588\n",
      "step 600 , total_loss: 0.7273, data_loss: 0.7273\n",
      "step 620 , total_loss: 0.7445, data_loss: 0.7445\n",
      "step 640 , total_loss: 0.7275, data_loss: 0.7275\n",
      "step 660 , total_loss: 0.7087, data_loss: 0.7087\n",
      "step 680 , total_loss: 0.7681, data_loss: 0.7681\n",
      "step 700 , total_loss: 0.7985, data_loss: 0.7985\n",
      "step 720 , total_loss: 0.7831, data_loss: 0.7831\n",
      "step 740 , total_loss: 0.7287, data_loss: 0.7287\n",
      "step 760 , total_loss: 0.7020, data_loss: 0.7020\n",
      "step 780 , total_loss: 0.7441, data_loss: 0.7441\n",
      "step 800 , total_loss: 0.8343, data_loss: 0.8343\n",
      "step 820 , total_loss: 0.8130, data_loss: 0.8130\n",
      "step 840 , total_loss: 0.8665, data_loss: 0.8665\n",
      "step 860 , total_loss: 0.7760, data_loss: 0.7760\n",
      "step 880 , total_loss: 0.7890, data_loss: 0.7890\n",
      "step 900 , total_loss: 0.7616, data_loss: 0.7616\n",
      "step 920 , total_loss: 0.7018, data_loss: 0.7018\n",
      "step 940 , total_loss: 0.8054, data_loss: 0.8054\n",
      "step 960 , total_loss: 0.7680, data_loss: 0.7680\n",
      "step 980 , total_loss: 0.7849, data_loss: 0.7849\n",
      "step 1000 , total_loss: 0.7782, data_loss: 0.7782\n",
      "step 1020 , total_loss: 0.7316, data_loss: 0.7316\n",
      "step 1040 , total_loss: 0.8209, data_loss: 0.8209\n",
      "step 1060 , total_loss: 0.7881, data_loss: 0.7881\n",
      "step 1080 , total_loss: 0.7648, data_loss: 0.7648\n",
      "step 1100 , total_loss: 0.8222, data_loss: 0.8222\n",
      "step 1120 , total_loss: 0.8646, data_loss: 0.8646\n",
      "step 1140 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 1160 , total_loss: 0.7048, data_loss: 0.7048\n",
      "step 1180 , total_loss: 0.7556, data_loss: 0.7556\n",
      "step 1200 , total_loss: 0.7864, data_loss: 0.7864\n",
      "step 1220 , total_loss: 0.7629, data_loss: 0.7629\n",
      "step 1240 , total_loss: 0.8088, data_loss: 0.8088\n",
      "step 1260 , total_loss: 0.8015, data_loss: 0.8015\n",
      "step 1280 , total_loss: 0.7412, data_loss: 0.7412\n",
      "step 1300 , total_loss: 0.7592, data_loss: 0.7592\n",
      "step 1320 , total_loss: 0.6988, data_loss: 0.6988\n",
      "step 1340 , total_loss: 0.7945, data_loss: 0.7945\n",
      "step 1360 , total_loss: 0.8698, data_loss: 0.8698\n",
      "step 1380 , total_loss: 0.7491, data_loss: 0.7491\n",
      "step 1400 , total_loss: 0.8171, data_loss: 0.8171\n",
      "step 1420 , total_loss: 0.8552, data_loss: 0.8552\n",
      "step 1440 , total_loss: 0.7789, data_loss: 0.7789\n",
      "step 1460 , total_loss: 0.8948, data_loss: 0.8948\n",
      "step 1480 , total_loss: 0.8120, data_loss: 0.8120\n",
      "step 1500 , total_loss: 0.7294, data_loss: 0.7294\n",
      "step 1520 , total_loss: 0.8092, data_loss: 0.8092\n",
      "step 1540 , total_loss: 0.8617, data_loss: 0.8617\n",
      "step 1560 , total_loss: 0.7896, data_loss: 0.7896\n",
      "step 1580 , total_loss: 0.9411, data_loss: 0.9411\n",
      "step 1600 , total_loss: 0.7897, data_loss: 0.7897\n",
      "step 1620 , total_loss: 0.7184, data_loss: 0.7184\n",
      "step 1640 , total_loss: 0.8127, data_loss: 0.8127\n",
      "step 1660 , total_loss: 0.8290, data_loss: 0.8290\n",
      "step 1680 , total_loss: 0.7779, data_loss: 0.7779\n",
      "step 1700 , total_loss: 0.6989, data_loss: 0.6989\n",
      "step 1720 , total_loss: 0.8475, data_loss: 0.8475\n",
      "step 1740 , total_loss: 0.7387, data_loss: 0.7387\n",
      "step 1760 , total_loss: 0.8325, data_loss: 0.8325\n",
      "step 1780 , total_loss: 0.8237, data_loss: 0.8237\n",
      "step 1800 , total_loss: 0.8327, data_loss: 0.8327\n",
      "step 1820 , total_loss: 0.7776, data_loss: 0.7776\n",
      "step 1840 , total_loss: 0.7754, data_loss: 0.7754\n",
      "step 1860 , total_loss: 0.7575, data_loss: 0.7575\n",
      "step 1880 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 1900 , total_loss: 0.7437, data_loss: 0.7437\n",
      "step 1920 , total_loss: 0.8103, data_loss: 0.8103\n",
      "step 1940 , total_loss: 0.8050, data_loss: 0.8050\n",
      "step 1960 , total_loss: 0.8135, data_loss: 0.8135\n",
      "step 1980 , total_loss: 0.8180, data_loss: 0.8180\n",
      "step 2000 , total_loss: 0.8707, data_loss: 0.8707\n",
      "step 2020 , total_loss: 0.7303, data_loss: 0.7303\n",
      "step 2040 , total_loss: 0.7087, data_loss: 0.7087\n",
      "step 2060 , total_loss: 0.7780, data_loss: 0.7780\n",
      "step 2080 , total_loss: 0.8607, data_loss: 0.8607\n",
      "step 2100 , total_loss: 0.7902, data_loss: 0.7902\n",
      "step 2120 , total_loss: 0.8552, data_loss: 0.8552\n",
      "step 2140 , total_loss: 0.8242, data_loss: 0.8242\n",
      "step 2160 , total_loss: 0.7153, data_loss: 0.7153\n",
      "step 2180 , total_loss: 0.7525, data_loss: 0.7525\n",
      "step 2200 , total_loss: 0.7548, data_loss: 0.7548\n",
      "step 2220 , total_loss: 0.7416, data_loss: 0.7416\n",
      "step 2240 , total_loss: 0.7935, data_loss: 0.7935\n",
      "step 2260 , total_loss: 0.7739, data_loss: 0.7739\n",
      "step 2280 , total_loss: 0.8344, data_loss: 0.8344\n",
      "step 2300 , total_loss: 0.7981, data_loss: 0.7981\n",
      "step 2320 , total_loss: 0.8634, data_loss: 0.8634\n",
      "step 2340 , total_loss: 0.7826, data_loss: 0.7826\n",
      "step 2360 , total_loss: 0.7587, data_loss: 0.7587\n",
      "step 2380 , total_loss: 0.8396, data_loss: 0.8396\n",
      "step 2400 , total_loss: 0.9070, data_loss: 0.9070\n",
      "step 2420 , total_loss: 0.8555, data_loss: 0.8555\n",
      "step 2440 , total_loss: 0.8668, data_loss: 0.8668\n",
      "step 2460 , total_loss: 0.8563, data_loss: 0.8563\n",
      "step 2480 , total_loss: 0.7788, data_loss: 0.7788\n",
      "step 2500 , total_loss: 0.8311, data_loss: 0.8311\n",
      "step 2520 , total_loss: 0.8300, data_loss: 0.8300\n",
      "step 2540 , total_loss: 0.7792, data_loss: 0.7792\n",
      "step 2560 , total_loss: 0.7430, data_loss: 0.7430\n",
      "step 2580 , total_loss: 0.7627, data_loss: 0.7627\n",
      "step 2600 , total_loss: 0.7772, data_loss: 0.7772\n",
      "step 2620 , total_loss: 0.8253, data_loss: 0.8253\n",
      "step 2640 , total_loss: 0.8408, data_loss: 0.8408\n",
      "step 2660 , total_loss: 0.8077, data_loss: 0.8077\n",
      "step 2680 , total_loss: 0.7104, data_loss: 0.7104\n",
      "step 2700 , total_loss: 0.7860, data_loss: 0.7860\n",
      "step 2720 , total_loss: 0.6609, data_loss: 0.6609\n",
      "step 2740 , total_loss: 0.8066, data_loss: 0.8066\n",
      "step 2760 , total_loss: 0.8079, data_loss: 0.8079\n",
      "step 2780 , total_loss: 0.7886, data_loss: 0.7886\n",
      "step 2800 , total_loss: 0.7815, data_loss: 0.7815\n",
      "step 2820 , total_loss: 0.8100, data_loss: 0.8100\n",
      "step 2840 , total_loss: 0.7474, data_loss: 0.7474\n",
      "step 2860 , total_loss: 0.8616, data_loss: 0.8616\n",
      "step 2880 , total_loss: 0.7152, data_loss: 0.7152\n",
      "step 2900 , total_loss: 0.8099, data_loss: 0.8099\n",
      "step 2920 , total_loss: 0.8629, data_loss: 0.8629\n",
      "step 2940 , total_loss: 0.7656, data_loss: 0.7656\n",
      "step 2960 , total_loss: 0.7726, data_loss: 0.7726\n",
      "step 2980 , total_loss: 0.7054, data_loss: 0.7054\n",
      "step 3000 , total_loss: 0.8533, data_loss: 0.8533\n",
      "step 3020 , total_loss: 0.7260, data_loss: 0.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3040 , total_loss: 0.7939, data_loss: 0.7939\n",
      "step 3060 , total_loss: 0.8242, data_loss: 0.8242\n",
      "step 3080 , total_loss: 0.8210, data_loss: 0.8210\n",
      "step 3100 , total_loss: 0.7781, data_loss: 0.7781\n",
      "step 3120 , total_loss: 0.7714, data_loss: 0.7714\n",
      "step 3140 , total_loss: 0.7888, data_loss: 0.7888\n",
      "step 3160 , total_loss: 0.8094, data_loss: 0.8094\n",
      "step 3180 , total_loss: 0.8447, data_loss: 0.8447\n",
      "step 3200 , total_loss: 0.7349, data_loss: 0.7349\n",
      "step 3220 , total_loss: 0.7782, data_loss: 0.7782\n",
      "step 3240 , total_loss: 0.7795, data_loss: 0.7795\n",
      "step 3260 , total_loss: 0.7419, data_loss: 0.7419\n",
      "step 3280 , total_loss: 0.7804, data_loss: 0.7804\n",
      "step 3300 , total_loss: 0.8862, data_loss: 0.8862\n",
      "eval valid at epoch 8: auc:0.8477,logloss:1.3159,mean_mrr:0.7914,ndcg@2:0.7684,ndcg@4:0.8333,ndcg@6:0.8439,group_auc:0.8461\n",
      "step 20 , total_loss: 0.7944, data_loss: 0.7944\n",
      "step 40 , total_loss: 0.7107, data_loss: 0.7107\n",
      "step 60 , total_loss: 0.6760, data_loss: 0.6760\n",
      "step 80 , total_loss: 0.7372, data_loss: 0.7372\n",
      "step 100 , total_loss: 0.7058, data_loss: 0.7058\n",
      "step 120 , total_loss: 0.7347, data_loss: 0.7347\n",
      "step 140 , total_loss: 0.7810, data_loss: 0.7810\n",
      "step 160 , total_loss: 0.7254, data_loss: 0.7254\n",
      "step 180 , total_loss: 0.8775, data_loss: 0.8775\n",
      "step 200 , total_loss: 0.7768, data_loss: 0.7768\n",
      "step 220 , total_loss: 0.7686, data_loss: 0.7686\n",
      "step 240 , total_loss: 0.7312, data_loss: 0.7312\n",
      "step 260 , total_loss: 0.7544, data_loss: 0.7544\n",
      "step 280 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 300 , total_loss: 0.7595, data_loss: 0.7595\n",
      "step 320 , total_loss: 0.6918, data_loss: 0.6918\n",
      "step 340 , total_loss: 0.8851, data_loss: 0.8851\n",
      "step 360 , total_loss: 0.6848, data_loss: 0.6848\n",
      "step 380 , total_loss: 0.6826, data_loss: 0.6826\n",
      "step 400 , total_loss: 0.6955, data_loss: 0.6955\n",
      "step 420 , total_loss: 0.7915, data_loss: 0.7915\n",
      "step 440 , total_loss: 0.7020, data_loss: 0.7020\n",
      "step 460 , total_loss: 0.8363, data_loss: 0.8363\n",
      "step 480 , total_loss: 0.7338, data_loss: 0.7338\n",
      "step 500 , total_loss: 0.7573, data_loss: 0.7573\n",
      "step 520 , total_loss: 0.8650, data_loss: 0.8650\n",
      "step 540 , total_loss: 0.7355, data_loss: 0.7355\n",
      "step 560 , total_loss: 0.7809, data_loss: 0.7809\n",
      "step 580 , total_loss: 0.7594, data_loss: 0.7594\n",
      "step 600 , total_loss: 0.8351, data_loss: 0.8351\n",
      "step 620 , total_loss: 0.7964, data_loss: 0.7964\n",
      "step 640 , total_loss: 0.7436, data_loss: 0.7436\n",
      "step 660 , total_loss: 0.7719, data_loss: 0.7719\n",
      "step 680 , total_loss: 0.8172, data_loss: 0.8172\n",
      "step 700 , total_loss: 0.8031, data_loss: 0.8031\n",
      "step 720 , total_loss: 0.7106, data_loss: 0.7106\n",
      "step 740 , total_loss: 0.6938, data_loss: 0.6938\n",
      "step 760 , total_loss: 0.7271, data_loss: 0.7271\n",
      "step 780 , total_loss: 0.7753, data_loss: 0.7753\n",
      "step 800 , total_loss: 0.7106, data_loss: 0.7106\n",
      "step 820 , total_loss: 0.7254, data_loss: 0.7254\n",
      "step 840 , total_loss: 0.7144, data_loss: 0.7144\n",
      "step 860 , total_loss: 0.7257, data_loss: 0.7257\n",
      "step 880 , total_loss: 0.7950, data_loss: 0.7950\n",
      "step 900 , total_loss: 0.7717, data_loss: 0.7717\n",
      "step 920 , total_loss: 0.8154, data_loss: 0.8154\n",
      "step 940 , total_loss: 0.6972, data_loss: 0.6972\n",
      "step 960 , total_loss: 0.7602, data_loss: 0.7602\n",
      "step 980 , total_loss: 0.8024, data_loss: 0.8024\n",
      "step 1000 , total_loss: 0.7848, data_loss: 0.7848\n",
      "step 1020 , total_loss: 0.6918, data_loss: 0.6918\n",
      "step 1040 , total_loss: 0.7023, data_loss: 0.7023\n",
      "step 1060 , total_loss: 0.7546, data_loss: 0.7546\n",
      "step 1080 , total_loss: 0.8508, data_loss: 0.8508\n",
      "step 1100 , total_loss: 0.7436, data_loss: 0.7436\n",
      "step 1120 , total_loss: 0.8167, data_loss: 0.8167\n",
      "step 1140 , total_loss: 0.7909, data_loss: 0.7909\n",
      "step 1160 , total_loss: 0.7872, data_loss: 0.7872\n",
      "step 1180 , total_loss: 0.7432, data_loss: 0.7432\n",
      "step 1200 , total_loss: 0.6267, data_loss: 0.6267\n",
      "step 1220 , total_loss: 0.7192, data_loss: 0.7192\n",
      "step 1240 , total_loss: 0.7673, data_loss: 0.7673\n",
      "step 1260 , total_loss: 0.7855, data_loss: 0.7855\n",
      "step 1280 , total_loss: 0.7836, data_loss: 0.7836\n",
      "step 1300 , total_loss: 0.7422, data_loss: 0.7422\n",
      "step 1320 , total_loss: 0.7338, data_loss: 0.7338\n",
      "step 1340 , total_loss: 0.7371, data_loss: 0.7371\n",
      "step 1360 , total_loss: 0.7751, data_loss: 0.7751\n",
      "step 1380 , total_loss: 0.7533, data_loss: 0.7533\n",
      "step 1400 , total_loss: 0.7326, data_loss: 0.7326\n",
      "step 1420 , total_loss: 0.8500, data_loss: 0.8500\n",
      "step 1440 , total_loss: 0.7486, data_loss: 0.7486\n",
      "step 1460 , total_loss: 0.7531, data_loss: 0.7531\n",
      "step 1480 , total_loss: 0.7374, data_loss: 0.7374\n",
      "step 1500 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 1520 , total_loss: 0.7827, data_loss: 0.7827\n",
      "step 1540 , total_loss: 0.7580, data_loss: 0.7580\n",
      "step 1560 , total_loss: 0.7373, data_loss: 0.7373\n",
      "step 1580 , total_loss: 0.7627, data_loss: 0.7627\n",
      "step 1600 , total_loss: 0.8025, data_loss: 0.8025\n",
      "step 1620 , total_loss: 0.6835, data_loss: 0.6835\n",
      "step 1640 , total_loss: 0.7910, data_loss: 0.7910\n",
      "step 1660 , total_loss: 0.8399, data_loss: 0.8399\n",
      "step 1680 , total_loss: 0.8646, data_loss: 0.8646\n",
      "step 1700 , total_loss: 0.8011, data_loss: 0.8011\n",
      "step 1720 , total_loss: 0.6974, data_loss: 0.6974\n",
      "step 1740 , total_loss: 0.7187, data_loss: 0.7187\n",
      "step 1760 , total_loss: 0.7567, data_loss: 0.7567\n",
      "step 1780 , total_loss: 0.8246, data_loss: 0.8246\n",
      "step 1800 , total_loss: 0.7661, data_loss: 0.7661\n",
      "step 1820 , total_loss: 0.7294, data_loss: 0.7294\n",
      "step 1840 , total_loss: 0.7298, data_loss: 0.7298\n",
      "step 1860 , total_loss: 0.7802, data_loss: 0.7802\n",
      "step 1880 , total_loss: 0.8246, data_loss: 0.8246\n",
      "step 1900 , total_loss: 0.7176, data_loss: 0.7176\n",
      "step 1920 , total_loss: 0.7702, data_loss: 0.7702\n",
      "step 1940 , total_loss: 0.7250, data_loss: 0.7250\n",
      "step 1960 , total_loss: 0.7760, data_loss: 0.7760\n",
      "step 1980 , total_loss: 0.8136, data_loss: 0.8136\n",
      "step 2000 , total_loss: 0.7402, data_loss: 0.7402\n",
      "step 2020 , total_loss: 0.7997, data_loss: 0.7997\n",
      "step 2040 , total_loss: 0.7798, data_loss: 0.7798\n",
      "step 2060 , total_loss: 0.8487, data_loss: 0.8487\n",
      "step 2080 , total_loss: 0.8099, data_loss: 0.8099\n",
      "step 2100 , total_loss: 0.8424, data_loss: 0.8424\n",
      "step 2120 , total_loss: 0.7317, data_loss: 0.7317\n",
      "step 2140 , total_loss: 0.7887, data_loss: 0.7887\n",
      "step 2160 , total_loss: 0.7774, data_loss: 0.7774\n",
      "step 2180 , total_loss: 0.6890, data_loss: 0.6890\n",
      "step 2200 , total_loss: 0.8737, data_loss: 0.8737\n",
      "step 2220 , total_loss: 0.7388, data_loss: 0.7388\n",
      "step 2240 , total_loss: 0.7312, data_loss: 0.7312\n",
      "step 2260 , total_loss: 0.7180, data_loss: 0.7180\n",
      "step 2280 , total_loss: 0.7994, data_loss: 0.7994\n",
      "step 2300 , total_loss: 0.6632, data_loss: 0.6632\n",
      "step 2320 , total_loss: 0.8654, data_loss: 0.8654\n",
      "step 2340 , total_loss: 0.8181, data_loss: 0.8181\n",
      "step 2360 , total_loss: 0.7719, data_loss: 0.7719\n",
      "step 2380 , total_loss: 0.7687, data_loss: 0.7687\n",
      "step 2400 , total_loss: 0.7715, data_loss: 0.7715\n",
      "step 2420 , total_loss: 0.8789, data_loss: 0.8789\n",
      "step 2440 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 2460 , total_loss: 0.7105, data_loss: 0.7105\n",
      "step 2480 , total_loss: 0.7768, data_loss: 0.7768\n",
      "step 2500 , total_loss: 0.7528, data_loss: 0.7528\n",
      "step 2520 , total_loss: 0.7465, data_loss: 0.7465\n",
      "step 2540 , total_loss: 0.7899, data_loss: 0.7899\n",
      "step 2560 , total_loss: 0.7735, data_loss: 0.7735\n",
      "step 2580 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 2600 , total_loss: 0.7288, data_loss: 0.7288\n",
      "step 2620 , total_loss: 0.7346, data_loss: 0.7346\n",
      "step 2640 , total_loss: 0.7606, data_loss: 0.7606\n",
      "step 2660 , total_loss: 0.8323, data_loss: 0.8323\n",
      "step 2680 , total_loss: 0.8091, data_loss: 0.8091\n",
      "step 2700 , total_loss: 0.7528, data_loss: 0.7528\n",
      "step 2720 , total_loss: 0.7018, data_loss: 0.7018\n",
      "step 2740 , total_loss: 0.7594, data_loss: 0.7594\n",
      "step 2760 , total_loss: 0.7885, data_loss: 0.7885\n",
      "step 2780 , total_loss: 0.7672, data_loss: 0.7672\n",
      "step 2800 , total_loss: 0.7582, data_loss: 0.7582\n",
      "step 2820 , total_loss: 0.7779, data_loss: 0.7779\n",
      "step 2840 , total_loss: 0.7569, data_loss: 0.7569\n",
      "step 2860 , total_loss: 0.8325, data_loss: 0.8325\n",
      "step 2880 , total_loss: 0.7750, data_loss: 0.7750\n",
      "step 2900 , total_loss: 0.7354, data_loss: 0.7354\n",
      "step 2920 , total_loss: 0.8481, data_loss: 0.8481\n",
      "step 2940 , total_loss: 0.8191, data_loss: 0.8191\n",
      "step 2960 , total_loss: 0.8052, data_loss: 0.8052\n",
      "step 2980 , total_loss: 0.7996, data_loss: 0.7996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000 , total_loss: 0.7405, data_loss: 0.7405\n",
      "step 3020 , total_loss: 0.7502, data_loss: 0.7502\n",
      "step 3040 , total_loss: 0.8475, data_loss: 0.8475\n",
      "step 3060 , total_loss: 0.8076, data_loss: 0.8076\n",
      "step 3080 , total_loss: 0.6590, data_loss: 0.6590\n",
      "step 3100 , total_loss: 0.8089, data_loss: 0.8089\n",
      "step 3120 , total_loss: 0.8164, data_loss: 0.8164\n",
      "step 3140 , total_loss: 0.7684, data_loss: 0.7684\n",
      "step 3160 , total_loss: 0.7552, data_loss: 0.7552\n",
      "step 3180 , total_loss: 0.7392, data_loss: 0.7392\n",
      "step 3200 , total_loss: 0.8311, data_loss: 0.8311\n",
      "step 3220 , total_loss: 0.7427, data_loss: 0.7427\n",
      "step 3240 , total_loss: 0.8356, data_loss: 0.8356\n",
      "step 3260 , total_loss: 0.7827, data_loss: 0.7827\n",
      "step 3280 , total_loss: 0.7707, data_loss: 0.7707\n",
      "step 3300 , total_loss: 0.8522, data_loss: 0.8522\n",
      "eval valid at epoch 9: auc:0.8484,logloss:1.4569,mean_mrr:0.793,ndcg@2:0.7693,ndcg@4:0.8345,ndcg@6:0.8451,group_auc:0.8467\n",
      "step 20 , total_loss: 0.6409, data_loss: 0.6409\n",
      "step 40 , total_loss: 0.7746, data_loss: 0.7746\n",
      "step 60 , total_loss: 0.7231, data_loss: 0.7231\n",
      "step 80 , total_loss: 0.6244, data_loss: 0.6244\n",
      "step 100 , total_loss: 0.7537, data_loss: 0.7537\n",
      "step 120 , total_loss: 0.7022, data_loss: 0.7022\n",
      "step 140 , total_loss: 0.6325, data_loss: 0.6325\n",
      "step 160 , total_loss: 0.7751, data_loss: 0.7751\n",
      "step 180 , total_loss: 0.7913, data_loss: 0.7913\n",
      "step 200 , total_loss: 0.6923, data_loss: 0.6923\n",
      "step 220 , total_loss: 0.7387, data_loss: 0.7387\n",
      "step 240 , total_loss: 0.7402, data_loss: 0.7402\n",
      "step 260 , total_loss: 0.7049, data_loss: 0.7049\n",
      "step 280 , total_loss: 0.6731, data_loss: 0.6731\n",
      "step 300 , total_loss: 0.6756, data_loss: 0.6756\n",
      "step 320 , total_loss: 0.7626, data_loss: 0.7626\n",
      "step 340 , total_loss: 0.7820, data_loss: 0.7820\n",
      "step 360 , total_loss: 0.7124, data_loss: 0.7124\n",
      "step 380 , total_loss: 0.7099, data_loss: 0.7099\n",
      "step 400 , total_loss: 0.7953, data_loss: 0.7953\n",
      "step 420 , total_loss: 0.6642, data_loss: 0.6642\n",
      "step 440 , total_loss: 0.7133, data_loss: 0.7133\n",
      "step 460 , total_loss: 0.7358, data_loss: 0.7358\n",
      "step 480 , total_loss: 0.7348, data_loss: 0.7348\n",
      "step 500 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 520 , total_loss: 0.7190, data_loss: 0.7190\n",
      "step 540 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 560 , total_loss: 0.6407, data_loss: 0.6407\n",
      "step 580 , total_loss: 0.6298, data_loss: 0.6298\n",
      "step 600 , total_loss: 0.7317, data_loss: 0.7317\n",
      "step 620 , total_loss: 0.6996, data_loss: 0.6996\n",
      "step 640 , total_loss: 0.7310, data_loss: 0.7310\n",
      "step 660 , total_loss: 0.7200, data_loss: 0.7200\n",
      "step 680 , total_loss: 0.8246, data_loss: 0.8246\n",
      "step 700 , total_loss: 0.6488, data_loss: 0.6488\n",
      "step 720 , total_loss: 0.7576, data_loss: 0.7576\n",
      "step 740 , total_loss: 0.7181, data_loss: 0.7181\n",
      "step 760 , total_loss: 0.7305, data_loss: 0.7305\n",
      "step 780 , total_loss: 0.7164, data_loss: 0.7164\n",
      "step 800 , total_loss: 0.7215, data_loss: 0.7215\n",
      "step 820 , total_loss: 0.7538, data_loss: 0.7538\n",
      "step 840 , total_loss: 0.7183, data_loss: 0.7183\n",
      "step 860 , total_loss: 0.7667, data_loss: 0.7667\n",
      "step 880 , total_loss: 0.7401, data_loss: 0.7401\n",
      "step 900 , total_loss: 0.7174, data_loss: 0.7174\n",
      "step 920 , total_loss: 0.7454, data_loss: 0.7454\n",
      "step 940 , total_loss: 0.7255, data_loss: 0.7255\n",
      "step 960 , total_loss: 0.7998, data_loss: 0.7998\n",
      "step 980 , total_loss: 0.6826, data_loss: 0.6826\n",
      "step 1000 , total_loss: 0.6681, data_loss: 0.6681\n",
      "step 1020 , total_loss: 0.7655, data_loss: 0.7655\n",
      "step 1040 , total_loss: 0.7615, data_loss: 0.7615\n",
      "step 1060 , total_loss: 0.7780, data_loss: 0.7780\n",
      "step 1080 , total_loss: 0.7792, data_loss: 0.7792\n",
      "step 1100 , total_loss: 0.8518, data_loss: 0.8518\n",
      "step 1120 , total_loss: 0.7168, data_loss: 0.7168\n",
      "step 1140 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 1160 , total_loss: 0.7694, data_loss: 0.7694\n",
      "step 1180 , total_loss: 0.7662, data_loss: 0.7662\n",
      "step 1200 , total_loss: 0.6908, data_loss: 0.6908\n",
      "step 1220 , total_loss: 0.7551, data_loss: 0.7551\n",
      "step 1240 , total_loss: 0.7217, data_loss: 0.7217\n",
      "step 1260 , total_loss: 0.6730, data_loss: 0.6730\n",
      "step 1280 , total_loss: 0.8069, data_loss: 0.8069\n",
      "step 1300 , total_loss: 0.7443, data_loss: 0.7443\n",
      "step 1320 , total_loss: 0.7447, data_loss: 0.7447\n",
      "step 1340 , total_loss: 0.7372, data_loss: 0.7372\n",
      "step 1360 , total_loss: 0.7701, data_loss: 0.7701\n",
      "step 1380 , total_loss: 0.7787, data_loss: 0.7787\n",
      "step 1400 , total_loss: 0.7803, data_loss: 0.7803\n",
      "step 1420 , total_loss: 0.7101, data_loss: 0.7101\n",
      "step 1440 , total_loss: 0.7785, data_loss: 0.7785\n",
      "step 1460 , total_loss: 0.7821, data_loss: 0.7821\n",
      "step 1480 , total_loss: 0.7640, data_loss: 0.7640\n",
      "step 1500 , total_loss: 0.7679, data_loss: 0.7679\n",
      "step 1520 , total_loss: 0.7975, data_loss: 0.7975\n",
      "step 1540 , total_loss: 0.7615, data_loss: 0.7615\n",
      "step 1560 , total_loss: 0.7765, data_loss: 0.7765\n",
      "step 1580 , total_loss: 0.7801, data_loss: 0.7801\n",
      "step 1600 , total_loss: 0.7093, data_loss: 0.7093\n",
      "step 1620 , total_loss: 0.7729, data_loss: 0.7729\n",
      "step 1640 , total_loss: 0.7529, data_loss: 0.7529\n",
      "step 1660 , total_loss: 0.8197, data_loss: 0.8197\n",
      "step 1680 , total_loss: 0.6879, data_loss: 0.6879\n",
      "step 1700 , total_loss: 0.7476, data_loss: 0.7476\n",
      "step 1720 , total_loss: 0.7630, data_loss: 0.7630\n",
      "step 1740 , total_loss: 0.7979, data_loss: 0.7979\n",
      "step 1760 , total_loss: 0.7902, data_loss: 0.7902\n",
      "step 1780 , total_loss: 0.7119, data_loss: 0.7119\n",
      "step 1800 , total_loss: 0.7295, data_loss: 0.7295\n",
      "step 1820 , total_loss: 0.7539, data_loss: 0.7539\n",
      "step 1840 , total_loss: 0.7611, data_loss: 0.7611\n",
      "step 1860 , total_loss: 0.7971, data_loss: 0.7971\n",
      "step 1880 , total_loss: 0.7762, data_loss: 0.7762\n",
      "step 1900 , total_loss: 0.7566, data_loss: 0.7566\n",
      "step 1920 , total_loss: 0.6972, data_loss: 0.6972\n",
      "step 1940 , total_loss: 0.7496, data_loss: 0.7496\n",
      "step 1960 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 1980 , total_loss: 0.8301, data_loss: 0.8301\n",
      "step 2000 , total_loss: 0.7252, data_loss: 0.7252\n",
      "step 2020 , total_loss: 0.7619, data_loss: 0.7619\n",
      "step 2040 , total_loss: 0.8220, data_loss: 0.8220\n",
      "step 2060 , total_loss: 0.8125, data_loss: 0.8125\n",
      "step 2080 , total_loss: 0.7339, data_loss: 0.7339\n",
      "step 2100 , total_loss: 0.7447, data_loss: 0.7447\n",
      "step 2120 , total_loss: 0.7102, data_loss: 0.7102\n",
      "step 2140 , total_loss: 0.6287, data_loss: 0.6287\n",
      "step 2160 , total_loss: 0.7636, data_loss: 0.7636\n",
      "step 2180 , total_loss: 0.8113, data_loss: 0.8113\n",
      "step 2200 , total_loss: 0.7517, data_loss: 0.7517\n",
      "step 2220 , total_loss: 0.7827, data_loss: 0.7827\n",
      "step 2240 , total_loss: 0.8174, data_loss: 0.8174\n",
      "step 2260 , total_loss: 0.6946, data_loss: 0.6946\n",
      "step 2280 , total_loss: 0.7886, data_loss: 0.7886\n",
      "step 2300 , total_loss: 0.7237, data_loss: 0.7237\n",
      "step 2320 , total_loss: 0.8197, data_loss: 0.8197\n",
      "step 2340 , total_loss: 0.8475, data_loss: 0.8475\n",
      "step 2360 , total_loss: 0.7394, data_loss: 0.7394\n",
      "step 2380 , total_loss: 0.7481, data_loss: 0.7481\n",
      "step 2400 , total_loss: 0.7489, data_loss: 0.7489\n",
      "step 2420 , total_loss: 0.7601, data_loss: 0.7601\n",
      "step 2440 , total_loss: 0.7612, data_loss: 0.7612\n",
      "step 2460 , total_loss: 0.7483, data_loss: 0.7483\n",
      "step 2480 , total_loss: 0.8274, data_loss: 0.8274\n",
      "step 2500 , total_loss: 0.7944, data_loss: 0.7944\n",
      "step 2520 , total_loss: 0.7349, data_loss: 0.7349\n",
      "step 2540 , total_loss: 0.7746, data_loss: 0.7746\n",
      "step 2560 , total_loss: 0.7426, data_loss: 0.7426\n",
      "step 2580 , total_loss: 0.7334, data_loss: 0.7334\n",
      "step 2600 , total_loss: 0.7575, data_loss: 0.7575\n",
      "step 2620 , total_loss: 0.7411, data_loss: 0.7411\n",
      "step 2640 , total_loss: 0.8007, data_loss: 0.8007\n",
      "step 2660 , total_loss: 0.7505, data_loss: 0.7505\n",
      "step 2680 , total_loss: 0.7586, data_loss: 0.7586\n",
      "step 2700 , total_loss: 0.7708, data_loss: 0.7708\n",
      "step 2720 , total_loss: 0.7109, data_loss: 0.7109\n",
      "step 2740 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 2760 , total_loss: 0.7018, data_loss: 0.7018\n",
      "step 2780 , total_loss: 0.7503, data_loss: 0.7503\n",
      "step 2800 , total_loss: 0.7801, data_loss: 0.7801\n",
      "step 2820 , total_loss: 0.7883, data_loss: 0.7883\n",
      "step 2840 , total_loss: 0.7705, data_loss: 0.7705\n",
      "step 2860 , total_loss: 0.8275, data_loss: 0.8275\n",
      "step 2880 , total_loss: 0.7843, data_loss: 0.7843\n",
      "step 2900 , total_loss: 0.7773, data_loss: 0.7773\n",
      "step 2920 , total_loss: 0.7379, data_loss: 0.7379\n",
      "step 2940 , total_loss: 0.8156, data_loss: 0.8156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2960 , total_loss: 0.7696, data_loss: 0.7696\n",
      "step 2980 , total_loss: 0.7284, data_loss: 0.7284\n",
      "step 3000 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 3020 , total_loss: 0.7917, data_loss: 0.7917\n",
      "step 3040 , total_loss: 0.7965, data_loss: 0.7965\n",
      "step 3060 , total_loss: 0.6914, data_loss: 0.6914\n",
      "step 3080 , total_loss: 0.8013, data_loss: 0.8013\n",
      "step 3100 , total_loss: 0.7989, data_loss: 0.7989\n",
      "step 3120 , total_loss: 0.7792, data_loss: 0.7792\n",
      "step 3140 , total_loss: 0.6727, data_loss: 0.6727\n",
      "step 3160 , total_loss: 0.8523, data_loss: 0.8523\n",
      "step 3180 , total_loss: 0.6744, data_loss: 0.6744\n",
      "step 3200 , total_loss: 0.7534, data_loss: 0.7534\n",
      "step 3220 , total_loss: 0.7395, data_loss: 0.7395\n",
      "step 3240 , total_loss: 0.7258, data_loss: 0.7258\n",
      "step 3260 , total_loss: 0.8168, data_loss: 0.8168\n",
      "step 3280 , total_loss: 0.7196, data_loss: 0.7196\n",
      "step 3300 , total_loss: 0.7326, data_loss: 0.7326\n",
      "eval valid at epoch 10: auc:0.8474,logloss:1.5456,mean_mrr:0.7922,ndcg@2:0.7686,ndcg@4:0.8337,ndcg@6:0.8445,group_auc:0.8461\n",
      "[(1, {'auc': 0.7983, 'logloss': 0.6678, 'mean_mrr': 0.7267, 'ndcg@2': 0.689, 'ndcg@4': 0.7783, 'ndcg@6': 0.7952, 'group_auc': 0.7905}), (2, {'auc': 0.8228, 'logloss': 0.7184, 'mean_mrr': 0.757, 'ndcg@2': 0.7264, 'ndcg@4': 0.8045, 'ndcg@6': 0.8181, 'group_auc': 0.8174}), (3, {'auc': 0.8329, 'logloss': 0.7964, 'mean_mrr': 0.7702, 'ndcg@2': 0.7424, 'ndcg@4': 0.8157, 'ndcg@6': 0.828, 'group_auc': 0.8284}), (4, {'auc': 0.8396, 'logloss': 0.9247, 'mean_mrr': 0.779, 'ndcg@2': 0.7533, 'ndcg@4': 0.823, 'ndcg@6': 0.8346, 'group_auc': 0.836}), (5, {'auc': 0.8429, 'logloss': 0.9989, 'mean_mrr': 0.7835, 'ndcg@2': 0.7592, 'ndcg@4': 0.8269, 'ndcg@6': 0.838, 'group_auc': 0.84}), (6, {'auc': 0.8448, 'logloss': 1.0981, 'mean_mrr': 0.7864, 'ndcg@2': 0.7617, 'ndcg@4': 0.8291, 'ndcg@6': 0.8402, 'group_auc': 0.8418}), (7, {'auc': 0.8468, 'logloss': 1.2043, 'mean_mrr': 0.7895, 'ndcg@2': 0.7658, 'ndcg@4': 0.8322, 'ndcg@6': 0.8425, 'group_auc': 0.8449}), (8, {'auc': 0.8477, 'logloss': 1.3159, 'mean_mrr': 0.7914, 'ndcg@2': 0.7684, 'ndcg@4': 0.8333, 'ndcg@6': 0.8439, 'group_auc': 0.8461}), (9, {'auc': 0.8484, 'logloss': 1.4569, 'mean_mrr': 0.793, 'ndcg@2': 0.7693, 'ndcg@4': 0.8345, 'ndcg@6': 0.8451, 'group_auc': 0.8467}), (10, {'auc': 0.8474, 'logloss': 1.5456, 'mean_mrr': 0.7922, 'ndcg@2': 0.7686, 'ndcg@4': 0.8337, 'ndcg@6': 0.8445, 'group_auc': 0.8461})]\n",
      "best epoch: 9\n",
      "Time cost for training is 67.12 mins\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e41391",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34419/492773312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_num_ngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_syn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved model in ../../tests/resources/deeprec/slirec/model/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 01:53:58.730536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 01:53:58.730693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 01:53:58.730801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 01:53:58.730938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 01:53:58.731045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-03 01:53:58.731136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9682 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.8392,\n",
       " 'logloss': 1.659,\n",
       " 'mean_mrr': 0.6699,\n",
       " 'ndcg@2': 0.6194,\n",
       " 'ndcg@4': 0.6939,\n",
       " 'ndcg@6': 0.7246,\n",
       " 'group_auc': 0.836}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
