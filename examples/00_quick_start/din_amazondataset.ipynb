{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f00ce69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/caozheng/Desktop/code/recommenders/examples/00_quick_start',\n",
       " '/home/caozheng/anaconda3/lib/python39.zip',\n",
       " '/home/caozheng/anaconda3/lib/python3.9',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/scikit_surprise-1.1.1-py3.9-linux-x86_64.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/pandera-0.11.0-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/retrying-1.3.3-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/cornac-1.14.2-py3.9-linux-x86_64.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/PyYAML-5.4.1-py3.9-linux-x86_64.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/category_encoders-1.3.0-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/transformers-4.19.2-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/nltk-3.7-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/memory_profiler-0.60.0-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/lightgbm-3.3.2-py3.9-linux-x86_64.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/hypothesis-6.46.7-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/typing_inspect-0.7.1-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/pydantic-1.9.1-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/pyarrow-8.0.0-py3.9-linux-x86_64.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/powerlaw-1.5-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/urllib3-1.26.9-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/tokenizers-0.12.1-py3.9-linux-x86_64.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/huggingface_hub-0.6.0-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/click-8.1.3-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/pytz-2022.1-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg',\n",
       " '/home/caozheng/anaconda3/lib/python3.9/site-packages/IPython/extensions',\n",
       " '/home/caozheng/.ipython',\n",
       " '../../recommenders',\n",
       " '../..']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6d328",
   "metadata": {},
   "source": [
    "# To analyze the dataset of Amazon\n",
    "\n",
    "number of users: 123,960\n",
    "\n",
    "number of movies: 50,024\n",
    "\n",
    "number of categories: 164\n",
    "\n",
    "number of historical actions for each user:\n",
    "\n",
    "number of train samples: 1,325,653\n",
    "\n",
    "number of test samples: 1,239,600\n",
    "\n",
    "number of pos samples: \n",
    "\n",
    "number of neg samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a804b2d5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['relu', 'relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50bf318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_yaml = load_dict(hparams.user_vocab)\n",
    "item_yaml = load_dict(hparams.item_vocab)\n",
    "cate_yaml = load_dict(hparams.cate_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2622c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123960, 50024, 164)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_yaml), len(item_yaml), len(cate_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6a1733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 01:36:35.003906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:35.022400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:35.022595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-05-24 01:36:36.951477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-24 01:36:36.952614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:36.952788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:36.952917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:37.246772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:37.246936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:37.247048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 01:36:37.247148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7947 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-05-24 01:36:37.278688: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-05-24 01:38:26.765000: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.5962, data_loss: 1.5962\n",
      "step 40 , total_loss: 1.5645, data_loss: 1.5645\n",
      "step 60 , total_loss: 1.5578, data_loss: 1.5578\n",
      "step 80 , total_loss: 1.5354, data_loss: 1.5354\n",
      "step 100 , total_loss: 1.5639, data_loss: 1.5639\n",
      "step 120 , total_loss: 1.5204, data_loss: 1.5204\n",
      "step 140 , total_loss: 1.5545, data_loss: 1.5545\n",
      "step 160 , total_loss: 1.5359, data_loss: 1.5359\n",
      "step 180 , total_loss: 1.5305, data_loss: 1.5305\n",
      "step 200 , total_loss: 1.5194, data_loss: 1.5194\n",
      "step 220 , total_loss: 1.4985, data_loss: 1.4985\n",
      "step 240 , total_loss: 1.5533, data_loss: 1.5533\n",
      "step 260 , total_loss: 1.4546, data_loss: 1.4546\n",
      "step 280 , total_loss: 1.4668, data_loss: 1.4668\n",
      "step 300 , total_loss: 1.5295, data_loss: 1.5295\n",
      "step 320 , total_loss: 1.4794, data_loss: 1.4794\n",
      "step 340 , total_loss: 1.4892, data_loss: 1.4892\n",
      "step 360 , total_loss: 1.4649, data_loss: 1.4649\n",
      "step 380 , total_loss: 1.4442, data_loss: 1.4442\n",
      "step 400 , total_loss: 1.4854, data_loss: 1.4854\n",
      "step 420 , total_loss: 1.4400, data_loss: 1.4400\n",
      "step 440 , total_loss: 1.4139, data_loss: 1.4139\n",
      "step 460 , total_loss: 1.4551, data_loss: 1.4551\n",
      "step 480 , total_loss: 1.4140, data_loss: 1.4140\n",
      "step 500 , total_loss: 1.4072, data_loss: 1.4072\n",
      "step 520 , total_loss: 1.3732, data_loss: 1.3732\n",
      "step 540 , total_loss: 1.3917, data_loss: 1.3917\n",
      "step 560 , total_loss: 1.4132, data_loss: 1.4132\n",
      "step 580 , total_loss: 1.4175, data_loss: 1.4175\n",
      "step 600 , total_loss: 1.3391, data_loss: 1.3391\n",
      "step 620 , total_loss: 1.4363, data_loss: 1.4363\n",
      "step 640 , total_loss: 1.4083, data_loss: 1.4083\n",
      "step 660 , total_loss: 1.3520, data_loss: 1.3520\n",
      "step 680 , total_loss: 1.3908, data_loss: 1.3908\n",
      "step 700 , total_loss: 1.3496, data_loss: 1.3496\n",
      "step 720 , total_loss: 1.3322, data_loss: 1.3322\n",
      "step 740 , total_loss: 1.3270, data_loss: 1.3270\n",
      "step 760 , total_loss: 1.3824, data_loss: 1.3824\n",
      "step 780 , total_loss: 1.3553, data_loss: 1.3553\n",
      "step 800 , total_loss: 1.3490, data_loss: 1.3490\n",
      "step 820 , total_loss: 1.3515, data_loss: 1.3515\n",
      "step 840 , total_loss: 1.2876, data_loss: 1.2876\n",
      "step 860 , total_loss: 1.3012, data_loss: 1.3012\n",
      "step 880 , total_loss: 1.3221, data_loss: 1.3221\n",
      "step 900 , total_loss: 1.3309, data_loss: 1.3309\n",
      "step 920 , total_loss: 1.2874, data_loss: 1.2874\n",
      "step 940 , total_loss: 1.2677, data_loss: 1.2677\n",
      "step 960 , total_loss: 1.3450, data_loss: 1.3450\n",
      "step 980 , total_loss: 1.2535, data_loss: 1.2535\n",
      "step 1000 , total_loss: 1.2530, data_loss: 1.2530\n",
      "step 1020 , total_loss: 1.2570, data_loss: 1.2570\n",
      "step 1040 , total_loss: 1.3254, data_loss: 1.3254\n",
      "step 1060 , total_loss: 1.3210, data_loss: 1.3210\n",
      "step 1080 , total_loss: 1.2712, data_loss: 1.2712\n",
      "step 1100 , total_loss: 1.2450, data_loss: 1.2450\n",
      "step 1120 , total_loss: 1.2972, data_loss: 1.2972\n",
      "step 1140 , total_loss: 1.2373, data_loss: 1.2373\n",
      "step 1160 , total_loss: 1.2016, data_loss: 1.2016\n",
      "step 1180 , total_loss: 1.1731, data_loss: 1.1731\n",
      "step 1200 , total_loss: 1.2586, data_loss: 1.2586\n",
      "step 1220 , total_loss: 1.2718, data_loss: 1.2718\n",
      "step 1240 , total_loss: 1.2575, data_loss: 1.2575\n",
      "step 1260 , total_loss: 1.2791, data_loss: 1.2791\n",
      "step 1280 , total_loss: 1.2511, data_loss: 1.2511\n",
      "step 1300 , total_loss: 1.2483, data_loss: 1.2483\n",
      "step 1320 , total_loss: 1.1931, data_loss: 1.1931\n",
      "step 1340 , total_loss: 1.2277, data_loss: 1.2277\n",
      "step 1360 , total_loss: 1.2297, data_loss: 1.2297\n",
      "step 1380 , total_loss: 1.2417, data_loss: 1.2417\n",
      "step 1400 , total_loss: 1.1755, data_loss: 1.1755\n",
      "step 1420 , total_loss: 1.2551, data_loss: 1.2551\n",
      "step 1440 , total_loss: 1.1832, data_loss: 1.1832\n",
      "step 1460 , total_loss: 1.2223, data_loss: 1.2223\n",
      "step 1480 , total_loss: 1.2313, data_loss: 1.2313\n",
      "step 1500 , total_loss: 1.2254, data_loss: 1.2254\n",
      "step 1520 , total_loss: 1.2646, data_loss: 1.2646\n",
      "step 1540 , total_loss: 1.1825, data_loss: 1.1825\n",
      "step 1560 , total_loss: 1.2314, data_loss: 1.2314\n",
      "step 1580 , total_loss: 1.2078, data_loss: 1.2078\n",
      "step 1600 , total_loss: 1.1681, data_loss: 1.1681\n",
      "step 1620 , total_loss: 1.1816, data_loss: 1.1816\n",
      "step 1640 , total_loss: 1.1602, data_loss: 1.1602\n",
      "step 1660 , total_loss: 1.1056, data_loss: 1.1056\n",
      "step 1680 , total_loss: 1.1746, data_loss: 1.1746\n",
      "step 1700 , total_loss: 1.1427, data_loss: 1.1427\n",
      "step 1720 , total_loss: 1.0867, data_loss: 1.0867\n",
      "step 1740 , total_loss: 1.1308, data_loss: 1.1308\n",
      "step 1760 , total_loss: 1.1603, data_loss: 1.1603\n",
      "step 1780 , total_loss: 1.0952, data_loss: 1.0952\n",
      "step 1800 , total_loss: 1.1388, data_loss: 1.1388\n",
      "step 1820 , total_loss: 1.1461, data_loss: 1.1461\n",
      "step 1840 , total_loss: 1.1402, data_loss: 1.1402\n",
      "step 1860 , total_loss: 1.1404, data_loss: 1.1404\n",
      "step 1880 , total_loss: 1.1013, data_loss: 1.1013\n",
      "step 1900 , total_loss: 1.1524, data_loss: 1.1524\n",
      "step 1920 , total_loss: 1.1175, data_loss: 1.1175\n",
      "step 1940 , total_loss: 1.0786, data_loss: 1.0786\n",
      "step 1960 , total_loss: 1.1152, data_loss: 1.1152\n",
      "step 1980 , total_loss: 1.1323, data_loss: 1.1323\n",
      "step 2000 , total_loss: 1.1089, data_loss: 1.1089\n",
      "step 2020 , total_loss: 1.1539, data_loss: 1.1539\n",
      "step 2040 , total_loss: 1.0956, data_loss: 1.0956\n",
      "step 2060 , total_loss: 1.1066, data_loss: 1.1066\n",
      "step 2080 , total_loss: 1.1159, data_loss: 1.1159\n",
      "step 2100 , total_loss: 1.0685, data_loss: 1.0685\n",
      "step 2120 , total_loss: 1.0999, data_loss: 1.0999\n",
      "step 2140 , total_loss: 1.0774, data_loss: 1.0774\n",
      "step 2160 , total_loss: 1.1583, data_loss: 1.1583\n",
      "step 2180 , total_loss: 1.1559, data_loss: 1.1559\n",
      "step 2200 , total_loss: 1.0715, data_loss: 1.0715\n",
      "step 2220 , total_loss: 1.0014, data_loss: 1.0014\n",
      "step 2240 , total_loss: 1.0685, data_loss: 1.0685\n",
      "step 2260 , total_loss: 1.0742, data_loss: 1.0742\n",
      "step 2280 , total_loss: 1.1015, data_loss: 1.1015\n",
      "step 2300 , total_loss: 1.0250, data_loss: 1.0250\n",
      "step 2320 , total_loss: 1.0658, data_loss: 1.0658\n",
      "step 2340 , total_loss: 1.0949, data_loss: 1.0949\n",
      "step 2360 , total_loss: 0.9668, data_loss: 0.9668\n",
      "step 2380 , total_loss: 1.0602, data_loss: 1.0602\n",
      "step 2400 , total_loss: 1.0489, data_loss: 1.0489\n",
      "step 2420 , total_loss: 1.0413, data_loss: 1.0413\n",
      "step 2440 , total_loss: 1.0730, data_loss: 1.0730\n",
      "step 2460 , total_loss: 1.0254, data_loss: 1.0254\n",
      "step 2480 , total_loss: 1.0382, data_loss: 1.0382\n",
      "step 2500 , total_loss: 1.0192, data_loss: 1.0192\n",
      "step 2520 , total_loss: 1.0150, data_loss: 1.0150\n",
      "step 2540 , total_loss: 1.1123, data_loss: 1.1123\n",
      "step 2560 , total_loss: 1.1046, data_loss: 1.1046\n",
      "step 2580 , total_loss: 1.0343, data_loss: 1.0343\n",
      "step 2600 , total_loss: 1.0315, data_loss: 1.0315\n",
      "step 2620 , total_loss: 1.0409, data_loss: 1.0409\n",
      "step 2640 , total_loss: 1.0389, data_loss: 1.0389\n",
      "step 2660 , total_loss: 0.9954, data_loss: 0.9954\n",
      "step 2680 , total_loss: 1.0924, data_loss: 1.0924\n",
      "step 2700 , total_loss: 1.0134, data_loss: 1.0134\n",
      "step 2720 , total_loss: 1.0596, data_loss: 1.0596\n",
      "step 2740 , total_loss: 1.0613, data_loss: 1.0613\n",
      "step 2760 , total_loss: 1.0549, data_loss: 1.0549\n",
      "step 2780 , total_loss: 0.9911, data_loss: 0.9911\n",
      "step 2800 , total_loss: 1.0611, data_loss: 1.0611\n",
      "step 2820 , total_loss: 1.0452, data_loss: 1.0452\n",
      "step 2840 , total_loss: 1.1115, data_loss: 1.1115\n",
      "step 2860 , total_loss: 1.0183, data_loss: 1.0183\n",
      "step 2880 , total_loss: 1.0676, data_loss: 1.0676\n",
      "step 2900 , total_loss: 1.0182, data_loss: 1.0182\n",
      "step 2920 , total_loss: 1.0363, data_loss: 1.0363\n",
      "step 2940 , total_loss: 1.0534, data_loss: 1.0534\n",
      "step 2960 , total_loss: 1.0303, data_loss: 1.0303\n",
      "step 2980 , total_loss: 1.0055, data_loss: 1.0055\n",
      "step 3000 , total_loss: 1.0403, data_loss: 1.0403\n",
      "step 3020 , total_loss: 1.0172, data_loss: 1.0172\n",
      "step 3040 , total_loss: 1.0122, data_loss: 1.0122\n",
      "step 3060 , total_loss: 0.9873, data_loss: 0.9873\n",
      "step 3080 , total_loss: 1.0238, data_loss: 1.0238\n",
      "step 3100 , total_loss: 1.0294, data_loss: 1.0294\n",
      "step 3120 , total_loss: 1.0205, data_loss: 1.0205\n",
      "step 3140 , total_loss: 1.0308, data_loss: 1.0308\n",
      "step 3160 , total_loss: 1.0381, data_loss: 1.0381\n",
      "step 3180 , total_loss: 1.0045, data_loss: 1.0045\n",
      "step 3200 , total_loss: 1.0053, data_loss: 1.0053\n",
      "step 3220 , total_loss: 1.0134, data_loss: 1.0134\n",
      "step 3240 , total_loss: 0.9849, data_loss: 0.9849\n",
      "step 3260 , total_loss: 1.0380, data_loss: 1.0380\n",
      "step 3280 , total_loss: 0.9686, data_loss: 0.9686\n",
      "step 3300 , total_loss: 1.0756, data_loss: 1.0756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval valid at epoch 1: auc:0.8234,logloss:0.5657,mean_mrr:0.7524,ndcg@2:0.7213,ndcg@4:0.8008,ndcg@6:0.8146,group_auc:0.814\n",
      "step 20 , total_loss: 0.9565, data_loss: 0.9565\n",
      "step 40 , total_loss: 0.8961, data_loss: 0.8961\n",
      "step 60 , total_loss: 0.9166, data_loss: 0.9166\n",
      "step 80 , total_loss: 1.0226, data_loss: 1.0226\n",
      "step 100 , total_loss: 0.8890, data_loss: 0.8890\n",
      "step 120 , total_loss: 0.9297, data_loss: 0.9297\n",
      "step 140 , total_loss: 0.9306, data_loss: 0.9306\n",
      "step 160 , total_loss: 0.8765, data_loss: 0.8765\n",
      "step 180 , total_loss: 0.9649, data_loss: 0.9649\n",
      "step 200 , total_loss: 0.9458, data_loss: 0.9458\n",
      "step 220 , total_loss: 0.9578, data_loss: 0.9578\n",
      "step 240 , total_loss: 0.9478, data_loss: 0.9478\n",
      "step 260 , total_loss: 1.0113, data_loss: 1.0113\n",
      "step 280 , total_loss: 0.8936, data_loss: 0.8936\n",
      "step 300 , total_loss: 0.9532, data_loss: 0.9532\n",
      "step 320 , total_loss: 0.9423, data_loss: 0.9423\n",
      "step 340 , total_loss: 0.9704, data_loss: 0.9704\n",
      "step 360 , total_loss: 0.8271, data_loss: 0.8271\n",
      "step 380 , total_loss: 0.9415, data_loss: 0.9415\n",
      "step 400 , total_loss: 0.9333, data_loss: 0.9333\n",
      "step 420 , total_loss: 0.8761, data_loss: 0.8761\n",
      "step 440 , total_loss: 0.9699, data_loss: 0.9699\n",
      "step 460 , total_loss: 0.9428, data_loss: 0.9428\n",
      "step 480 , total_loss: 0.9529, data_loss: 0.9529\n",
      "step 500 , total_loss: 1.0187, data_loss: 1.0187\n",
      "step 520 , total_loss: 0.9072, data_loss: 0.9072\n",
      "step 540 , total_loss: 0.9524, data_loss: 0.9524\n",
      "step 560 , total_loss: 1.0112, data_loss: 1.0112\n",
      "step 580 , total_loss: 1.0517, data_loss: 1.0517\n",
      "step 600 , total_loss: 0.9683, data_loss: 0.9683\n",
      "step 620 , total_loss: 0.8940, data_loss: 0.8940\n",
      "step 640 , total_loss: 0.9434, data_loss: 0.9434\n",
      "step 660 , total_loss: 1.0129, data_loss: 1.0129\n",
      "step 680 , total_loss: 1.0179, data_loss: 1.0179\n",
      "step 700 , total_loss: 0.9184, data_loss: 0.9184\n",
      "step 720 , total_loss: 0.9581, data_loss: 0.9581\n",
      "step 740 , total_loss: 0.8850, data_loss: 0.8850\n",
      "step 760 , total_loss: 0.9629, data_loss: 0.9629\n",
      "step 780 , total_loss: 0.9312, data_loss: 0.9312\n",
      "step 800 , total_loss: 0.8370, data_loss: 0.8370\n",
      "step 820 , total_loss: 0.9606, data_loss: 0.9606\n",
      "step 840 , total_loss: 1.0392, data_loss: 1.0392\n",
      "step 860 , total_loss: 0.9225, data_loss: 0.9225\n",
      "step 880 , total_loss: 0.9688, data_loss: 0.9688\n",
      "step 900 , total_loss: 0.9233, data_loss: 0.9233\n",
      "step 920 , total_loss: 0.9096, data_loss: 0.9096\n",
      "step 940 , total_loss: 0.8925, data_loss: 0.8925\n",
      "step 960 , total_loss: 0.9454, data_loss: 0.9454\n",
      "step 980 , total_loss: 0.9067, data_loss: 0.9067\n",
      "step 1000 , total_loss: 0.9324, data_loss: 0.9324\n",
      "step 1020 , total_loss: 0.9322, data_loss: 0.9322\n",
      "step 1040 , total_loss: 0.9446, data_loss: 0.9446\n",
      "step 1060 , total_loss: 0.9552, data_loss: 0.9552\n",
      "step 1080 , total_loss: 0.9314, data_loss: 0.9314\n",
      "step 1100 , total_loss: 0.8941, data_loss: 0.8941\n",
      "step 1120 , total_loss: 0.8906, data_loss: 0.8906\n",
      "step 1140 , total_loss: 1.0044, data_loss: 1.0044\n",
      "step 1160 , total_loss: 0.9573, data_loss: 0.9573\n",
      "step 1180 , total_loss: 0.9521, data_loss: 0.9521\n",
      "step 1200 , total_loss: 0.9699, data_loss: 0.9699\n",
      "step 1220 , total_loss: 0.8799, data_loss: 0.8799\n",
      "step 1240 , total_loss: 0.9529, data_loss: 0.9529\n",
      "step 1260 , total_loss: 0.8962, data_loss: 0.8962\n",
      "step 1280 , total_loss: 0.9209, data_loss: 0.9209\n",
      "step 1300 , total_loss: 0.8989, data_loss: 0.8989\n",
      "step 1320 , total_loss: 0.9028, data_loss: 0.9028\n",
      "step 1340 , total_loss: 0.9929, data_loss: 0.9929\n",
      "step 1360 , total_loss: 0.9272, data_loss: 0.9272\n",
      "step 1380 , total_loss: 0.8585, data_loss: 0.8585\n",
      "step 1400 , total_loss: 0.9461, data_loss: 0.9461\n",
      "step 1420 , total_loss: 0.9565, data_loss: 0.9565\n",
      "step 1440 , total_loss: 0.9564, data_loss: 0.9564\n",
      "step 1460 , total_loss: 0.9242, data_loss: 0.9242\n",
      "step 1480 , total_loss: 0.9322, data_loss: 0.9322\n",
      "step 1500 , total_loss: 1.0016, data_loss: 1.0016\n",
      "step 1520 , total_loss: 0.9944, data_loss: 0.9944\n",
      "step 1540 , total_loss: 0.8826, data_loss: 0.8826\n",
      "step 1560 , total_loss: 0.9434, data_loss: 0.9434\n",
      "step 1580 , total_loss: 0.9772, data_loss: 0.9772\n",
      "step 1600 , total_loss: 0.8735, data_loss: 0.8735\n",
      "step 1620 , total_loss: 0.8915, data_loss: 0.8915\n",
      "step 1640 , total_loss: 0.8854, data_loss: 0.8854\n",
      "step 1660 , total_loss: 0.8499, data_loss: 0.8499\n",
      "step 1680 , total_loss: 0.8726, data_loss: 0.8726\n",
      "step 1700 , total_loss: 0.8545, data_loss: 0.8545\n",
      "step 1720 , total_loss: 0.8764, data_loss: 0.8764\n",
      "step 1740 , total_loss: 0.9520, data_loss: 0.9520\n",
      "step 1760 , total_loss: 0.9252, data_loss: 0.9252\n",
      "step 1780 , total_loss: 0.9600, data_loss: 0.9600\n",
      "step 1800 , total_loss: 0.8806, data_loss: 0.8806\n",
      "step 1820 , total_loss: 0.9331, data_loss: 0.9331\n",
      "step 1840 , total_loss: 1.0006, data_loss: 1.0006\n",
      "step 1860 , total_loss: 0.8923, data_loss: 0.8923\n",
      "step 1880 , total_loss: 0.8998, data_loss: 0.8998\n",
      "step 1900 , total_loss: 0.9185, data_loss: 0.9185\n",
      "step 1920 , total_loss: 0.8898, data_loss: 0.8898\n",
      "step 1940 , total_loss: 0.9526, data_loss: 0.9526\n",
      "step 1960 , total_loss: 0.9839, data_loss: 0.9839\n",
      "step 1980 , total_loss: 0.8909, data_loss: 0.8909\n",
      "step 2000 , total_loss: 0.9241, data_loss: 0.9241\n",
      "step 2020 , total_loss: 0.9517, data_loss: 0.9517\n",
      "step 2040 , total_loss: 0.8963, data_loss: 0.8963\n",
      "step 2060 , total_loss: 0.9184, data_loss: 0.9184\n",
      "step 2080 , total_loss: 0.8840, data_loss: 0.8840\n",
      "step 2100 , total_loss: 0.9871, data_loss: 0.9871\n",
      "step 2120 , total_loss: 0.9967, data_loss: 0.9967\n",
      "step 2140 , total_loss: 0.9543, data_loss: 0.9543\n",
      "step 2160 , total_loss: 0.8674, data_loss: 0.8674\n",
      "step 2180 , total_loss: 0.9071, data_loss: 0.9071\n",
      "step 2200 , total_loss: 0.9721, data_loss: 0.9721\n",
      "step 2220 , total_loss: 0.8717, data_loss: 0.8717\n",
      "step 2240 , total_loss: 0.9604, data_loss: 0.9604\n",
      "step 2260 , total_loss: 0.8326, data_loss: 0.8326\n",
      "step 2280 , total_loss: 0.8578, data_loss: 0.8578\n",
      "step 2300 , total_loss: 0.8624, data_loss: 0.8624\n",
      "step 2320 , total_loss: 0.9110, data_loss: 0.9110\n",
      "step 2340 , total_loss: 0.9458, data_loss: 0.9458\n",
      "step 2360 , total_loss: 0.9396, data_loss: 0.9396\n",
      "step 2380 , total_loss: 0.8871, data_loss: 0.8871\n",
      "step 2400 , total_loss: 0.8964, data_loss: 0.8964\n",
      "step 2420 , total_loss: 0.9561, data_loss: 0.9561\n",
      "step 2440 , total_loss: 0.9398, data_loss: 0.9398\n",
      "step 2460 , total_loss: 0.9632, data_loss: 0.9632\n",
      "step 2480 , total_loss: 0.9602, data_loss: 0.9602\n",
      "step 2500 , total_loss: 0.8387, data_loss: 0.8387\n",
      "step 2520 , total_loss: 0.9027, data_loss: 0.9027\n",
      "step 2540 , total_loss: 1.0161, data_loss: 1.0161\n",
      "step 2560 , total_loss: 0.9261, data_loss: 0.9261\n",
      "step 2580 , total_loss: 0.8932, data_loss: 0.8932\n",
      "step 2600 , total_loss: 0.9103, data_loss: 0.9103\n",
      "step 2620 , total_loss: 0.8930, data_loss: 0.8930\n",
      "step 2640 , total_loss: 0.8829, data_loss: 0.8829\n",
      "step 2660 , total_loss: 0.9948, data_loss: 0.9948\n",
      "step 2680 , total_loss: 0.9335, data_loss: 0.9335\n",
      "step 2700 , total_loss: 0.9295, data_loss: 0.9295\n",
      "step 2720 , total_loss: 0.9827, data_loss: 0.9827\n",
      "step 2740 , total_loss: 0.8852, data_loss: 0.8852\n",
      "step 2760 , total_loss: 0.8277, data_loss: 0.8277\n",
      "step 2780 , total_loss: 0.8305, data_loss: 0.8305\n",
      "step 2800 , total_loss: 1.0149, data_loss: 1.0149\n",
      "step 2820 , total_loss: 0.9055, data_loss: 0.9055\n",
      "step 2840 , total_loss: 0.9029, data_loss: 0.9029\n",
      "step 2860 , total_loss: 0.9127, data_loss: 0.9127\n",
      "step 2880 , total_loss: 0.9266, data_loss: 0.9266\n",
      "step 2900 , total_loss: 0.8110, data_loss: 0.8110\n",
      "step 2920 , total_loss: 0.8718, data_loss: 0.8718\n",
      "step 2940 , total_loss: 0.8115, data_loss: 0.8115\n",
      "step 2960 , total_loss: 0.9092, data_loss: 0.9092\n",
      "step 2980 , total_loss: 0.9717, data_loss: 0.9717\n",
      "step 3000 , total_loss: 0.8333, data_loss: 0.8333\n",
      "step 3020 , total_loss: 0.8705, data_loss: 0.8705\n",
      "step 3040 , total_loss: 0.8946, data_loss: 0.8946\n",
      "step 3060 , total_loss: 0.9120, data_loss: 0.9120\n",
      "step 3080 , total_loss: 0.8798, data_loss: 0.8798\n",
      "step 3100 , total_loss: 0.8824, data_loss: 0.8824\n",
      "step 3120 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 3140 , total_loss: 0.8706, data_loss: 0.8706\n",
      "step 3160 , total_loss: 0.9046, data_loss: 0.9046\n",
      "step 3180 , total_loss: 0.9362, data_loss: 0.9362\n",
      "step 3200 , total_loss: 0.8813, data_loss: 0.8813\n",
      "step 3220 , total_loss: 0.8589, data_loss: 0.8589\n",
      "step 3240 , total_loss: 0.9617, data_loss: 0.9617\n",
      "step 3260 , total_loss: 0.9555, data_loss: 0.9555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3280 , total_loss: 0.9087, data_loss: 0.9087\n",
      "step 3300 , total_loss: 0.8889, data_loss: 0.8889\n",
      "eval valid at epoch 2: auc:0.8514,logloss:0.5886,mean_mrr:0.7838,ndcg@2:0.7593,ndcg@4:0.8279,ndcg@6:0.8383,group_auc:0.841\n",
      "step 20 , total_loss: 0.7759, data_loss: 0.7759\n",
      "step 40 , total_loss: 0.8570, data_loss: 0.8570\n",
      "step 60 , total_loss: 0.8029, data_loss: 0.8029\n",
      "step 80 , total_loss: 0.7823, data_loss: 0.7823\n",
      "step 100 , total_loss: 0.8487, data_loss: 0.8487\n",
      "step 120 , total_loss: 0.7812, data_loss: 0.7812\n",
      "step 140 , total_loss: 0.8420, data_loss: 0.8420\n",
      "step 160 , total_loss: 0.7717, data_loss: 0.7717\n",
      "step 180 , total_loss: 0.8426, data_loss: 0.8426\n",
      "step 200 , total_loss: 0.8598, data_loss: 0.8598\n",
      "step 220 , total_loss: 0.8086, data_loss: 0.8086\n",
      "step 240 , total_loss: 0.8192, data_loss: 0.8192\n",
      "step 260 , total_loss: 0.9218, data_loss: 0.9218\n",
      "step 280 , total_loss: 0.8046, data_loss: 0.8046\n",
      "step 300 , total_loss: 0.8346, data_loss: 0.8346\n",
      "step 320 , total_loss: 0.7909, data_loss: 0.7909\n",
      "step 340 , total_loss: 0.7741, data_loss: 0.7741\n",
      "step 360 , total_loss: 0.8045, data_loss: 0.8045\n",
      "step 380 , total_loss: 0.8318, data_loss: 0.8318\n",
      "step 400 , total_loss: 0.7743, data_loss: 0.7743\n",
      "step 420 , total_loss: 0.9020, data_loss: 0.9020\n",
      "step 440 , total_loss: 0.7980, data_loss: 0.7980\n",
      "step 460 , total_loss: 0.8606, data_loss: 0.8606\n",
      "step 480 , total_loss: 0.7412, data_loss: 0.7412\n",
      "step 500 , total_loss: 0.9327, data_loss: 0.9327\n",
      "step 520 , total_loss: 0.8192, data_loss: 0.8192\n",
      "step 540 , total_loss: 0.7795, data_loss: 0.7795\n",
      "step 560 , total_loss: 0.9005, data_loss: 0.9005\n",
      "step 580 , total_loss: 0.7858, data_loss: 0.7858\n",
      "step 600 , total_loss: 0.7802, data_loss: 0.7802\n",
      "step 620 , total_loss: 0.8921, data_loss: 0.8921\n",
      "step 640 , total_loss: 0.7841, data_loss: 0.7841\n",
      "step 660 , total_loss: 0.8257, data_loss: 0.8257\n",
      "step 680 , total_loss: 0.8479, data_loss: 0.8479\n",
      "step 700 , total_loss: 0.8574, data_loss: 0.8574\n",
      "step 720 , total_loss: 0.8847, data_loss: 0.8847\n",
      "step 740 , total_loss: 0.8849, data_loss: 0.8849\n",
      "step 760 , total_loss: 0.7617, data_loss: 0.7617\n",
      "step 780 , total_loss: 0.9363, data_loss: 0.9363\n",
      "step 800 , total_loss: 0.7873, data_loss: 0.7873\n",
      "step 820 , total_loss: 0.9290, data_loss: 0.9290\n",
      "step 840 , total_loss: 0.8617, data_loss: 0.8617\n",
      "step 860 , total_loss: 0.7874, data_loss: 0.7874\n",
      "step 880 , total_loss: 0.8401, data_loss: 0.8401\n",
      "step 900 , total_loss: 0.8967, data_loss: 0.8967\n",
      "step 920 , total_loss: 0.7927, data_loss: 0.7927\n",
      "step 940 , total_loss: 0.8467, data_loss: 0.8467\n",
      "step 960 , total_loss: 0.8661, data_loss: 0.8661\n",
      "step 980 , total_loss: 0.9025, data_loss: 0.9025\n",
      "step 1000 , total_loss: 0.8357, data_loss: 0.8357\n",
      "step 1020 , total_loss: 0.7969, data_loss: 0.7969\n",
      "step 1040 , total_loss: 0.7839, data_loss: 0.7839\n",
      "step 1060 , total_loss: 0.8399, data_loss: 0.8399\n",
      "step 1080 , total_loss: 0.8087, data_loss: 0.8087\n",
      "step 1100 , total_loss: 0.8522, data_loss: 0.8522\n",
      "step 1120 , total_loss: 0.8302, data_loss: 0.8302\n",
      "step 1140 , total_loss: 0.8362, data_loss: 0.8362\n",
      "step 1160 , total_loss: 0.8464, data_loss: 0.8464\n",
      "step 1180 , total_loss: 0.7729, data_loss: 0.7729\n",
      "step 1200 , total_loss: 0.8408, data_loss: 0.8408\n",
      "step 1220 , total_loss: 0.8925, data_loss: 0.8925\n",
      "step 1240 , total_loss: 0.8076, data_loss: 0.8076\n",
      "step 1260 , total_loss: 0.8702, data_loss: 0.8702\n",
      "step 1280 , total_loss: 0.8093, data_loss: 0.8093\n",
      "step 1300 , total_loss: 0.8171, data_loss: 0.8171\n",
      "step 1320 , total_loss: 0.8642, data_loss: 0.8642\n",
      "step 1340 , total_loss: 0.8911, data_loss: 0.8911\n",
      "step 1360 , total_loss: 0.8104, data_loss: 0.8104\n",
      "step 1380 , total_loss: 0.8865, data_loss: 0.8865\n",
      "step 1400 , total_loss: 0.7441, data_loss: 0.7441\n",
      "step 1420 , total_loss: 0.8148, data_loss: 0.8148\n",
      "step 1440 , total_loss: 0.7699, data_loss: 0.7699\n",
      "step 1460 , total_loss: 0.8942, data_loss: 0.8942\n",
      "step 1480 , total_loss: 0.9183, data_loss: 0.9183\n",
      "step 1500 , total_loss: 0.7639, data_loss: 0.7639\n",
      "step 1520 , total_loss: 0.7619, data_loss: 0.7619\n",
      "step 1540 , total_loss: 0.7577, data_loss: 0.7577\n",
      "step 1560 , total_loss: 0.8381, data_loss: 0.8381\n",
      "step 1580 , total_loss: 0.8465, data_loss: 0.8465\n",
      "step 1600 , total_loss: 0.8539, data_loss: 0.8539\n",
      "step 1620 , total_loss: 0.7725, data_loss: 0.7725\n",
      "step 1640 , total_loss: 0.8063, data_loss: 0.8063\n",
      "step 1660 , total_loss: 0.8181, data_loss: 0.8181\n",
      "step 1680 , total_loss: 0.8048, data_loss: 0.8048\n",
      "step 1700 , total_loss: 0.8282, data_loss: 0.8282\n",
      "step 1720 , total_loss: 0.7801, data_loss: 0.7801\n",
      "step 1740 , total_loss: 0.7956, data_loss: 0.7956\n",
      "step 1760 , total_loss: 0.8192, data_loss: 0.8192\n",
      "step 1780 , total_loss: 0.8121, data_loss: 0.8121\n",
      "step 1800 , total_loss: 0.8221, data_loss: 0.8221\n",
      "step 1820 , total_loss: 0.8100, data_loss: 0.8100\n",
      "step 1840 , total_loss: 0.8289, data_loss: 0.8289\n",
      "step 1860 , total_loss: 0.8079, data_loss: 0.8079\n",
      "step 1880 , total_loss: 0.7831, data_loss: 0.7831\n",
      "step 1900 , total_loss: 0.8302, data_loss: 0.8302\n",
      "step 1920 , total_loss: 0.8858, data_loss: 0.8858\n",
      "step 1940 , total_loss: 0.8311, data_loss: 0.8311\n",
      "step 1960 , total_loss: 0.7922, data_loss: 0.7922\n",
      "step 1980 , total_loss: 0.8247, data_loss: 0.8247\n",
      "step 2000 , total_loss: 0.8557, data_loss: 0.8557\n",
      "step 2020 , total_loss: 0.8655, data_loss: 0.8655\n",
      "step 2040 , total_loss: 0.8447, data_loss: 0.8447\n",
      "step 2060 , total_loss: 0.8026, data_loss: 0.8026\n",
      "step 2080 , total_loss: 0.8065, data_loss: 0.8065\n",
      "step 2100 , total_loss: 0.8355, data_loss: 0.8355\n",
      "step 2120 , total_loss: 0.8511, data_loss: 0.8511\n",
      "step 2140 , total_loss: 0.8598, data_loss: 0.8598\n",
      "step 2160 , total_loss: 0.8350, data_loss: 0.8350\n",
      "step 2180 , total_loss: 0.8254, data_loss: 0.8254\n",
      "step 2200 , total_loss: 0.7569, data_loss: 0.7569\n",
      "step 2220 , total_loss: 0.8601, data_loss: 0.8601\n",
      "step 2240 , total_loss: 0.8049, data_loss: 0.8049\n",
      "step 2260 , total_loss: 0.8592, data_loss: 0.8592\n",
      "step 2280 , total_loss: 0.8619, data_loss: 0.8619\n",
      "step 2300 , total_loss: 0.8788, data_loss: 0.8788\n",
      "step 2320 , total_loss: 0.8644, data_loss: 0.8644\n",
      "step 2340 , total_loss: 0.8955, data_loss: 0.8955\n",
      "step 2360 , total_loss: 0.8041, data_loss: 0.8041\n",
      "step 2380 , total_loss: 0.9062, data_loss: 0.9062\n",
      "step 2400 , total_loss: 0.7282, data_loss: 0.7282\n",
      "step 2420 , total_loss: 0.7701, data_loss: 0.7701\n",
      "step 2440 , total_loss: 0.8423, data_loss: 0.8423\n",
      "step 2460 , total_loss: 0.7906, data_loss: 0.7906\n",
      "step 2480 , total_loss: 0.8227, data_loss: 0.8227\n",
      "step 2500 , total_loss: 0.9168, data_loss: 0.9168\n",
      "step 2520 , total_loss: 0.8648, data_loss: 0.8648\n",
      "step 2540 , total_loss: 0.9137, data_loss: 0.9137\n",
      "step 2560 , total_loss: 0.8255, data_loss: 0.8255\n",
      "step 2580 , total_loss: 0.8309, data_loss: 0.8309\n",
      "step 2600 , total_loss: 0.8151, data_loss: 0.8151\n",
      "step 2620 , total_loss: 0.8351, data_loss: 0.8351\n",
      "step 2640 , total_loss: 0.8243, data_loss: 0.8243\n",
      "step 2660 , total_loss: 0.8013, data_loss: 0.8013\n",
      "step 2680 , total_loss: 0.8478, data_loss: 0.8478\n",
      "step 2700 , total_loss: 0.7495, data_loss: 0.7495\n",
      "step 2720 , total_loss: 0.8010, data_loss: 0.8010\n",
      "step 2740 , total_loss: 0.8149, data_loss: 0.8149\n",
      "step 2760 , total_loss: 0.8176, data_loss: 0.8176\n",
      "step 2780 , total_loss: 0.9294, data_loss: 0.9294\n",
      "step 2800 , total_loss: 0.7785, data_loss: 0.7785\n",
      "step 2820 , total_loss: 0.7886, data_loss: 0.7886\n",
      "step 2840 , total_loss: 0.8008, data_loss: 0.8008\n",
      "step 2860 , total_loss: 0.8681, data_loss: 0.8681\n",
      "step 2880 , total_loss: 0.8196, data_loss: 0.8196\n",
      "step 2900 , total_loss: 0.8397, data_loss: 0.8397\n",
      "step 2920 , total_loss: 0.8526, data_loss: 0.8526\n",
      "step 2940 , total_loss: 0.8427, data_loss: 0.8427\n",
      "step 2960 , total_loss: 0.7032, data_loss: 0.7032\n",
      "step 2980 , total_loss: 0.8281, data_loss: 0.8281\n",
      "step 3000 , total_loss: 0.7959, data_loss: 0.7959\n",
      "step 3020 , total_loss: 0.8338, data_loss: 0.8338\n",
      "step 3040 , total_loss: 0.8523, data_loss: 0.8523\n",
      "step 3060 , total_loss: 0.8142, data_loss: 0.8142\n",
      "step 3080 , total_loss: 0.8083, data_loss: 0.8083\n",
      "step 3100 , total_loss: 0.8348, data_loss: 0.8348\n",
      "step 3120 , total_loss: 0.8381, data_loss: 0.8381\n",
      "step 3140 , total_loss: 0.8441, data_loss: 0.8441\n",
      "step 3160 , total_loss: 0.7931, data_loss: 0.7931\n",
      "step 3180 , total_loss: 0.7751, data_loss: 0.7751\n",
      "step 3200 , total_loss: 0.8091, data_loss: 0.8091\n",
      "step 3220 , total_loss: 0.8142, data_loss: 0.8142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3240 , total_loss: 0.8982, data_loss: 0.8982\n",
      "step 3260 , total_loss: 0.7971, data_loss: 0.7971\n",
      "step 3280 , total_loss: 0.8324, data_loss: 0.8324\n",
      "step 3300 , total_loss: 0.8017, data_loss: 0.8017\n",
      "eval valid at epoch 3: auc:0.8597,logloss:0.6299,mean_mrr:0.7973,ndcg@2:0.7753,ndcg@4:0.8389,ndcg@6:0.8484,group_auc:0.852\n",
      "step 20 , total_loss: 0.7803, data_loss: 0.7803\n",
      "step 40 , total_loss: 0.7818, data_loss: 0.7818\n",
      "step 60 , total_loss: 0.6993, data_loss: 0.6993\n",
      "step 80 , total_loss: 0.7842, data_loss: 0.7842\n",
      "step 100 , total_loss: 0.7506, data_loss: 0.7506\n",
      "step 120 , total_loss: 0.7033, data_loss: 0.7033\n",
      "step 140 , total_loss: 0.7871, data_loss: 0.7871\n",
      "step 160 , total_loss: 0.7303, data_loss: 0.7303\n",
      "step 180 , total_loss: 0.7689, data_loss: 0.7689\n",
      "step 200 , total_loss: 0.7822, data_loss: 0.7822\n",
      "step 220 , total_loss: 0.8179, data_loss: 0.8179\n",
      "step 240 , total_loss: 0.7720, data_loss: 0.7720\n",
      "step 260 , total_loss: 0.7611, data_loss: 0.7611\n",
      "step 280 , total_loss: 0.8572, data_loss: 0.8572\n",
      "step 300 , total_loss: 0.7628, data_loss: 0.7628\n",
      "step 320 , total_loss: 0.7358, data_loss: 0.7358\n",
      "step 340 , total_loss: 0.7452, data_loss: 0.7452\n",
      "step 360 , total_loss: 0.7383, data_loss: 0.7383\n",
      "step 380 , total_loss: 0.7812, data_loss: 0.7812\n",
      "step 400 , total_loss: 0.7625, data_loss: 0.7625\n",
      "step 420 , total_loss: 0.7747, data_loss: 0.7747\n",
      "step 440 , total_loss: 0.7831, data_loss: 0.7831\n",
      "step 460 , total_loss: 0.7219, data_loss: 0.7219\n",
      "step 480 , total_loss: 0.7528, data_loss: 0.7528\n",
      "step 500 , total_loss: 0.7151, data_loss: 0.7151\n",
      "step 520 , total_loss: 0.7477, data_loss: 0.7477\n",
      "step 540 , total_loss: 0.8223, data_loss: 0.8223\n",
      "step 560 , total_loss: 0.7916, data_loss: 0.7916\n",
      "step 580 , total_loss: 0.7686, data_loss: 0.7686\n",
      "step 600 , total_loss: 0.7262, data_loss: 0.7262\n",
      "step 620 , total_loss: 0.7269, data_loss: 0.7269\n",
      "step 640 , total_loss: 0.7158, data_loss: 0.7158\n",
      "step 660 , total_loss: 0.7367, data_loss: 0.7367\n",
      "step 680 , total_loss: 0.8284, data_loss: 0.8284\n",
      "step 700 , total_loss: 0.8011, data_loss: 0.8011\n",
      "step 720 , total_loss: 0.7891, data_loss: 0.7891\n",
      "step 740 , total_loss: 0.8330, data_loss: 0.8330\n",
      "step 760 , total_loss: 0.8894, data_loss: 0.8894\n",
      "step 780 , total_loss: 0.7358, data_loss: 0.7358\n",
      "step 800 , total_loss: 0.7413, data_loss: 0.7413\n",
      "step 820 , total_loss: 0.7335, data_loss: 0.7335\n",
      "step 840 , total_loss: 0.7487, data_loss: 0.7487\n",
      "step 860 , total_loss: 0.7142, data_loss: 0.7142\n",
      "step 880 , total_loss: 0.7381, data_loss: 0.7381\n",
      "step 900 , total_loss: 0.7909, data_loss: 0.7909\n",
      "step 920 , total_loss: 0.8264, data_loss: 0.8264\n",
      "step 940 , total_loss: 0.7936, data_loss: 0.7936\n",
      "step 960 , total_loss: 0.7767, data_loss: 0.7767\n",
      "step 980 , total_loss: 0.7221, data_loss: 0.7221\n",
      "step 1000 , total_loss: 0.7955, data_loss: 0.7955\n",
      "step 1020 , total_loss: 0.8096, data_loss: 0.8096\n",
      "step 1040 , total_loss: 0.7707, data_loss: 0.7707\n",
      "step 1060 , total_loss: 0.7624, data_loss: 0.7624\n",
      "step 1080 , total_loss: 0.8652, data_loss: 0.8652\n",
      "step 1100 , total_loss: 0.7879, data_loss: 0.7879\n",
      "step 1120 , total_loss: 0.7773, data_loss: 0.7773\n",
      "step 1140 , total_loss: 0.7182, data_loss: 0.7182\n",
      "step 1160 , total_loss: 0.7950, data_loss: 0.7950\n",
      "step 1180 , total_loss: 0.8498, data_loss: 0.8498\n",
      "step 1200 , total_loss: 0.7453, data_loss: 0.7453\n",
      "step 1220 , total_loss: 0.8196, data_loss: 0.8196\n",
      "step 1240 , total_loss: 0.7724, data_loss: 0.7724\n",
      "step 1260 , total_loss: 0.7192, data_loss: 0.7192\n",
      "step 1280 , total_loss: 0.7025, data_loss: 0.7025\n",
      "step 1300 , total_loss: 0.7996, data_loss: 0.7996\n",
      "step 1320 , total_loss: 0.7395, data_loss: 0.7395\n",
      "step 1340 , total_loss: 0.7260, data_loss: 0.7260\n",
      "step 1360 , total_loss: 0.8138, data_loss: 0.8138\n",
      "step 1380 , total_loss: 0.7189, data_loss: 0.7189\n",
      "step 1400 , total_loss: 0.7977, data_loss: 0.7977\n",
      "step 1420 , total_loss: 0.8430, data_loss: 0.8430\n",
      "step 1440 , total_loss: 0.8104, data_loss: 0.8104\n",
      "step 1460 , total_loss: 0.7892, data_loss: 0.7892\n",
      "step 1480 , total_loss: 0.8625, data_loss: 0.8625\n",
      "step 1500 , total_loss: 0.7932, data_loss: 0.7932\n",
      "step 1520 , total_loss: 0.7352, data_loss: 0.7352\n",
      "step 1540 , total_loss: 0.8082, data_loss: 0.8082\n",
      "step 1560 , total_loss: 0.8218, data_loss: 0.8218\n",
      "step 1580 , total_loss: 0.7867, data_loss: 0.7867\n",
      "step 1600 , total_loss: 0.8157, data_loss: 0.8157\n",
      "step 1620 , total_loss: 0.8007, data_loss: 0.8007\n",
      "step 1640 , total_loss: 0.8038, data_loss: 0.8038\n",
      "step 1660 , total_loss: 0.8481, data_loss: 0.8481\n",
      "step 1680 , total_loss: 0.7524, data_loss: 0.7524\n",
      "step 1700 , total_loss: 0.8649, data_loss: 0.8649\n",
      "step 1720 , total_loss: 0.7708, data_loss: 0.7708\n",
      "step 1740 , total_loss: 0.7287, data_loss: 0.7287\n",
      "step 1760 , total_loss: 0.8414, data_loss: 0.8414\n",
      "step 1780 , total_loss: 0.7985, data_loss: 0.7985\n",
      "step 1800 , total_loss: 0.7502, data_loss: 0.7502\n",
      "step 1820 , total_loss: 0.7892, data_loss: 0.7892\n",
      "step 1840 , total_loss: 0.8546, data_loss: 0.8546\n",
      "step 1860 , total_loss: 0.7558, data_loss: 0.7558\n",
      "step 1880 , total_loss: 0.6969, data_loss: 0.6969\n",
      "step 1900 , total_loss: 0.7327, data_loss: 0.7327\n",
      "step 1920 , total_loss: 0.7876, data_loss: 0.7876\n",
      "step 1940 , total_loss: 0.7851, data_loss: 0.7851\n",
      "step 1960 , total_loss: 0.7103, data_loss: 0.7103\n",
      "step 1980 , total_loss: 0.7985, data_loss: 0.7985\n",
      "step 2000 , total_loss: 0.7621, data_loss: 0.7621\n",
      "step 2020 , total_loss: 0.7878, data_loss: 0.7878\n",
      "step 2040 , total_loss: 0.7847, data_loss: 0.7847\n",
      "step 2060 , total_loss: 0.7554, data_loss: 0.7554\n",
      "step 2080 , total_loss: 0.7755, data_loss: 0.7755\n",
      "step 2100 , total_loss: 0.6886, data_loss: 0.6886\n",
      "step 2120 , total_loss: 0.7198, data_loss: 0.7198\n",
      "step 2140 , total_loss: 0.7312, data_loss: 0.7312\n",
      "step 2160 , total_loss: 0.7761, data_loss: 0.7761\n",
      "step 2180 , total_loss: 0.7420, data_loss: 0.7420\n",
      "step 2200 , total_loss: 0.7684, data_loss: 0.7684\n",
      "step 2220 , total_loss: 0.7023, data_loss: 0.7023\n",
      "step 2240 , total_loss: 0.7673, data_loss: 0.7673\n",
      "step 2260 , total_loss: 0.7436, data_loss: 0.7436\n",
      "step 2280 , total_loss: 0.7815, data_loss: 0.7815\n",
      "step 2300 , total_loss: 0.7747, data_loss: 0.7747\n",
      "step 2320 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 2340 , total_loss: 0.6885, data_loss: 0.6885\n",
      "step 2360 , total_loss: 0.7152, data_loss: 0.7152\n",
      "step 2380 , total_loss: 0.7228, data_loss: 0.7228\n",
      "step 2400 , total_loss: 0.7811, data_loss: 0.7811\n",
      "step 2420 , total_loss: 0.7791, data_loss: 0.7791\n",
      "step 2440 , total_loss: 0.7968, data_loss: 0.7968\n",
      "step 2460 , total_loss: 0.7269, data_loss: 0.7269\n",
      "step 2480 , total_loss: 0.7386, data_loss: 0.7386\n",
      "step 2500 , total_loss: 0.7412, data_loss: 0.7412\n",
      "step 2520 , total_loss: 0.7636, data_loss: 0.7636\n",
      "step 2540 , total_loss: 0.8057, data_loss: 0.8057\n",
      "step 2560 , total_loss: 0.7486, data_loss: 0.7486\n",
      "step 2580 , total_loss: 0.7477, data_loss: 0.7477\n",
      "step 2600 , total_loss: 0.8377, data_loss: 0.8377\n",
      "step 2620 , total_loss: 0.8064, data_loss: 0.8064\n",
      "step 2640 , total_loss: 0.7789, data_loss: 0.7789\n",
      "step 2660 , total_loss: 0.7777, data_loss: 0.7777\n",
      "step 2680 , total_loss: 0.7828, data_loss: 0.7828\n",
      "step 2700 , total_loss: 0.7699, data_loss: 0.7699\n",
      "step 2720 , total_loss: 0.8111, data_loss: 0.8111\n",
      "step 2740 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 2760 , total_loss: 0.7902, data_loss: 0.7902\n",
      "step 2780 , total_loss: 0.8492, data_loss: 0.8492\n",
      "step 2800 , total_loss: 0.8463, data_loss: 0.8463\n",
      "step 2820 , total_loss: 0.7889, data_loss: 0.7889\n",
      "step 2840 , total_loss: 0.7655, data_loss: 0.7655\n",
      "step 2860 , total_loss: 0.7910, data_loss: 0.7910\n",
      "step 2880 , total_loss: 0.7657, data_loss: 0.7657\n",
      "step 2900 , total_loss: 0.7406, data_loss: 0.7406\n",
      "step 2920 , total_loss: 0.8442, data_loss: 0.8442\n",
      "step 2940 , total_loss: 0.7272, data_loss: 0.7272\n",
      "step 2960 , total_loss: 0.7483, data_loss: 0.7483\n",
      "step 2980 , total_loss: 0.7572, data_loss: 0.7572\n",
      "step 3000 , total_loss: 0.6959, data_loss: 0.6959\n",
      "step 3020 , total_loss: 0.7829, data_loss: 0.7829\n",
      "step 3040 , total_loss: 0.7854, data_loss: 0.7854\n",
      "step 3060 , total_loss: 0.7963, data_loss: 0.7963\n",
      "step 3080 , total_loss: 0.6957, data_loss: 0.6957\n",
      "step 3100 , total_loss: 0.7749, data_loss: 0.7749\n",
      "step 3120 , total_loss: 0.7721, data_loss: 0.7721\n",
      "step 3140 , total_loss: 0.7017, data_loss: 0.7017\n",
      "step 3160 , total_loss: 0.7883, data_loss: 0.7883\n",
      "step 3180 , total_loss: 0.7689, data_loss: 0.7689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200 , total_loss: 0.8325, data_loss: 0.8325\n",
      "step 3220 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 3240 , total_loss: 0.8447, data_loss: 0.8447\n",
      "step 3260 , total_loss: 0.7237, data_loss: 0.7237\n",
      "step 3280 , total_loss: 0.7226, data_loss: 0.7226\n",
      "step 3300 , total_loss: 0.7449, data_loss: 0.7449\n",
      "eval valid at epoch 4: auc:0.8659,logloss:0.6379,mean_mrr:0.8058,ndcg@2:0.7858,ndcg@4:0.8458,ndcg@6:0.8548,group_auc:0.8586\n",
      "step 20 , total_loss: 0.6602, data_loss: 0.6602\n",
      "step 40 , total_loss: 0.6964, data_loss: 0.6964\n",
      "step 60 , total_loss: 0.6832, data_loss: 0.6832\n",
      "step 80 , total_loss: 0.6675, data_loss: 0.6675\n",
      "step 100 , total_loss: 0.7002, data_loss: 0.7002\n",
      "step 120 , total_loss: 0.7581, data_loss: 0.7581\n",
      "step 140 , total_loss: 0.6328, data_loss: 0.6328\n",
      "step 160 , total_loss: 0.7335, data_loss: 0.7335\n",
      "step 180 , total_loss: 0.7955, data_loss: 0.7955\n",
      "step 200 , total_loss: 0.6876, data_loss: 0.6876\n",
      "step 220 , total_loss: 0.6424, data_loss: 0.6424\n",
      "step 240 , total_loss: 0.6942, data_loss: 0.6942\n",
      "step 260 , total_loss: 0.6665, data_loss: 0.6665\n",
      "step 280 , total_loss: 0.7772, data_loss: 0.7772\n",
      "step 300 , total_loss: 0.6883, data_loss: 0.6883\n",
      "step 320 , total_loss: 0.7516, data_loss: 0.7516\n",
      "step 340 , total_loss: 0.6577, data_loss: 0.6577\n",
      "step 360 , total_loss: 0.7272, data_loss: 0.7272\n",
      "step 380 , total_loss: 0.7080, data_loss: 0.7080\n",
      "step 400 , total_loss: 0.6985, data_loss: 0.6985\n",
      "step 420 , total_loss: 0.8109, data_loss: 0.8109\n",
      "step 440 , total_loss: 0.7360, data_loss: 0.7360\n",
      "step 460 , total_loss: 0.6823, data_loss: 0.6823\n",
      "step 480 , total_loss: 0.7217, data_loss: 0.7217\n",
      "step 500 , total_loss: 0.7417, data_loss: 0.7417\n",
      "step 520 , total_loss: 0.6854, data_loss: 0.6854\n",
      "step 540 , total_loss: 0.6907, data_loss: 0.6907\n",
      "step 560 , total_loss: 0.7499, data_loss: 0.7499\n",
      "step 580 , total_loss: 0.7765, data_loss: 0.7765\n",
      "step 600 , total_loss: 0.7661, data_loss: 0.7661\n",
      "step 620 , total_loss: 0.7298, data_loss: 0.7298\n",
      "step 640 , total_loss: 0.6864, data_loss: 0.6864\n",
      "step 660 , total_loss: 0.6215, data_loss: 0.6215\n",
      "step 680 , total_loss: 0.6612, data_loss: 0.6612\n",
      "step 700 , total_loss: 0.7395, data_loss: 0.7395\n",
      "step 720 , total_loss: 0.7313, data_loss: 0.7313\n",
      "step 740 , total_loss: 0.7250, data_loss: 0.7250\n",
      "step 760 , total_loss: 0.7726, data_loss: 0.7726\n",
      "step 780 , total_loss: 0.6979, data_loss: 0.6979\n",
      "step 800 , total_loss: 0.8332, data_loss: 0.8332\n",
      "step 820 , total_loss: 0.6909, data_loss: 0.6909\n",
      "step 840 , total_loss: 0.7674, data_loss: 0.7674\n",
      "step 860 , total_loss: 0.8237, data_loss: 0.8237\n",
      "step 880 , total_loss: 0.6971, data_loss: 0.6971\n",
      "step 900 , total_loss: 0.7388, data_loss: 0.7388\n",
      "step 920 , total_loss: 0.7377, data_loss: 0.7377\n",
      "step 940 , total_loss: 0.6682, data_loss: 0.6682\n",
      "step 960 , total_loss: 0.6951, data_loss: 0.6951\n",
      "step 980 , total_loss: 0.8429, data_loss: 0.8429\n",
      "step 1000 , total_loss: 0.7673, data_loss: 0.7673\n",
      "step 1020 , total_loss: 0.8575, data_loss: 0.8575\n",
      "step 1040 , total_loss: 0.7394, data_loss: 0.7394\n",
      "step 1060 , total_loss: 0.7710, data_loss: 0.7710\n",
      "step 1080 , total_loss: 0.7855, data_loss: 0.7855\n",
      "step 1100 , total_loss: 0.6833, data_loss: 0.6833\n",
      "step 1120 , total_loss: 0.6868, data_loss: 0.6868\n",
      "step 1140 , total_loss: 0.6992, data_loss: 0.6992\n",
      "step 1160 , total_loss: 0.7627, data_loss: 0.7627\n",
      "step 1180 , total_loss: 0.6938, data_loss: 0.6938\n",
      "step 1200 , total_loss: 0.7902, data_loss: 0.7902\n",
      "step 1220 , total_loss: 0.7503, data_loss: 0.7503\n",
      "step 1240 , total_loss: 0.7455, data_loss: 0.7455\n",
      "step 1260 , total_loss: 0.6991, data_loss: 0.6991\n",
      "step 1280 , total_loss: 0.6854, data_loss: 0.6854\n",
      "step 1300 , total_loss: 0.7100, data_loss: 0.7100\n",
      "step 1320 , total_loss: 0.6940, data_loss: 0.6940\n",
      "step 1340 , total_loss: 0.7016, data_loss: 0.7016\n",
      "step 1360 , total_loss: 0.6740, data_loss: 0.6740\n",
      "step 1380 , total_loss: 0.7055, data_loss: 0.7055\n",
      "step 1400 , total_loss: 0.7738, data_loss: 0.7738\n",
      "step 1420 , total_loss: 0.7248, data_loss: 0.7248\n",
      "step 1440 , total_loss: 0.6939, data_loss: 0.6939\n",
      "step 1460 , total_loss: 0.8174, data_loss: 0.8174\n",
      "step 1480 , total_loss: 0.7016, data_loss: 0.7016\n",
      "step 1500 , total_loss: 0.7644, data_loss: 0.7644\n",
      "step 1520 , total_loss: 0.7780, data_loss: 0.7780\n",
      "step 1540 , total_loss: 0.7842, data_loss: 0.7842\n",
      "step 1560 , total_loss: 0.6440, data_loss: 0.6440\n",
      "step 1580 , total_loss: 0.7384, data_loss: 0.7384\n",
      "step 1600 , total_loss: 0.6927, data_loss: 0.6927\n",
      "step 1620 , total_loss: 0.6470, data_loss: 0.6470\n",
      "step 1640 , total_loss: 0.7031, data_loss: 0.7031\n",
      "step 1660 , total_loss: 0.7558, data_loss: 0.7558\n",
      "step 1680 , total_loss: 0.8136, data_loss: 0.8136\n",
      "step 1700 , total_loss: 0.7259, data_loss: 0.7259\n",
      "step 1720 , total_loss: 0.8228, data_loss: 0.8228\n",
      "step 1740 , total_loss: 0.7879, data_loss: 0.7879\n",
      "step 1760 , total_loss: 0.7110, data_loss: 0.7110\n",
      "step 1780 , total_loss: 0.7693, data_loss: 0.7693\n",
      "step 1800 , total_loss: 0.7584, data_loss: 0.7584\n",
      "step 1820 , total_loss: 0.6311, data_loss: 0.6311\n",
      "step 1840 , total_loss: 0.7027, data_loss: 0.7027\n",
      "step 1860 , total_loss: 0.6994, data_loss: 0.6994\n",
      "step 1880 , total_loss: 0.8240, data_loss: 0.8240\n",
      "step 1900 , total_loss: 0.8096, data_loss: 0.8096\n",
      "step 1920 , total_loss: 0.7231, data_loss: 0.7231\n",
      "step 1940 , total_loss: 0.7374, data_loss: 0.7374\n",
      "step 1960 , total_loss: 0.7448, data_loss: 0.7448\n",
      "step 1980 , total_loss: 0.6916, data_loss: 0.6916\n",
      "step 2000 , total_loss: 0.7418, data_loss: 0.7418\n",
      "step 2020 , total_loss: 0.7479, data_loss: 0.7479\n",
      "step 2040 , total_loss: 0.7398, data_loss: 0.7398\n",
      "step 2060 , total_loss: 0.7079, data_loss: 0.7079\n",
      "step 2080 , total_loss: 0.7302, data_loss: 0.7302\n",
      "step 2100 , total_loss: 0.7378, data_loss: 0.7378\n",
      "step 2120 , total_loss: 0.6921, data_loss: 0.6921\n",
      "step 2140 , total_loss: 0.8333, data_loss: 0.8333\n",
      "step 2160 , total_loss: 0.7033, data_loss: 0.7033\n",
      "step 2180 , total_loss: 0.7464, data_loss: 0.7464\n",
      "step 2200 , total_loss: 0.7166, data_loss: 0.7166\n",
      "step 2220 , total_loss: 0.7112, data_loss: 0.7112\n",
      "step 2240 , total_loss: 0.7284, data_loss: 0.7284\n",
      "step 2260 , total_loss: 0.6714, data_loss: 0.6714\n",
      "step 2280 , total_loss: 0.6665, data_loss: 0.6665\n",
      "step 2300 , total_loss: 0.7328, data_loss: 0.7328\n",
      "step 2320 , total_loss: 0.6871, data_loss: 0.6871\n",
      "step 2340 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 2360 , total_loss: 0.7446, data_loss: 0.7446\n",
      "step 2380 , total_loss: 0.7021, data_loss: 0.7021\n",
      "step 2400 , total_loss: 0.7827, data_loss: 0.7827\n",
      "step 2420 , total_loss: 0.7034, data_loss: 0.7034\n",
      "step 2440 , total_loss: 0.7126, data_loss: 0.7126\n",
      "step 2460 , total_loss: 0.7457, data_loss: 0.7457\n",
      "step 2480 , total_loss: 0.6467, data_loss: 0.6467\n",
      "step 2500 , total_loss: 0.6598, data_loss: 0.6598\n",
      "step 2520 , total_loss: 0.7422, data_loss: 0.7422\n",
      "step 2540 , total_loss: 0.7581, data_loss: 0.7581\n",
      "step 2560 , total_loss: 0.6981, data_loss: 0.6981\n",
      "step 2580 , total_loss: 0.6861, data_loss: 0.6861\n",
      "step 2600 , total_loss: 0.7729, data_loss: 0.7729\n",
      "step 2620 , total_loss: 0.8443, data_loss: 0.8443\n",
      "step 2640 , total_loss: 0.7221, data_loss: 0.7221\n",
      "step 2660 , total_loss: 0.8026, data_loss: 0.8026\n",
      "step 2680 , total_loss: 0.7850, data_loss: 0.7850\n",
      "step 2700 , total_loss: 0.6718, data_loss: 0.6718\n",
      "step 2720 , total_loss: 0.7714, data_loss: 0.7714\n",
      "step 2740 , total_loss: 0.7693, data_loss: 0.7693\n",
      "step 2760 , total_loss: 0.8372, data_loss: 0.8372\n",
      "step 2780 , total_loss: 0.7969, data_loss: 0.7969\n",
      "step 2800 , total_loss: 0.7464, data_loss: 0.7464\n",
      "step 2820 , total_loss: 0.7609, data_loss: 0.7609\n",
      "step 2840 , total_loss: 0.7349, data_loss: 0.7349\n",
      "step 2860 , total_loss: 0.7816, data_loss: 0.7816\n",
      "step 2880 , total_loss: 0.7091, data_loss: 0.7091\n",
      "step 2900 , total_loss: 0.7382, data_loss: 0.7382\n",
      "step 2920 , total_loss: 0.7449, data_loss: 0.7449\n",
      "step 2940 , total_loss: 0.7033, data_loss: 0.7033\n",
      "step 2960 , total_loss: 0.7301, data_loss: 0.7301\n",
      "step 2980 , total_loss: 0.7067, data_loss: 0.7067\n",
      "step 3000 , total_loss: 0.6963, data_loss: 0.6963\n",
      "step 3020 , total_loss: 0.6752, data_loss: 0.6752\n",
      "step 3040 , total_loss: 0.7288, data_loss: 0.7288\n",
      "step 3060 , total_loss: 0.6614, data_loss: 0.6614\n",
      "step 3080 , total_loss: 0.7258, data_loss: 0.7258\n",
      "step 3100 , total_loss: 0.7444, data_loss: 0.7444\n",
      "step 3120 , total_loss: 0.7765, data_loss: 0.7765\n",
      "step 3140 , total_loss: 0.6967, data_loss: 0.6967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3160 , total_loss: 0.7190, data_loss: 0.7190\n",
      "step 3180 , total_loss: 0.6824, data_loss: 0.6824\n",
      "step 3200 , total_loss: 0.7830, data_loss: 0.7830\n",
      "step 3220 , total_loss: 0.7178, data_loss: 0.7178\n",
      "step 3240 , total_loss: 0.7096, data_loss: 0.7096\n",
      "step 3260 , total_loss: 0.7035, data_loss: 0.7035\n",
      "step 3280 , total_loss: 0.8276, data_loss: 0.8276\n",
      "step 3300 , total_loss: 0.6459, data_loss: 0.6459\n",
      "eval valid at epoch 5: auc:0.8728,logloss:0.7035,mean_mrr:0.8128,ndcg@2:0.7947,ndcg@4:0.8519,ndcg@6:0.8601,group_auc:0.865\n",
      "step 20 , total_loss: 0.6809, data_loss: 0.6809\n",
      "step 40 , total_loss: 0.6767, data_loss: 0.6767\n",
      "step 60 , total_loss: 0.6458, data_loss: 0.6458\n",
      "step 80 , total_loss: 0.7209, data_loss: 0.7209\n",
      "step 100 , total_loss: 0.7163, data_loss: 0.7163\n",
      "step 120 , total_loss: 0.6155, data_loss: 0.6155\n",
      "step 140 , total_loss: 0.7312, data_loss: 0.7312\n",
      "step 160 , total_loss: 0.6881, data_loss: 0.6881\n",
      "step 180 , total_loss: 0.6097, data_loss: 0.6097\n",
      "step 200 , total_loss: 0.7546, data_loss: 0.7546\n",
      "step 220 , total_loss: 0.8003, data_loss: 0.8003\n",
      "step 240 , total_loss: 0.6791, data_loss: 0.6791\n",
      "step 260 , total_loss: 0.7803, data_loss: 0.7803\n",
      "step 280 , total_loss: 0.6989, data_loss: 0.6989\n",
      "step 300 , total_loss: 0.5843, data_loss: 0.5843\n",
      "step 320 , total_loss: 0.6462, data_loss: 0.6462\n",
      "step 340 , total_loss: 0.7041, data_loss: 0.7041\n",
      "step 360 , total_loss: 0.6659, data_loss: 0.6659\n",
      "step 380 , total_loss: 0.6386, data_loss: 0.6386\n",
      "step 400 , total_loss: 0.6985, data_loss: 0.6985\n",
      "step 420 , total_loss: 0.6273, data_loss: 0.6273\n",
      "step 440 , total_loss: 0.6790, data_loss: 0.6790\n",
      "step 460 , total_loss: 0.6821, data_loss: 0.6821\n",
      "step 480 , total_loss: 0.7528, data_loss: 0.7528\n",
      "step 500 , total_loss: 0.6803, data_loss: 0.6803\n",
      "step 520 , total_loss: 0.6672, data_loss: 0.6672\n",
      "step 540 , total_loss: 0.7188, data_loss: 0.7188\n",
      "step 560 , total_loss: 0.6556, data_loss: 0.6556\n",
      "step 580 , total_loss: 0.7717, data_loss: 0.7717\n",
      "step 600 , total_loss: 0.6418, data_loss: 0.6418\n",
      "step 620 , total_loss: 0.6800, data_loss: 0.6800\n",
      "step 640 , total_loss: 0.6303, data_loss: 0.6303\n",
      "step 660 , total_loss: 0.6618, data_loss: 0.6618\n",
      "step 680 , total_loss: 0.7313, data_loss: 0.7313\n",
      "step 700 , total_loss: 0.6508, data_loss: 0.6508\n",
      "step 720 , total_loss: 0.6459, data_loss: 0.6459\n",
      "step 740 , total_loss: 0.6391, data_loss: 0.6391\n",
      "step 760 , total_loss: 0.6448, data_loss: 0.6448\n",
      "step 780 , total_loss: 0.6633, data_loss: 0.6633\n",
      "step 800 , total_loss: 0.5999, data_loss: 0.5999\n",
      "step 820 , total_loss: 0.6230, data_loss: 0.6230\n",
      "step 840 , total_loss: 0.7068, data_loss: 0.7068\n",
      "step 860 , total_loss: 0.6370, data_loss: 0.6370\n",
      "step 880 , total_loss: 0.6363, data_loss: 0.6363\n",
      "step 900 , total_loss: 0.5960, data_loss: 0.5960\n",
      "step 920 , total_loss: 0.6401, data_loss: 0.6401\n",
      "step 940 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 960 , total_loss: 0.6958, data_loss: 0.6958\n",
      "step 980 , total_loss: 0.6299, data_loss: 0.6299\n",
      "step 1000 , total_loss: 0.6555, data_loss: 0.6555\n",
      "step 1020 , total_loss: 0.6946, data_loss: 0.6946\n",
      "step 1040 , total_loss: 0.7204, data_loss: 0.7204\n",
      "step 1060 , total_loss: 0.6379, data_loss: 0.6379\n",
      "step 1080 , total_loss: 0.7251, data_loss: 0.7251\n",
      "step 1100 , total_loss: 0.6050, data_loss: 0.6050\n",
      "step 1120 , total_loss: 0.6791, data_loss: 0.6791\n",
      "step 1140 , total_loss: 0.7091, data_loss: 0.7091\n",
      "step 1160 , total_loss: 0.6571, data_loss: 0.6571\n",
      "step 1180 , total_loss: 0.7140, data_loss: 0.7140\n",
      "step 1200 , total_loss: 0.6944, data_loss: 0.6944\n",
      "step 1220 , total_loss: 0.6823, data_loss: 0.6823\n",
      "step 1240 , total_loss: 0.6339, data_loss: 0.6339\n",
      "step 1260 , total_loss: 0.6284, data_loss: 0.6284\n",
      "step 1280 , total_loss: 0.7383, data_loss: 0.7383\n",
      "step 1300 , total_loss: 0.6440, data_loss: 0.6440\n",
      "step 1320 , total_loss: 0.6710, data_loss: 0.6710\n",
      "step 1340 , total_loss: 0.6575, data_loss: 0.6575\n",
      "step 1360 , total_loss: 0.6377, data_loss: 0.6377\n",
      "step 1380 , total_loss: 0.7117, data_loss: 0.7117\n",
      "step 1400 , total_loss: 0.6579, data_loss: 0.6579\n",
      "step 1420 , total_loss: 0.6838, data_loss: 0.6838\n",
      "step 1440 , total_loss: 0.6338, data_loss: 0.6338\n",
      "step 1460 , total_loss: 0.7236, data_loss: 0.7236\n",
      "step 1480 , total_loss: 0.6432, data_loss: 0.6432\n",
      "step 1500 , total_loss: 0.6992, data_loss: 0.6992\n",
      "step 1520 , total_loss: 0.7061, data_loss: 0.7061\n",
      "step 1540 , total_loss: 0.6683, data_loss: 0.6683\n",
      "step 1560 , total_loss: 0.6772, data_loss: 0.6772\n",
      "step 1580 , total_loss: 0.7536, data_loss: 0.7536\n",
      "step 1600 , total_loss: 0.6763, data_loss: 0.6763\n",
      "step 1620 , total_loss: 0.7077, data_loss: 0.7077\n",
      "step 1640 , total_loss: 0.6370, data_loss: 0.6370\n",
      "step 1660 , total_loss: 0.6301, data_loss: 0.6301\n",
      "step 1680 , total_loss: 0.8052, data_loss: 0.8052\n",
      "step 1700 , total_loss: 0.6355, data_loss: 0.6355\n",
      "step 1720 , total_loss: 0.7297, data_loss: 0.7297\n",
      "step 1740 , total_loss: 0.7111, data_loss: 0.7111\n",
      "step 1760 , total_loss: 0.7100, data_loss: 0.7100\n",
      "step 1780 , total_loss: 0.7552, data_loss: 0.7552\n",
      "step 1800 , total_loss: 0.7367, data_loss: 0.7367\n",
      "step 1820 , total_loss: 0.6459, data_loss: 0.6459\n",
      "step 1840 , total_loss: 0.6271, data_loss: 0.6271\n",
      "step 1860 , total_loss: 0.7401, data_loss: 0.7401\n",
      "step 1880 , total_loss: 0.7502, data_loss: 0.7502\n",
      "step 1900 , total_loss: 0.7514, data_loss: 0.7514\n",
      "step 1920 , total_loss: 0.7131, data_loss: 0.7131\n",
      "step 1940 , total_loss: 0.6705, data_loss: 0.6705\n",
      "step 1960 , total_loss: 0.6222, data_loss: 0.6222\n",
      "step 1980 , total_loss: 0.7551, data_loss: 0.7551\n",
      "step 2000 , total_loss: 0.6622, data_loss: 0.6622\n",
      "step 2020 , total_loss: 0.6761, data_loss: 0.6761\n",
      "step 2040 , total_loss: 0.6414, data_loss: 0.6414\n",
      "step 2060 , total_loss: 0.7679, data_loss: 0.7679\n",
      "step 2080 , total_loss: 0.7718, data_loss: 0.7718\n",
      "step 2100 , total_loss: 0.7824, data_loss: 0.7824\n",
      "step 2120 , total_loss: 0.6897, data_loss: 0.6897\n",
      "step 2140 , total_loss: 0.6738, data_loss: 0.6738\n",
      "step 2160 , total_loss: 0.6757, data_loss: 0.6757\n",
      "step 2180 , total_loss: 0.8095, data_loss: 0.8095\n",
      "step 2200 , total_loss: 0.6602, data_loss: 0.6602\n",
      "step 2220 , total_loss: 0.7344, data_loss: 0.7344\n",
      "step 2240 , total_loss: 0.6750, data_loss: 0.6750\n",
      "step 2260 , total_loss: 0.7229, data_loss: 0.7229\n",
      "step 2280 , total_loss: 0.6571, data_loss: 0.6571\n",
      "step 2300 , total_loss: 0.6979, data_loss: 0.6979\n",
      "step 2320 , total_loss: 0.7092, data_loss: 0.7092\n",
      "step 2340 , total_loss: 0.6862, data_loss: 0.6862\n",
      "step 2360 , total_loss: 0.7985, data_loss: 0.7985\n",
      "step 2380 , total_loss: 0.6221, data_loss: 0.6221\n",
      "step 2400 , total_loss: 0.7197, data_loss: 0.7197\n",
      "step 2420 , total_loss: 0.7199, data_loss: 0.7199\n",
      "step 2440 , total_loss: 0.6899, data_loss: 0.6899\n",
      "step 2460 , total_loss: 0.7500, data_loss: 0.7500\n",
      "step 2480 , total_loss: 0.6873, data_loss: 0.6873\n",
      "step 2500 , total_loss: 0.6983, data_loss: 0.6983\n",
      "step 2520 , total_loss: 0.6903, data_loss: 0.6903\n",
      "step 2540 , total_loss: 0.6540, data_loss: 0.6540\n",
      "step 2560 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 2580 , total_loss: 0.7122, data_loss: 0.7122\n",
      "step 2600 , total_loss: 0.7187, data_loss: 0.7187\n",
      "step 2620 , total_loss: 0.7199, data_loss: 0.7199\n",
      "step 2640 , total_loss: 0.7651, data_loss: 0.7651\n",
      "step 2660 , total_loss: 0.7516, data_loss: 0.7516\n",
      "step 2680 , total_loss: 0.7256, data_loss: 0.7256\n",
      "step 2700 , total_loss: 0.6891, data_loss: 0.6891\n",
      "step 2720 , total_loss: 0.7841, data_loss: 0.7841\n",
      "step 2740 , total_loss: 0.7549, data_loss: 0.7549\n",
      "step 2760 , total_loss: 0.6466, data_loss: 0.6466\n",
      "step 2780 , total_loss: 0.6955, data_loss: 0.6955\n",
      "step 2800 , total_loss: 0.7454, data_loss: 0.7454\n",
      "step 2820 , total_loss: 0.7502, data_loss: 0.7502\n",
      "step 2840 , total_loss: 0.6599, data_loss: 0.6599\n",
      "step 2860 , total_loss: 0.7283, data_loss: 0.7283\n",
      "step 2880 , total_loss: 0.6883, data_loss: 0.6883\n",
      "step 2900 , total_loss: 0.6444, data_loss: 0.6444\n",
      "step 2920 , total_loss: 0.8330, data_loss: 0.8330\n",
      "step 2940 , total_loss: 0.6822, data_loss: 0.6822\n",
      "step 2960 , total_loss: 0.6765, data_loss: 0.6765\n",
      "step 2980 , total_loss: 0.6894, data_loss: 0.6894\n",
      "step 3000 , total_loss: 0.7361, data_loss: 0.7361\n",
      "step 3020 , total_loss: 0.7506, data_loss: 0.7506\n",
      "step 3040 , total_loss: 0.6679, data_loss: 0.6679\n",
      "step 3060 , total_loss: 0.6752, data_loss: 0.6752\n",
      "step 3080 , total_loss: 0.6569, data_loss: 0.6569\n",
      "step 3100 , total_loss: 0.6595, data_loss: 0.6595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3120 , total_loss: 0.6497, data_loss: 0.6497\n",
      "step 3140 , total_loss: 0.7040, data_loss: 0.7040\n",
      "step 3160 , total_loss: 0.7166, data_loss: 0.7166\n",
      "step 3180 , total_loss: 0.7923, data_loss: 0.7923\n",
      "step 3200 , total_loss: 0.6941, data_loss: 0.6941\n",
      "step 3220 , total_loss: 0.7387, data_loss: 0.7387\n",
      "step 3240 , total_loss: 0.6739, data_loss: 0.6739\n",
      "step 3260 , total_loss: 0.6853, data_loss: 0.6853\n",
      "step 3280 , total_loss: 0.6998, data_loss: 0.6998\n",
      "step 3300 , total_loss: 0.7076, data_loss: 0.7076\n",
      "eval valid at epoch 6: auc:0.8742,logloss:0.7442,mean_mrr:0.8158,ndcg@2:0.7979,ndcg@4:0.8543,ndcg@6:0.8623,group_auc:0.8671\n",
      "step 20 , total_loss: 0.6962, data_loss: 0.6962\n",
      "step 40 , total_loss: 0.5327, data_loss: 0.5327\n",
      "step 60 , total_loss: 0.6270, data_loss: 0.6270\n",
      "step 80 , total_loss: 0.6917, data_loss: 0.6917\n",
      "step 100 , total_loss: 0.6974, data_loss: 0.6974\n",
      "step 120 , total_loss: 0.6651, data_loss: 0.6651\n",
      "step 140 , total_loss: 0.6398, data_loss: 0.6398\n",
      "step 160 , total_loss: 0.6102, data_loss: 0.6102\n",
      "step 180 , total_loss: 0.6458, data_loss: 0.6458\n",
      "step 200 , total_loss: 0.5930, data_loss: 0.5930\n",
      "step 220 , total_loss: 0.7006, data_loss: 0.7006\n",
      "step 240 , total_loss: 0.7130, data_loss: 0.7130\n",
      "step 260 , total_loss: 0.6612, data_loss: 0.6612\n",
      "step 280 , total_loss: 0.6262, data_loss: 0.6262\n",
      "step 300 , total_loss: 0.7508, data_loss: 0.7508\n",
      "step 320 , total_loss: 0.6932, data_loss: 0.6932\n",
      "step 340 , total_loss: 0.6685, data_loss: 0.6685\n",
      "step 360 , total_loss: 0.6620, data_loss: 0.6620\n",
      "step 380 , total_loss: 0.6900, data_loss: 0.6900\n",
      "step 400 , total_loss: 0.7146, data_loss: 0.7146\n",
      "step 420 , total_loss: 0.6467, data_loss: 0.6467\n",
      "step 440 , total_loss: 0.6652, data_loss: 0.6652\n",
      "step 460 , total_loss: 0.5738, data_loss: 0.5738\n",
      "step 480 , total_loss: 0.6729, data_loss: 0.6729\n",
      "step 500 , total_loss: 0.6794, data_loss: 0.6794\n",
      "step 520 , total_loss: 0.6166, data_loss: 0.6166\n",
      "step 540 , total_loss: 0.6327, data_loss: 0.6327\n",
      "step 560 , total_loss: 0.5598, data_loss: 0.5598\n",
      "step 580 , total_loss: 0.6392, data_loss: 0.6392\n",
      "step 600 , total_loss: 0.6604, data_loss: 0.6604\n",
      "step 620 , total_loss: 0.5690, data_loss: 0.5690\n",
      "step 640 , total_loss: 0.6506, data_loss: 0.6506\n",
      "step 660 , total_loss: 0.6503, data_loss: 0.6503\n",
      "step 680 , total_loss: 0.5886, data_loss: 0.5886\n",
      "step 700 , total_loss: 0.7794, data_loss: 0.7794\n",
      "step 720 , total_loss: 0.7153, data_loss: 0.7153\n",
      "step 740 , total_loss: 0.6823, data_loss: 0.6823\n",
      "step 760 , total_loss: 0.6139, data_loss: 0.6139\n",
      "step 780 , total_loss: 0.6751, data_loss: 0.6751\n",
      "step 800 , total_loss: 0.6805, data_loss: 0.6805\n",
      "step 820 , total_loss: 0.6338, data_loss: 0.6338\n",
      "step 840 , total_loss: 0.6149, data_loss: 0.6149\n",
      "step 860 , total_loss: 0.6335, data_loss: 0.6335\n",
      "step 880 , total_loss: 0.6511, data_loss: 0.6511\n",
      "step 900 , total_loss: 0.7113, data_loss: 0.7113\n",
      "step 920 , total_loss: 0.6946, data_loss: 0.6946\n",
      "step 940 , total_loss: 0.6477, data_loss: 0.6477\n",
      "step 960 , total_loss: 0.6318, data_loss: 0.6318\n",
      "step 980 , total_loss: 0.6552, data_loss: 0.6552\n",
      "step 1000 , total_loss: 0.6891, data_loss: 0.6891\n",
      "step 1020 , total_loss: 0.6847, data_loss: 0.6847\n",
      "step 1040 , total_loss: 0.7254, data_loss: 0.7254\n",
      "step 1060 , total_loss: 0.7051, data_loss: 0.7051\n",
      "step 1080 , total_loss: 0.5791, data_loss: 0.5791\n",
      "step 1100 , total_loss: 0.6487, data_loss: 0.6487\n",
      "step 1120 , total_loss: 0.7383, data_loss: 0.7383\n",
      "step 1140 , total_loss: 0.5850, data_loss: 0.5850\n",
      "step 1160 , total_loss: 0.6745, data_loss: 0.6745\n",
      "step 1180 , total_loss: 0.6006, data_loss: 0.6006\n",
      "step 1200 , total_loss: 0.6836, data_loss: 0.6836\n",
      "step 1220 , total_loss: 0.6758, data_loss: 0.6758\n",
      "step 1240 , total_loss: 0.6313, data_loss: 0.6313\n",
      "step 1260 , total_loss: 0.7885, data_loss: 0.7885\n",
      "step 1280 , total_loss: 0.6980, data_loss: 0.6980\n",
      "step 1300 , total_loss: 0.6549, data_loss: 0.6549\n",
      "step 1320 , total_loss: 0.6311, data_loss: 0.6311\n",
      "step 1340 , total_loss: 0.6275, data_loss: 0.6275\n",
      "step 1360 , total_loss: 0.7194, data_loss: 0.7194\n",
      "step 1380 , total_loss: 0.6406, data_loss: 0.6406\n",
      "step 1400 , total_loss: 0.6654, data_loss: 0.6654\n",
      "step 1420 , total_loss: 0.7731, data_loss: 0.7731\n",
      "step 1440 , total_loss: 0.6572, data_loss: 0.6572\n",
      "step 1460 , total_loss: 0.6217, data_loss: 0.6217\n",
      "step 1480 , total_loss: 0.6638, data_loss: 0.6638\n",
      "step 1500 , total_loss: 0.6331, data_loss: 0.6331\n",
      "step 1520 , total_loss: 0.6483, data_loss: 0.6483\n",
      "step 1540 , total_loss: 0.6039, data_loss: 0.6039\n",
      "step 1560 , total_loss: 0.6923, data_loss: 0.6923\n",
      "step 1580 , total_loss: 0.6799, data_loss: 0.6799\n",
      "step 1600 , total_loss: 0.6845, data_loss: 0.6845\n",
      "step 1620 , total_loss: 0.6119, data_loss: 0.6119\n",
      "step 1640 , total_loss: 0.6655, data_loss: 0.6655\n",
      "step 1660 , total_loss: 0.6372, data_loss: 0.6372\n",
      "step 1680 , total_loss: 0.6054, data_loss: 0.6054\n",
      "step 1700 , total_loss: 0.6551, data_loss: 0.6551\n",
      "step 1720 , total_loss: 0.6774, data_loss: 0.6774\n",
      "step 1740 , total_loss: 0.6423, data_loss: 0.6423\n",
      "step 1760 , total_loss: 0.7122, data_loss: 0.7122\n",
      "step 1780 , total_loss: 0.7211, data_loss: 0.7211\n",
      "step 1800 , total_loss: 0.7225, data_loss: 0.7225\n",
      "step 1820 , total_loss: 0.6512, data_loss: 0.6512\n",
      "step 1840 , total_loss: 0.6282, data_loss: 0.6282\n",
      "step 1860 , total_loss: 0.6039, data_loss: 0.6039\n",
      "step 1880 , total_loss: 0.6587, data_loss: 0.6587\n",
      "step 1900 , total_loss: 0.6071, data_loss: 0.6071\n",
      "step 1920 , total_loss: 0.7299, data_loss: 0.7299\n",
      "step 1940 , total_loss: 0.6830, data_loss: 0.6830\n",
      "step 1960 , total_loss: 0.6179, data_loss: 0.6179\n",
      "step 1980 , total_loss: 0.6716, data_loss: 0.6716\n",
      "step 2000 , total_loss: 0.7861, data_loss: 0.7861\n",
      "step 2020 , total_loss: 0.7511, data_loss: 0.7511\n",
      "step 2040 , total_loss: 0.6786, data_loss: 0.6786\n",
      "step 2060 , total_loss: 0.6401, data_loss: 0.6401\n",
      "step 2080 , total_loss: 0.6118, data_loss: 0.6118\n",
      "step 2100 , total_loss: 0.7428, data_loss: 0.7428\n",
      "step 2120 , total_loss: 0.7238, data_loss: 0.7238\n",
      "step 2140 , total_loss: 0.6668, data_loss: 0.6668\n",
      "step 2160 , total_loss: 0.6992, data_loss: 0.6992\n",
      "step 2180 , total_loss: 0.6905, data_loss: 0.6905\n",
      "step 2200 , total_loss: 0.6451, data_loss: 0.6451\n",
      "step 2220 , total_loss: 0.6657, data_loss: 0.6657\n",
      "step 2240 , total_loss: 0.6902, data_loss: 0.6902\n",
      "step 2260 , total_loss: 0.6708, data_loss: 0.6708\n",
      "step 2280 , total_loss: 0.6028, data_loss: 0.6028\n",
      "step 2300 , total_loss: 0.6523, data_loss: 0.6523\n",
      "step 2320 , total_loss: 0.6762, data_loss: 0.6762\n",
      "step 2340 , total_loss: 0.7347, data_loss: 0.7347\n",
      "step 2360 , total_loss: 0.7374, data_loss: 0.7374\n",
      "step 2380 , total_loss: 0.6544, data_loss: 0.6544\n",
      "step 2400 , total_loss: 0.7673, data_loss: 0.7673\n",
      "step 2420 , total_loss: 0.6551, data_loss: 0.6551\n",
      "step 2440 , total_loss: 0.7040, data_loss: 0.7040\n",
      "step 2460 , total_loss: 0.6889, data_loss: 0.6889\n",
      "step 2480 , total_loss: 0.7198, data_loss: 0.7198\n",
      "step 2500 , total_loss: 0.6798, data_loss: 0.6798\n",
      "step 2520 , total_loss: 0.6794, data_loss: 0.6794\n",
      "step 2540 , total_loss: 0.6675, data_loss: 0.6675\n",
      "step 2560 , total_loss: 0.6059, data_loss: 0.6059\n",
      "step 2580 , total_loss: 0.7306, data_loss: 0.7306\n",
      "step 2600 , total_loss: 0.6248, data_loss: 0.6248\n",
      "step 2620 , total_loss: 0.6424, data_loss: 0.6424\n",
      "step 2640 , total_loss: 0.7158, data_loss: 0.7158\n",
      "step 2660 , total_loss: 0.6973, data_loss: 0.6973\n",
      "step 2680 , total_loss: 0.7084, data_loss: 0.7084\n",
      "step 2700 , total_loss: 0.6789, data_loss: 0.6789\n",
      "step 2720 , total_loss: 0.6329, data_loss: 0.6329\n",
      "step 2740 , total_loss: 0.6696, data_loss: 0.6696\n",
      "step 2760 , total_loss: 0.6514, data_loss: 0.6514\n",
      "step 2780 , total_loss: 0.6779, data_loss: 0.6779\n",
      "step 2800 , total_loss: 0.7270, data_loss: 0.7270\n",
      "step 2820 , total_loss: 0.6979, data_loss: 0.6979\n",
      "step 2840 , total_loss: 0.6577, data_loss: 0.6577\n",
      "step 2860 , total_loss: 0.6850, data_loss: 0.6850\n",
      "step 2880 , total_loss: 0.7004, data_loss: 0.7004\n",
      "step 2900 , total_loss: 0.6844, data_loss: 0.6844\n",
      "step 2920 , total_loss: 0.6770, data_loss: 0.6770\n",
      "step 2940 , total_loss: 0.5873, data_loss: 0.5873\n",
      "step 2960 , total_loss: 0.7007, data_loss: 0.7007\n",
      "step 2980 , total_loss: 0.6639, data_loss: 0.6639\n",
      "step 3000 , total_loss: 0.6532, data_loss: 0.6532\n",
      "step 3020 , total_loss: 0.6888, data_loss: 0.6888\n",
      "step 3040 , total_loss: 0.6585, data_loss: 0.6585\n",
      "step 3060 , total_loss: 0.7121, data_loss: 0.7121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3080 , total_loss: 0.6374, data_loss: 0.6374\n",
      "step 3100 , total_loss: 0.6679, data_loss: 0.6679\n",
      "step 3120 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 3140 , total_loss: 0.6626, data_loss: 0.6626\n",
      "step 3160 , total_loss: 0.6484, data_loss: 0.6484\n",
      "step 3180 , total_loss: 0.6921, data_loss: 0.6921\n",
      "step 3200 , total_loss: 0.7183, data_loss: 0.7183\n",
      "step 3220 , total_loss: 0.6853, data_loss: 0.6853\n",
      "step 3240 , total_loss: 0.6661, data_loss: 0.6661\n",
      "step 3260 , total_loss: 0.6780, data_loss: 0.6780\n",
      "step 3280 , total_loss: 0.7619, data_loss: 0.7619\n",
      "step 3300 , total_loss: 0.6539, data_loss: 0.6539\n",
      "eval valid at epoch 7: auc:0.876,logloss:0.7891,mean_mrr:0.8182,ndcg@2:0.8008,ndcg@4:0.8563,ndcg@6:0.8641,group_auc:0.8691\n",
      "step 20 , total_loss: 0.6765, data_loss: 0.6765\n",
      "step 40 , total_loss: 0.5686, data_loss: 0.5686\n",
      "step 60 , total_loss: 0.6469, data_loss: 0.6469\n",
      "step 80 , total_loss: 0.6122, data_loss: 0.6122\n",
      "step 100 , total_loss: 0.6012, data_loss: 0.6012\n",
      "step 120 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 140 , total_loss: 0.6859, data_loss: 0.6859\n",
      "step 160 , total_loss: 0.6502, data_loss: 0.6502\n",
      "step 180 , total_loss: 0.5868, data_loss: 0.5868\n",
      "step 200 , total_loss: 0.6497, data_loss: 0.6497\n",
      "step 220 , total_loss: 0.6563, data_loss: 0.6563\n",
      "step 240 , total_loss: 0.6340, data_loss: 0.6340\n",
      "step 260 , total_loss: 0.6407, data_loss: 0.6407\n",
      "step 280 , total_loss: 0.5896, data_loss: 0.5896\n",
      "step 300 , total_loss: 0.5535, data_loss: 0.5535\n",
      "step 320 , total_loss: 0.6990, data_loss: 0.6990\n",
      "step 340 , total_loss: 0.6217, data_loss: 0.6217\n",
      "step 360 , total_loss: 0.5467, data_loss: 0.5467\n",
      "step 380 , total_loss: 0.6791, data_loss: 0.6791\n",
      "step 400 , total_loss: 0.6108, data_loss: 0.6108\n",
      "step 420 , total_loss: 0.6062, data_loss: 0.6062\n",
      "step 440 , total_loss: 0.6905, data_loss: 0.6905\n",
      "step 460 , total_loss: 0.5684, data_loss: 0.5684\n",
      "step 480 , total_loss: 0.6150, data_loss: 0.6150\n",
      "step 500 , total_loss: 0.6264, data_loss: 0.6264\n",
      "step 520 , total_loss: 0.5518, data_loss: 0.5518\n",
      "step 540 , total_loss: 0.5927, data_loss: 0.5927\n",
      "step 560 , total_loss: 0.5998, data_loss: 0.5998\n",
      "step 580 , total_loss: 0.5829, data_loss: 0.5829\n",
      "step 600 , total_loss: 0.6230, data_loss: 0.6230\n",
      "step 620 , total_loss: 0.5581, data_loss: 0.5581\n",
      "step 640 , total_loss: 0.6511, data_loss: 0.6511\n",
      "step 660 , total_loss: 0.6177, data_loss: 0.6177\n",
      "step 680 , total_loss: 0.7051, data_loss: 0.7051\n",
      "step 700 , total_loss: 0.6365, data_loss: 0.6365\n",
      "step 720 , total_loss: 0.6308, data_loss: 0.6308\n",
      "step 740 , total_loss: 0.6114, data_loss: 0.6114\n",
      "step 760 , total_loss: 0.6258, data_loss: 0.6258\n",
      "step 780 , total_loss: 0.6547, data_loss: 0.6547\n",
      "step 800 , total_loss: 0.5857, data_loss: 0.5857\n",
      "step 820 , total_loss: 0.6227, data_loss: 0.6227\n",
      "step 840 , total_loss: 0.6919, data_loss: 0.6919\n",
      "step 860 , total_loss: 0.6237, data_loss: 0.6237\n",
      "step 880 , total_loss: 0.6092, data_loss: 0.6092\n",
      "step 900 , total_loss: 0.6856, data_loss: 0.6856\n",
      "step 920 , total_loss: 0.6491, data_loss: 0.6491\n",
      "step 940 , total_loss: 0.6517, data_loss: 0.6517\n",
      "step 960 , total_loss: 0.6011, data_loss: 0.6011\n",
      "step 980 , total_loss: 0.5938, data_loss: 0.5938\n",
      "step 1000 , total_loss: 0.5864, data_loss: 0.5864\n",
      "step 1020 , total_loss: 0.7048, data_loss: 0.7048\n",
      "step 1040 , total_loss: 0.5552, data_loss: 0.5552\n",
      "step 1060 , total_loss: 0.6236, data_loss: 0.6236\n",
      "step 1080 , total_loss: 0.7138, data_loss: 0.7138\n",
      "step 1100 , total_loss: 0.5452, data_loss: 0.5452\n",
      "step 1120 , total_loss: 0.6926, data_loss: 0.6926\n",
      "step 1140 , total_loss: 0.6671, data_loss: 0.6671\n",
      "step 1160 , total_loss: 0.5894, data_loss: 0.5894\n",
      "step 1180 , total_loss: 0.6485, data_loss: 0.6485\n",
      "step 1200 , total_loss: 0.6091, data_loss: 0.6091\n",
      "step 1220 , total_loss: 0.6401, data_loss: 0.6401\n",
      "step 1240 , total_loss: 0.6623, data_loss: 0.6623\n",
      "step 1260 , total_loss: 0.5750, data_loss: 0.5750\n",
      "step 1280 , total_loss: 0.6163, data_loss: 0.6163\n",
      "step 1300 , total_loss: 0.6778, data_loss: 0.6778\n",
      "step 1320 , total_loss: 0.6411, data_loss: 0.6411\n",
      "step 1340 , total_loss: 0.6221, data_loss: 0.6221\n",
      "step 1360 , total_loss: 0.6368, data_loss: 0.6368\n",
      "step 1380 , total_loss: 0.6783, data_loss: 0.6783\n",
      "step 1400 , total_loss: 0.6186, data_loss: 0.6186\n",
      "step 1420 , total_loss: 0.6716, data_loss: 0.6716\n",
      "step 1440 , total_loss: 0.6290, data_loss: 0.6290\n",
      "step 1460 , total_loss: 0.5657, data_loss: 0.5657\n",
      "step 1480 , total_loss: 0.6310, data_loss: 0.6310\n",
      "step 1500 , total_loss: 0.6760, data_loss: 0.6760\n",
      "step 1520 , total_loss: 0.6516, data_loss: 0.6516\n",
      "step 1540 , total_loss: 0.6556, data_loss: 0.6556\n",
      "step 1560 , total_loss: 0.6751, data_loss: 0.6751\n",
      "step 1580 , total_loss: 0.5900, data_loss: 0.5900\n",
      "step 1600 , total_loss: 0.7063, data_loss: 0.7063\n",
      "step 1620 , total_loss: 0.7364, data_loss: 0.7364\n",
      "step 1640 , total_loss: 0.6852, data_loss: 0.6852\n",
      "step 1660 , total_loss: 0.5784, data_loss: 0.5784\n",
      "step 1680 , total_loss: 0.6355, data_loss: 0.6355\n",
      "step 1700 , total_loss: 0.5804, data_loss: 0.5804\n",
      "step 1720 , total_loss: 0.6399, data_loss: 0.6399\n",
      "step 1740 , total_loss: 0.6354, data_loss: 0.6354\n",
      "step 1760 , total_loss: 0.7026, data_loss: 0.7026\n",
      "step 1780 , total_loss: 0.6261, data_loss: 0.6261\n",
      "step 1800 , total_loss: 0.6910, data_loss: 0.6910\n",
      "step 1820 , total_loss: 0.5843, data_loss: 0.5843\n",
      "step 1840 , total_loss: 0.7114, data_loss: 0.7114\n",
      "step 1860 , total_loss: 0.7245, data_loss: 0.7245\n",
      "step 1880 , total_loss: 0.6294, data_loss: 0.6294\n",
      "step 1900 , total_loss: 0.6133, data_loss: 0.6133\n",
      "step 1920 , total_loss: 0.6570, data_loss: 0.6570\n",
      "step 1940 , total_loss: 0.6506, data_loss: 0.6506\n",
      "step 1960 , total_loss: 0.7200, data_loss: 0.7200\n",
      "step 1980 , total_loss: 0.5736, data_loss: 0.5736\n",
      "step 2000 , total_loss: 0.5803, data_loss: 0.5803\n",
      "step 2020 , total_loss: 0.6890, data_loss: 0.6890\n",
      "step 2040 , total_loss: 0.6028, data_loss: 0.6028\n",
      "step 2060 , total_loss: 0.6017, data_loss: 0.6017\n",
      "step 2080 , total_loss: 0.6524, data_loss: 0.6524\n",
      "step 2100 , total_loss: 0.6287, data_loss: 0.6287\n",
      "step 2120 , total_loss: 0.6016, data_loss: 0.6016\n",
      "step 2140 , total_loss: 0.6568, data_loss: 0.6568\n",
      "step 2160 , total_loss: 0.6864, data_loss: 0.6864\n",
      "step 2180 , total_loss: 0.6977, data_loss: 0.6977\n",
      "step 2200 , total_loss: 0.6450, data_loss: 0.6450\n",
      "step 2220 , total_loss: 0.6279, data_loss: 0.6279\n",
      "step 2240 , total_loss: 0.6709, data_loss: 0.6709\n",
      "step 2260 , total_loss: 0.6444, data_loss: 0.6444\n",
      "step 2280 , total_loss: 0.6563, data_loss: 0.6563\n",
      "step 2300 , total_loss: 0.6517, data_loss: 0.6517\n",
      "step 2320 , total_loss: 0.6584, data_loss: 0.6584\n",
      "step 2340 , total_loss: 0.6442, data_loss: 0.6442\n",
      "step 2360 , total_loss: 0.6507, data_loss: 0.6507\n",
      "step 2380 , total_loss: 0.6657, data_loss: 0.6657\n",
      "step 2400 , total_loss: 0.5987, data_loss: 0.5987\n",
      "step 2420 , total_loss: 0.6878, data_loss: 0.6878\n",
      "step 2440 , total_loss: 0.6254, data_loss: 0.6254\n",
      "step 2460 , total_loss: 0.6715, data_loss: 0.6715\n",
      "step 2480 , total_loss: 0.6185, data_loss: 0.6185\n",
      "step 2500 , total_loss: 0.6413, data_loss: 0.6413\n",
      "step 2520 , total_loss: 0.6786, data_loss: 0.6786\n",
      "step 2540 , total_loss: 0.7345, data_loss: 0.7345\n",
      "step 2560 , total_loss: 0.6758, data_loss: 0.6758\n",
      "step 2580 , total_loss: 0.6984, data_loss: 0.6984\n",
      "step 2600 , total_loss: 0.6245, data_loss: 0.6245\n",
      "step 2620 , total_loss: 0.6877, data_loss: 0.6877\n",
      "step 2640 , total_loss: 0.7191, data_loss: 0.7191\n",
      "step 2660 , total_loss: 0.6202, data_loss: 0.6202\n",
      "step 2680 , total_loss: 0.6656, data_loss: 0.6656\n",
      "step 2700 , total_loss: 0.6519, data_loss: 0.6519\n",
      "step 2720 , total_loss: 0.6802, data_loss: 0.6802\n",
      "step 2740 , total_loss: 0.6865, data_loss: 0.6865\n",
      "step 2760 , total_loss: 0.6825, data_loss: 0.6825\n",
      "step 2780 , total_loss: 0.6568, data_loss: 0.6568\n",
      "step 2800 , total_loss: 0.6174, data_loss: 0.6174\n",
      "step 2820 , total_loss: 0.6314, data_loss: 0.6314\n",
      "step 2840 , total_loss: 0.6454, data_loss: 0.6454\n",
      "step 2860 , total_loss: 0.6557, data_loss: 0.6557\n",
      "step 2880 , total_loss: 0.5663, data_loss: 0.5663\n",
      "step 2900 , total_loss: 0.6972, data_loss: 0.6972\n",
      "step 2920 , total_loss: 0.6503, data_loss: 0.6503\n",
      "step 2940 , total_loss: 0.6205, data_loss: 0.6205\n",
      "step 2960 , total_loss: 0.7032, data_loss: 0.7032\n",
      "step 2980 , total_loss: 0.5979, data_loss: 0.5979\n",
      "step 3000 , total_loss: 0.6464, data_loss: 0.6464\n",
      "step 3020 , total_loss: 0.7070, data_loss: 0.7070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3040 , total_loss: 0.6103, data_loss: 0.6103\n",
      "step 3060 , total_loss: 0.6177, data_loss: 0.6177\n",
      "step 3080 , total_loss: 0.6606, data_loss: 0.6606\n",
      "step 3100 , total_loss: 0.6705, data_loss: 0.6705\n",
      "step 3120 , total_loss: 0.6434, data_loss: 0.6434\n",
      "step 3140 , total_loss: 0.7198, data_loss: 0.7198\n",
      "step 3160 , total_loss: 0.6717, data_loss: 0.6717\n",
      "step 3180 , total_loss: 0.6523, data_loss: 0.6523\n",
      "step 3200 , total_loss: 0.6631, data_loss: 0.6631\n",
      "step 3220 , total_loss: 0.6549, data_loss: 0.6549\n",
      "step 3240 , total_loss: 0.6781, data_loss: 0.6781\n",
      "step 3260 , total_loss: 0.6760, data_loss: 0.6760\n",
      "step 3280 , total_loss: 0.7392, data_loss: 0.7392\n",
      "step 3300 , total_loss: 0.6817, data_loss: 0.6817\n",
      "eval valid at epoch 8: auc:0.8765,logloss:0.8245,mean_mrr:0.8197,ndcg@2:0.8027,ndcg@4:0.8574,ndcg@6:0.8652,group_auc:0.8702\n",
      "step 20 , total_loss: 0.5704, data_loss: 0.5704\n",
      "step 40 , total_loss: 0.5186, data_loss: 0.5186\n",
      "step 60 , total_loss: 0.5409, data_loss: 0.5409\n",
      "step 80 , total_loss: 0.5708, data_loss: 0.5708\n",
      "step 100 , total_loss: 0.5633, data_loss: 0.5633\n",
      "step 120 , total_loss: 0.5378, data_loss: 0.5378\n",
      "step 140 , total_loss: 0.5728, data_loss: 0.5728\n",
      "step 160 , total_loss: 0.6063, data_loss: 0.6063\n",
      "step 180 , total_loss: 0.5776, data_loss: 0.5776\n",
      "step 200 , total_loss: 0.6037, data_loss: 0.6037\n",
      "step 220 , total_loss: 0.6038, data_loss: 0.6038\n",
      "step 240 , total_loss: 0.6323, data_loss: 0.6323\n",
      "step 260 , total_loss: 0.6291, data_loss: 0.6291\n",
      "step 280 , total_loss: 0.5874, data_loss: 0.5874\n",
      "step 300 , total_loss: 0.5740, data_loss: 0.5740\n",
      "step 320 , total_loss: 0.6438, data_loss: 0.6438\n",
      "step 340 , total_loss: 0.5977, data_loss: 0.5977\n",
      "step 360 , total_loss: 0.5653, data_loss: 0.5653\n",
      "step 380 , total_loss: 0.6881, data_loss: 0.6881\n",
      "step 400 , total_loss: 0.5749, data_loss: 0.5749\n",
      "step 420 , total_loss: 0.6254, data_loss: 0.6254\n",
      "step 440 , total_loss: 0.6210, data_loss: 0.6210\n",
      "step 460 , total_loss: 0.6522, data_loss: 0.6522\n",
      "step 480 , total_loss: 0.6157, data_loss: 0.6157\n",
      "step 500 , total_loss: 0.6157, data_loss: 0.6157\n",
      "step 520 , total_loss: 0.6019, data_loss: 0.6019\n",
      "step 540 , total_loss: 0.6462, data_loss: 0.6462\n",
      "step 560 , total_loss: 0.6389, data_loss: 0.6389\n",
      "step 580 , total_loss: 0.5265, data_loss: 0.5265\n",
      "step 600 , total_loss: 0.5961, data_loss: 0.5961\n",
      "step 620 , total_loss: 0.5981, data_loss: 0.5981\n",
      "step 640 , total_loss: 0.6016, data_loss: 0.6016\n",
      "step 660 , total_loss: 0.6211, data_loss: 0.6211\n",
      "step 680 , total_loss: 0.6727, data_loss: 0.6727\n",
      "step 700 , total_loss: 0.6346, data_loss: 0.6346\n",
      "step 720 , total_loss: 0.6307, data_loss: 0.6307\n",
      "step 740 , total_loss: 0.5990, data_loss: 0.5990\n",
      "step 760 , total_loss: 0.6705, data_loss: 0.6705\n",
      "step 780 , total_loss: 0.5746, data_loss: 0.5746\n",
      "step 800 , total_loss: 0.5705, data_loss: 0.5705\n",
      "step 820 , total_loss: 0.6345, data_loss: 0.6345\n",
      "step 840 , total_loss: 0.6554, data_loss: 0.6554\n",
      "step 860 , total_loss: 0.6161, data_loss: 0.6161\n",
      "step 880 , total_loss: 0.6549, data_loss: 0.6549\n",
      "step 900 , total_loss: 0.6430, data_loss: 0.6430\n",
      "step 920 , total_loss: 0.6855, data_loss: 0.6855\n",
      "step 940 , total_loss: 0.6258, data_loss: 0.6258\n",
      "step 960 , total_loss: 0.6295, data_loss: 0.6295\n",
      "step 980 , total_loss: 0.5663, data_loss: 0.5663\n",
      "step 1000 , total_loss: 0.6709, data_loss: 0.6709\n",
      "step 1020 , total_loss: 0.5940, data_loss: 0.5940\n",
      "step 1040 , total_loss: 0.5581, data_loss: 0.5581\n",
      "step 1060 , total_loss: 0.6183, data_loss: 0.6183\n",
      "step 1080 , total_loss: 0.5855, data_loss: 0.5855\n",
      "step 1100 , total_loss: 0.6865, data_loss: 0.6865\n",
      "step 1120 , total_loss: 0.6009, data_loss: 0.6009\n",
      "step 1140 , total_loss: 0.6242, data_loss: 0.6242\n",
      "step 1160 , total_loss: 0.6187, data_loss: 0.6187\n",
      "step 1180 , total_loss: 0.5230, data_loss: 0.5230\n",
      "step 1200 , total_loss: 0.5666, data_loss: 0.5666\n",
      "step 1220 , total_loss: 0.6141, data_loss: 0.6141\n",
      "step 1240 , total_loss: 0.6135, data_loss: 0.6135\n",
      "step 1260 , total_loss: 0.6081, data_loss: 0.6081\n",
      "step 1280 , total_loss: 0.6399, data_loss: 0.6399\n",
      "step 1300 , total_loss: 0.6242, data_loss: 0.6242\n",
      "step 1320 , total_loss: 0.6147, data_loss: 0.6147\n",
      "step 1340 , total_loss: 0.5700, data_loss: 0.5700\n",
      "step 1360 , total_loss: 0.5631, data_loss: 0.5631\n",
      "step 1380 , total_loss: 0.6197, data_loss: 0.6197\n",
      "step 1400 , total_loss: 0.6643, data_loss: 0.6643\n",
      "step 1420 , total_loss: 0.6634, data_loss: 0.6634\n",
      "step 1440 , total_loss: 0.5899, data_loss: 0.5899\n",
      "step 1460 , total_loss: 0.6370, data_loss: 0.6370\n",
      "step 1480 , total_loss: 0.7262, data_loss: 0.7262\n",
      "step 1500 , total_loss: 0.6366, data_loss: 0.6366\n",
      "step 1520 , total_loss: 0.5887, data_loss: 0.5887\n",
      "step 1540 , total_loss: 0.6271, data_loss: 0.6271\n",
      "step 1560 , total_loss: 0.5982, data_loss: 0.5982\n",
      "step 1580 , total_loss: 0.6855, data_loss: 0.6855\n",
      "step 1600 , total_loss: 0.5756, data_loss: 0.5756\n",
      "step 1620 , total_loss: 0.6899, data_loss: 0.6899\n",
      "step 1640 , total_loss: 0.5495, data_loss: 0.5495\n",
      "step 1660 , total_loss: 0.6827, data_loss: 0.6827\n",
      "step 1680 , total_loss: 0.6428, data_loss: 0.6428\n",
      "step 1700 , total_loss: 0.5956, data_loss: 0.5956\n",
      "step 1720 , total_loss: 0.6744, data_loss: 0.6744\n",
      "step 1740 , total_loss: 0.6376, data_loss: 0.6376\n",
      "step 1760 , total_loss: 0.6663, data_loss: 0.6663\n",
      "step 1780 , total_loss: 0.6352, data_loss: 0.6352\n",
      "step 1800 , total_loss: 0.6022, data_loss: 0.6022\n",
      "step 1820 , total_loss: 0.6195, data_loss: 0.6195\n",
      "step 1840 , total_loss: 0.6274, data_loss: 0.6274\n",
      "step 1860 , total_loss: 0.5354, data_loss: 0.5354\n",
      "step 1880 , total_loss: 0.6104, data_loss: 0.6104\n",
      "step 1900 , total_loss: 0.6440, data_loss: 0.6440\n",
      "step 1920 , total_loss: 0.6592, data_loss: 0.6592\n",
      "step 1940 , total_loss: 0.5928, data_loss: 0.5928\n",
      "step 1960 , total_loss: 0.5992, data_loss: 0.5992\n",
      "step 1980 , total_loss: 0.7499, data_loss: 0.7499\n",
      "step 2000 , total_loss: 0.6175, data_loss: 0.6175\n",
      "step 2020 , total_loss: 0.6136, data_loss: 0.6136\n",
      "step 2040 , total_loss: 0.5549, data_loss: 0.5549\n",
      "step 2060 , total_loss: 0.6659, data_loss: 0.6659\n",
      "step 2080 , total_loss: 0.6538, data_loss: 0.6538\n",
      "step 2100 , total_loss: 0.6240, data_loss: 0.6240\n",
      "step 2120 , total_loss: 0.6709, data_loss: 0.6709\n",
      "step 2140 , total_loss: 0.6894, data_loss: 0.6894\n",
      "step 2160 , total_loss: 0.6425, data_loss: 0.6425\n",
      "step 2180 , total_loss: 0.6141, data_loss: 0.6141\n",
      "step 2200 , total_loss: 0.7428, data_loss: 0.7428\n",
      "step 2220 , total_loss: 0.6092, data_loss: 0.6092\n",
      "step 2240 , total_loss: 0.6608, data_loss: 0.6608\n",
      "step 2260 , total_loss: 0.6785, data_loss: 0.6785\n",
      "step 2280 , total_loss: 0.5833, data_loss: 0.5833\n",
      "step 2300 , total_loss: 0.7242, data_loss: 0.7242\n",
      "step 2320 , total_loss: 0.5805, data_loss: 0.5805\n",
      "step 2340 , total_loss: 0.5928, data_loss: 0.5928\n",
      "step 2360 , total_loss: 0.6410, data_loss: 0.6410\n",
      "step 2380 , total_loss: 0.5516, data_loss: 0.5516\n",
      "step 2400 , total_loss: 0.5975, data_loss: 0.5975\n",
      "step 2420 , total_loss: 0.6508, data_loss: 0.6508\n",
      "step 2440 , total_loss: 0.6460, data_loss: 0.6460\n",
      "step 2460 , total_loss: 0.6194, data_loss: 0.6194\n",
      "step 2480 , total_loss: 0.6134, data_loss: 0.6134\n",
      "step 2500 , total_loss: 0.5793, data_loss: 0.5793\n",
      "step 2520 , total_loss: 0.6603, data_loss: 0.6603\n",
      "step 2540 , total_loss: 0.6534, data_loss: 0.6534\n",
      "step 2560 , total_loss: 0.5626, data_loss: 0.5626\n",
      "step 2580 , total_loss: 0.6467, data_loss: 0.6467\n",
      "step 2600 , total_loss: 0.6456, data_loss: 0.6456\n",
      "step 2620 , total_loss: 0.5818, data_loss: 0.5818\n",
      "step 2640 , total_loss: 0.5823, data_loss: 0.5823\n",
      "step 2660 , total_loss: 0.6013, data_loss: 0.6013\n",
      "step 2680 , total_loss: 0.6004, data_loss: 0.6004\n",
      "step 2700 , total_loss: 0.5773, data_loss: 0.5773\n",
      "step 2720 , total_loss: 0.6036, data_loss: 0.6036\n",
      "step 2740 , total_loss: 0.6193, data_loss: 0.6193\n",
      "step 2760 , total_loss: 0.6261, data_loss: 0.6261\n",
      "step 2780 , total_loss: 0.5866, data_loss: 0.5866\n",
      "step 2800 , total_loss: 0.6286, data_loss: 0.6286\n",
      "step 2820 , total_loss: 0.6470, data_loss: 0.6470\n",
      "step 2840 , total_loss: 0.6014, data_loss: 0.6014\n",
      "step 2860 , total_loss: 0.5909, data_loss: 0.5909\n",
      "step 2880 , total_loss: 0.7114, data_loss: 0.7114\n",
      "step 2900 , total_loss: 0.6409, data_loss: 0.6409\n",
      "step 2920 , total_loss: 0.6113, data_loss: 0.6113\n",
      "step 2940 , total_loss: 0.6996, data_loss: 0.6996\n",
      "step 2960 , total_loss: 0.7267, data_loss: 0.7267\n",
      "step 2980 , total_loss: 0.6352, data_loss: 0.6352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000 , total_loss: 0.6278, data_loss: 0.6278\n",
      "step 3020 , total_loss: 0.5987, data_loss: 0.5987\n",
      "step 3040 , total_loss: 0.6115, data_loss: 0.6115\n",
      "step 3060 , total_loss: 0.6976, data_loss: 0.6976\n",
      "step 3080 , total_loss: 0.6739, data_loss: 0.6739\n",
      "step 3100 , total_loss: 0.6482, data_loss: 0.6482\n",
      "step 3120 , total_loss: 0.5961, data_loss: 0.5961\n",
      "step 3140 , total_loss: 0.5792, data_loss: 0.5792\n",
      "step 3160 , total_loss: 0.6207, data_loss: 0.6207\n",
      "step 3180 , total_loss: 0.6952, data_loss: 0.6952\n",
      "step 3200 , total_loss: 0.6202, data_loss: 0.6202\n",
      "step 3220 , total_loss: 0.6633, data_loss: 0.6633\n",
      "step 3240 , total_loss: 0.6344, data_loss: 0.6344\n",
      "step 3260 , total_loss: 0.6568, data_loss: 0.6568\n",
      "step 3280 , total_loss: 0.6090, data_loss: 0.6090\n",
      "step 3300 , total_loss: 0.5851, data_loss: 0.5851\n",
      "eval valid at epoch 9: auc:0.8779,logloss:0.8184,mean_mrr:0.8211,ndcg@2:0.8036,ndcg@4:0.8585,ndcg@6:0.8663,group_auc:0.8709\n",
      "step 20 , total_loss: 0.5905, data_loss: 0.5905\n",
      "step 40 , total_loss: 0.5659, data_loss: 0.5659\n",
      "step 60 , total_loss: 0.5347, data_loss: 0.5347\n",
      "step 80 , total_loss: 0.5838, data_loss: 0.5838\n",
      "step 100 , total_loss: 0.5367, data_loss: 0.5367\n",
      "step 120 , total_loss: 0.5990, data_loss: 0.5990\n",
      "step 140 , total_loss: 0.5567, data_loss: 0.5567\n",
      "step 160 , total_loss: 0.5273, data_loss: 0.5273\n",
      "step 180 , total_loss: 0.5809, data_loss: 0.5809\n",
      "step 200 , total_loss: 0.5993, data_loss: 0.5993\n",
      "step 220 , total_loss: 0.5779, data_loss: 0.5779\n",
      "step 240 , total_loss: 0.5301, data_loss: 0.5301\n",
      "step 260 , total_loss: 0.6089, data_loss: 0.6089\n",
      "step 280 , total_loss: 0.5155, data_loss: 0.5155\n",
      "step 300 , total_loss: 0.6324, data_loss: 0.6324\n",
      "step 320 , total_loss: 0.5422, data_loss: 0.5422\n",
      "step 340 , total_loss: 0.5420, data_loss: 0.5420\n",
      "step 360 , total_loss: 0.5855, data_loss: 0.5855\n",
      "step 380 , total_loss: 0.6188, data_loss: 0.6188\n",
      "step 400 , total_loss: 0.6107, data_loss: 0.6107\n",
      "step 420 , total_loss: 0.5473, data_loss: 0.5473\n",
      "step 440 , total_loss: 0.5287, data_loss: 0.5287\n",
      "step 460 , total_loss: 0.6046, data_loss: 0.6046\n",
      "step 480 , total_loss: 0.5402, data_loss: 0.5402\n",
      "step 500 , total_loss: 0.5749, data_loss: 0.5749\n",
      "step 520 , total_loss: 0.5509, data_loss: 0.5509\n",
      "step 540 , total_loss: 0.6128, data_loss: 0.6128\n",
      "step 560 , total_loss: 0.6145, data_loss: 0.6145\n",
      "step 580 , total_loss: 0.6148, data_loss: 0.6148\n",
      "step 600 , total_loss: 0.5699, data_loss: 0.5699\n",
      "step 620 , total_loss: 0.5875, data_loss: 0.5875\n",
      "step 640 , total_loss: 0.6148, data_loss: 0.6148\n",
      "step 660 , total_loss: 0.5727, data_loss: 0.5727\n",
      "step 680 , total_loss: 0.6194, data_loss: 0.6194\n",
      "step 700 , total_loss: 0.6739, data_loss: 0.6739\n",
      "step 720 , total_loss: 0.5268, data_loss: 0.5268\n",
      "step 740 , total_loss: 0.5921, data_loss: 0.5921\n",
      "step 760 , total_loss: 0.5053, data_loss: 0.5053\n",
      "step 780 , total_loss: 0.6149, data_loss: 0.6149\n",
      "step 800 , total_loss: 0.5343, data_loss: 0.5343\n",
      "step 820 , total_loss: 0.5670, data_loss: 0.5670\n",
      "step 840 , total_loss: 0.5852, data_loss: 0.5852\n",
      "step 860 , total_loss: 0.5852, data_loss: 0.5852\n",
      "step 880 , total_loss: 0.5854, data_loss: 0.5854\n",
      "step 900 , total_loss: 0.5389, data_loss: 0.5389\n",
      "step 920 , total_loss: 0.5736, data_loss: 0.5736\n",
      "step 940 , total_loss: 0.6750, data_loss: 0.6750\n",
      "step 960 , total_loss: 0.6661, data_loss: 0.6661\n",
      "step 980 , total_loss: 0.5724, data_loss: 0.5724\n",
      "step 1000 , total_loss: 0.6491, data_loss: 0.6491\n",
      "step 1020 , total_loss: 0.5937, data_loss: 0.5937\n",
      "step 1040 , total_loss: 0.5682, data_loss: 0.5682\n",
      "step 1060 , total_loss: 0.5706, data_loss: 0.5706\n",
      "step 1080 , total_loss: 0.6438, data_loss: 0.6438\n",
      "step 1100 , total_loss: 0.5895, data_loss: 0.5895\n",
      "step 1120 , total_loss: 0.6055, data_loss: 0.6055\n",
      "step 1140 , total_loss: 0.5882, data_loss: 0.5882\n",
      "step 1160 , total_loss: 0.6136, data_loss: 0.6136\n",
      "step 1180 , total_loss: 0.6650, data_loss: 0.6650\n",
      "step 1200 , total_loss: 0.6051, data_loss: 0.6051\n",
      "step 1220 , total_loss: 0.6753, data_loss: 0.6753\n",
      "step 1240 , total_loss: 0.6087, data_loss: 0.6087\n",
      "step 1260 , total_loss: 0.6603, data_loss: 0.6603\n",
      "step 1280 , total_loss: 0.6171, data_loss: 0.6171\n",
      "step 1300 , total_loss: 0.7254, data_loss: 0.7254\n",
      "step 1320 , total_loss: 0.6067, data_loss: 0.6067\n",
      "step 1340 , total_loss: 0.6055, data_loss: 0.6055\n",
      "step 1360 , total_loss: 0.5756, data_loss: 0.5756\n",
      "step 1380 , total_loss: 0.6584, data_loss: 0.6584\n",
      "step 1400 , total_loss: 0.6622, data_loss: 0.6622\n",
      "step 1420 , total_loss: 0.6482, data_loss: 0.6482\n",
      "step 1440 , total_loss: 0.5577, data_loss: 0.5577\n",
      "step 1460 , total_loss: 0.5900, data_loss: 0.5900\n",
      "step 1480 , total_loss: 0.6836, data_loss: 0.6836\n",
      "step 1500 , total_loss: 0.5850, data_loss: 0.5850\n",
      "step 1520 , total_loss: 0.6050, data_loss: 0.6050\n",
      "step 1540 , total_loss: 0.5952, data_loss: 0.5952\n",
      "step 1560 , total_loss: 0.6772, data_loss: 0.6772\n",
      "step 1580 , total_loss: 0.5610, data_loss: 0.5610\n",
      "step 1600 , total_loss: 0.6058, data_loss: 0.6058\n",
      "step 1620 , total_loss: 0.6920, data_loss: 0.6920\n",
      "step 1640 , total_loss: 0.6156, data_loss: 0.6156\n",
      "step 1660 , total_loss: 0.5940, data_loss: 0.5940\n",
      "step 1680 , total_loss: 0.5341, data_loss: 0.5341\n",
      "step 1700 , total_loss: 0.5464, data_loss: 0.5464\n",
      "step 1720 , total_loss: 0.6095, data_loss: 0.6095\n",
      "step 1740 , total_loss: 0.5351, data_loss: 0.5351\n",
      "step 1760 , total_loss: 0.5954, data_loss: 0.5954\n",
      "step 1780 , total_loss: 0.5414, data_loss: 0.5414\n",
      "step 1800 , total_loss: 0.6200, data_loss: 0.6200\n",
      "step 1820 , total_loss: 0.6475, data_loss: 0.6475\n",
      "step 1840 , total_loss: 0.5901, data_loss: 0.5901\n",
      "step 1860 , total_loss: 0.6164, data_loss: 0.6164\n",
      "step 1880 , total_loss: 0.7010, data_loss: 0.7010\n",
      "step 1900 , total_loss: 0.5658, data_loss: 0.5658\n",
      "step 1920 , total_loss: 0.6441, data_loss: 0.6441\n",
      "step 1940 , total_loss: 0.6169, data_loss: 0.6169\n",
      "step 1960 , total_loss: 0.5816, data_loss: 0.5816\n",
      "step 1980 , total_loss: 0.5999, data_loss: 0.5999\n",
      "step 2000 , total_loss: 0.6036, data_loss: 0.6036\n",
      "step 2020 , total_loss: 0.6151, data_loss: 0.6151\n",
      "step 2040 , total_loss: 0.6209, data_loss: 0.6209\n",
      "step 2060 , total_loss: 0.5419, data_loss: 0.5419\n",
      "step 2080 , total_loss: 0.6464, data_loss: 0.6464\n",
      "step 2100 , total_loss: 0.6002, data_loss: 0.6002\n",
      "step 2120 , total_loss: 0.5604, data_loss: 0.5604\n",
      "step 2140 , total_loss: 0.6547, data_loss: 0.6547\n",
      "step 2160 , total_loss: 0.5808, data_loss: 0.5808\n",
      "step 2180 , total_loss: 0.6622, data_loss: 0.6622\n",
      "step 2200 , total_loss: 0.6490, data_loss: 0.6490\n",
      "step 2220 , total_loss: 0.6290, data_loss: 0.6290\n",
      "step 2240 , total_loss: 0.5264, data_loss: 0.5264\n",
      "step 2260 , total_loss: 0.6824, data_loss: 0.6824\n",
      "step 2280 , total_loss: 0.6698, data_loss: 0.6698\n",
      "step 2300 , total_loss: 0.6197, data_loss: 0.6197\n",
      "step 2320 , total_loss: 0.6186, data_loss: 0.6186\n",
      "step 2340 , total_loss: 0.5824, data_loss: 0.5824\n",
      "step 2360 , total_loss: 0.6942, data_loss: 0.6942\n",
      "step 2380 , total_loss: 0.5826, data_loss: 0.5826\n",
      "step 2400 , total_loss: 0.5727, data_loss: 0.5727\n",
      "step 2420 , total_loss: 0.6711, data_loss: 0.6711\n",
      "step 2440 , total_loss: 0.6592, data_loss: 0.6592\n",
      "step 2460 , total_loss: 0.5051, data_loss: 0.5051\n",
      "step 2480 , total_loss: 0.6097, data_loss: 0.6097\n",
      "step 2500 , total_loss: 0.6519, data_loss: 0.6519\n",
      "step 2520 , total_loss: 0.6646, data_loss: 0.6646\n",
      "step 2540 , total_loss: 0.6669, data_loss: 0.6669\n",
      "step 2560 , total_loss: 0.6600, data_loss: 0.6600\n",
      "step 2580 , total_loss: 0.6694, data_loss: 0.6694\n",
      "step 2600 , total_loss: 0.5413, data_loss: 0.5413\n",
      "step 2620 , total_loss: 0.5659, data_loss: 0.5659\n",
      "step 2640 , total_loss: 0.5598, data_loss: 0.5598\n",
      "step 2660 , total_loss: 0.5762, data_loss: 0.5762\n",
      "step 2680 , total_loss: 0.5989, data_loss: 0.5989\n",
      "step 2700 , total_loss: 0.6803, data_loss: 0.6803\n",
      "step 2720 , total_loss: 0.5532, data_loss: 0.5532\n",
      "step 2740 , total_loss: 0.6081, data_loss: 0.6081\n",
      "step 2760 , total_loss: 0.5612, data_loss: 0.5612\n",
      "step 2780 , total_loss: 0.6258, data_loss: 0.6258\n",
      "step 2800 , total_loss: 0.6502, data_loss: 0.6502\n",
      "step 2820 , total_loss: 0.6286, data_loss: 0.6286\n",
      "step 2840 , total_loss: 0.6190, data_loss: 0.6190\n",
      "step 2860 , total_loss: 0.6412, data_loss: 0.6412\n",
      "step 2880 , total_loss: 0.6977, data_loss: 0.6977\n",
      "step 2900 , total_loss: 0.5976, data_loss: 0.5976\n",
      "step 2920 , total_loss: 0.6038, data_loss: 0.6038\n",
      "step 2940 , total_loss: 0.6019, data_loss: 0.6019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2960 , total_loss: 0.5635, data_loss: 0.5635\n",
      "step 2980 , total_loss: 0.6342, data_loss: 0.6342\n",
      "step 3000 , total_loss: 0.6068, data_loss: 0.6068\n",
      "step 3020 , total_loss: 0.6091, data_loss: 0.6091\n",
      "step 3040 , total_loss: 0.6741, data_loss: 0.6741\n",
      "step 3060 , total_loss: 0.6041, data_loss: 0.6041\n",
      "step 3080 , total_loss: 0.6068, data_loss: 0.6068\n",
      "step 3100 , total_loss: 0.5978, data_loss: 0.5978\n",
      "step 3120 , total_loss: 0.6106, data_loss: 0.6106\n",
      "step 3140 , total_loss: 0.6288, data_loss: 0.6288\n",
      "step 3160 , total_loss: 0.6017, data_loss: 0.6017\n",
      "step 3180 , total_loss: 0.6462, data_loss: 0.6462\n",
      "step 3200 , total_loss: 0.5918, data_loss: 0.5918\n",
      "step 3220 , total_loss: 0.6137, data_loss: 0.6137\n",
      "step 3240 , total_loss: 0.6285, data_loss: 0.6285\n",
      "step 3260 , total_loss: 0.6276, data_loss: 0.6276\n",
      "step 3280 , total_loss: 0.6439, data_loss: 0.6439\n",
      "step 3300 , total_loss: 0.6080, data_loss: 0.6080\n",
      "eval valid at epoch 10: auc:0.8761,logloss:0.8605,mean_mrr:0.8197,ndcg@2:0.8021,ndcg@4:0.8572,ndcg@6:0.8652,group_auc:0.8697\n",
      "[(1, {'auc': 0.8234, 'logloss': 0.5657, 'mean_mrr': 0.7524, 'ndcg@2': 0.7213, 'ndcg@4': 0.8008, 'ndcg@6': 0.8146, 'group_auc': 0.814}), (2, {'auc': 0.8514, 'logloss': 0.5886, 'mean_mrr': 0.7838, 'ndcg@2': 0.7593, 'ndcg@4': 0.8279, 'ndcg@6': 0.8383, 'group_auc': 0.841}), (3, {'auc': 0.8597, 'logloss': 0.6299, 'mean_mrr': 0.7973, 'ndcg@2': 0.7753, 'ndcg@4': 0.8389, 'ndcg@6': 0.8484, 'group_auc': 0.852}), (4, {'auc': 0.8659, 'logloss': 0.6379, 'mean_mrr': 0.8058, 'ndcg@2': 0.7858, 'ndcg@4': 0.8458, 'ndcg@6': 0.8548, 'group_auc': 0.8586}), (5, {'auc': 0.8728, 'logloss': 0.7035, 'mean_mrr': 0.8128, 'ndcg@2': 0.7947, 'ndcg@4': 0.8519, 'ndcg@6': 0.8601, 'group_auc': 0.865}), (6, {'auc': 0.8742, 'logloss': 0.7442, 'mean_mrr': 0.8158, 'ndcg@2': 0.7979, 'ndcg@4': 0.8543, 'ndcg@6': 0.8623, 'group_auc': 0.8671}), (7, {'auc': 0.876, 'logloss': 0.7891, 'mean_mrr': 0.8182, 'ndcg@2': 0.8008, 'ndcg@4': 0.8563, 'ndcg@6': 0.8641, 'group_auc': 0.8691}), (8, {'auc': 0.8765, 'logloss': 0.8245, 'mean_mrr': 0.8197, 'ndcg@2': 0.8027, 'ndcg@4': 0.8574, 'ndcg@6': 0.8652, 'group_auc': 0.8702}), (9, {'auc': 0.8779, 'logloss': 0.8184, 'mean_mrr': 0.8211, 'ndcg@2': 0.8036, 'ndcg@4': 0.8585, 'ndcg@6': 0.8663, 'group_auc': 0.8709}), (10, {'auc': 0.8761, 'logloss': 0.8605, 'mean_mrr': 0.8197, 'ndcg@2': 0.8021, 'ndcg@4': 0.8572, 'ndcg@6': 0.8652, 'group_auc': 0.8697})]\n",
      "best epoch: 9\n",
      "Time cost for training is 106.95 mins\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e41391",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34419/492773312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_num_ngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_syn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c751c05",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 08:41:37.748275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:37.799293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:37.799487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-05-24 08:41:39.609115: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-24 08:41:39.610497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:39.610688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:39.610805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:40.050764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:40.050932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:40.051045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 08:41:40.051143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8115 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-05-24 08:41:40.085201: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved model in ../../tests/resources/deeprec/slirec/model/best_model\n"
     ]
    }
   ],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 08:42:20.828762: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'auc': 0.8729,\n",
       " 'logloss': 0.8969,\n",
       " 'mean_mrr': 0.7128,\n",
       " 'ndcg@2': 0.6716,\n",
       " 'ndcg@4': 0.7388,\n",
       " 'ndcg@6': 0.7642,\n",
       " 'group_auc': 0.8648}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
