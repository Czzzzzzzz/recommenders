{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/din_sum_pooling/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/din_sum_pooling/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "                          attention_mode=\"sum_pooling\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a804b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['relu', 'relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'attention_mod': 'inner_product',\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/din_sum_pooling/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/din_sum_pooling/',\n",
       " 'attention_mode': 'sum_pooling'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa6a1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-04 08:55:17.465481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:17.522943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:17.523128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:17.973985: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-04 08:55:17.975060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:17.975233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:17.975346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:18.445710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:18.445858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:18.445963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 08:55:18.446049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9796 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-04 08:55:18.455776: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-06-04 08:57:08.035355: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.6080, data_loss: 1.6080\n",
      "step 40 , total_loss: 1.6070, data_loss: 1.6070\n",
      "step 60 , total_loss: 1.5773, data_loss: 1.5773\n",
      "step 80 , total_loss: 1.5347, data_loss: 1.5347\n",
      "step 100 , total_loss: 1.5522, data_loss: 1.5522\n",
      "step 120 , total_loss: 1.5548, data_loss: 1.5548\n",
      "step 140 , total_loss: 1.5520, data_loss: 1.5520\n",
      "step 160 , total_loss: 1.5496, data_loss: 1.5496\n",
      "step 180 , total_loss: 1.5579, data_loss: 1.5579\n",
      "step 200 , total_loss: 1.5484, data_loss: 1.5484\n",
      "step 220 , total_loss: 1.5317, data_loss: 1.5317\n",
      "step 240 , total_loss: 1.5367, data_loss: 1.5367\n",
      "step 260 , total_loss: 1.5607, data_loss: 1.5607\n",
      "step 280 , total_loss: 1.5311, data_loss: 1.5311\n",
      "step 300 , total_loss: 1.5217, data_loss: 1.5217\n",
      "step 320 , total_loss: 1.5196, data_loss: 1.5196\n",
      "step 340 , total_loss: 1.5241, data_loss: 1.5241\n",
      "step 360 , total_loss: 1.5295, data_loss: 1.5295\n",
      "step 380 , total_loss: 1.5077, data_loss: 1.5077\n",
      "step 400 , total_loss: 1.4783, data_loss: 1.4783\n",
      "step 420 , total_loss: 1.5214, data_loss: 1.5214\n",
      "step 440 , total_loss: 1.5077, data_loss: 1.5077\n",
      "step 460 , total_loss: 1.4605, data_loss: 1.4605\n",
      "step 480 , total_loss: 1.4869, data_loss: 1.4869\n",
      "step 500 , total_loss: 1.4646, data_loss: 1.4646\n",
      "step 520 , total_loss: 1.4672, data_loss: 1.4672\n",
      "step 540 , total_loss: 1.4748, data_loss: 1.4748\n",
      "step 560 , total_loss: 1.4550, data_loss: 1.4550\n",
      "step 580 , total_loss: 1.5363, data_loss: 1.5363\n",
      "step 600 , total_loss: 1.4671, data_loss: 1.4671\n",
      "step 620 , total_loss: 1.4707, data_loss: 1.4707\n",
      "step 640 , total_loss: 1.4372, data_loss: 1.4372\n",
      "step 660 , total_loss: 1.4470, data_loss: 1.4470\n",
      "step 680 , total_loss: 1.4372, data_loss: 1.4372\n",
      "step 700 , total_loss: 1.4124, data_loss: 1.4124\n",
      "step 720 , total_loss: 1.4444, data_loss: 1.4444\n",
      "step 740 , total_loss: 1.4629, data_loss: 1.4629\n",
      "step 760 , total_loss: 1.4271, data_loss: 1.4271\n",
      "step 780 , total_loss: 1.4580, data_loss: 1.4580\n",
      "step 800 , total_loss: 1.3977, data_loss: 1.3977\n",
      "step 820 , total_loss: 1.4349, data_loss: 1.4349\n",
      "step 840 , total_loss: 1.4414, data_loss: 1.4414\n",
      "step 860 , total_loss: 1.4020, data_loss: 1.4020\n",
      "step 880 , total_loss: 1.4326, data_loss: 1.4326\n",
      "step 900 , total_loss: 1.4225, data_loss: 1.4225\n",
      "step 920 , total_loss: 1.5252, data_loss: 1.5252\n",
      "step 940 , total_loss: 1.3552, data_loss: 1.3552\n",
      "step 960 , total_loss: 1.4402, data_loss: 1.4402\n",
      "step 980 , total_loss: 1.3870, data_loss: 1.3870\n",
      "step 1000 , total_loss: 1.4372, data_loss: 1.4372\n",
      "step 1020 , total_loss: 1.4424, data_loss: 1.4424\n",
      "step 1040 , total_loss: 1.4203, data_loss: 1.4203\n",
      "step 1060 , total_loss: 1.4329, data_loss: 1.4329\n",
      "step 1080 , total_loss: 1.4598, data_loss: 1.4598\n",
      "step 1100 , total_loss: 1.4086, data_loss: 1.4086\n",
      "step 1120 , total_loss: 1.4384, data_loss: 1.4384\n",
      "step 1140 , total_loss: 1.3731, data_loss: 1.3731\n",
      "step 1160 , total_loss: 1.3881, data_loss: 1.3881\n",
      "step 1180 , total_loss: 1.3816, data_loss: 1.3816\n",
      "step 1200 , total_loss: 1.4383, data_loss: 1.4383\n",
      "step 1220 , total_loss: 1.4189, data_loss: 1.4189\n",
      "step 1240 , total_loss: 1.3969, data_loss: 1.3969\n",
      "step 1260 , total_loss: 1.4334, data_loss: 1.4334\n",
      "step 1280 , total_loss: 1.3646, data_loss: 1.3646\n",
      "step 1300 , total_loss: 1.3828, data_loss: 1.3828\n",
      "step 1320 , total_loss: 1.4423, data_loss: 1.4423\n",
      "step 1340 , total_loss: 1.3804, data_loss: 1.3804\n",
      "step 1360 , total_loss: 1.3910, data_loss: 1.3910\n",
      "step 1380 , total_loss: 1.3905, data_loss: 1.3905\n",
      "step 1400 , total_loss: 1.4028, data_loss: 1.4028\n",
      "step 1420 , total_loss: 1.3567, data_loss: 1.3567\n",
      "step 1440 , total_loss: 1.3271, data_loss: 1.3271\n",
      "step 1460 , total_loss: 1.3884, data_loss: 1.3884\n",
      "step 1480 , total_loss: 1.3167, data_loss: 1.3167\n",
      "step 1500 , total_loss: 1.3505, data_loss: 1.3505\n",
      "step 1520 , total_loss: 1.3640, data_loss: 1.3640\n",
      "step 1540 , total_loss: 1.4144, data_loss: 1.4144\n",
      "step 1560 , total_loss: 1.3678, data_loss: 1.3678\n",
      "step 1580 , total_loss: 1.3908, data_loss: 1.3908\n",
      "step 1600 , total_loss: 1.4491, data_loss: 1.4491\n",
      "step 1620 , total_loss: 1.4374, data_loss: 1.4374\n",
      "step 1640 , total_loss: 1.3209, data_loss: 1.3209\n",
      "step 1660 , total_loss: 1.3645, data_loss: 1.3645\n",
      "step 1680 , total_loss: 1.4232, data_loss: 1.4232\n",
      "step 1700 , total_loss: 1.3772, data_loss: 1.3772\n",
      "step 1720 , total_loss: 1.4049, data_loss: 1.4049\n",
      "step 1740 , total_loss: 1.3564, data_loss: 1.3564\n",
      "step 1760 , total_loss: 1.3643, data_loss: 1.3643\n",
      "step 1780 , total_loss: 1.3332, data_loss: 1.3332\n",
      "step 1800 , total_loss: 1.4285, data_loss: 1.4285\n",
      "step 1820 , total_loss: 1.3189, data_loss: 1.3189\n",
      "step 1840 , total_loss: 1.3968, data_loss: 1.3968\n",
      "step 1860 , total_loss: 1.3366, data_loss: 1.3366\n",
      "step 1880 , total_loss: 1.3253, data_loss: 1.3253\n",
      "step 1900 , total_loss: 1.3801, data_loss: 1.3801\n",
      "step 1920 , total_loss: 1.3211, data_loss: 1.3211\n",
      "step 1940 , total_loss: 1.3953, data_loss: 1.3953\n",
      "step 1960 , total_loss: 1.3804, data_loss: 1.3804\n",
      "step 1980 , total_loss: 1.3226, data_loss: 1.3226\n",
      "step 2000 , total_loss: 1.3923, data_loss: 1.3923\n",
      "step 2020 , total_loss: 1.3133, data_loss: 1.3133\n",
      "step 2040 , total_loss: 1.3291, data_loss: 1.3291\n",
      "step 2060 , total_loss: 1.3331, data_loss: 1.3331\n",
      "step 2080 , total_loss: 1.2939, data_loss: 1.2939\n",
      "step 2100 , total_loss: 1.2836, data_loss: 1.2836\n",
      "step 2120 , total_loss: 1.3057, data_loss: 1.3057\n",
      "step 2140 , total_loss: 1.3107, data_loss: 1.3107\n",
      "step 2160 , total_loss: 1.2857, data_loss: 1.2857\n",
      "step 2180 , total_loss: 1.3121, data_loss: 1.3121\n",
      "step 2200 , total_loss: 1.2868, data_loss: 1.2868\n",
      "step 2220 , total_loss: 1.2843, data_loss: 1.2843\n",
      "step 2240 , total_loss: 1.3370, data_loss: 1.3370\n",
      "step 2260 , total_loss: 1.3157, data_loss: 1.3157\n",
      "step 2280 , total_loss: 1.3556, data_loss: 1.3556\n",
      "step 2300 , total_loss: 1.3763, data_loss: 1.3763\n",
      "step 2320 , total_loss: 1.3870, data_loss: 1.3870\n",
      "step 2340 , total_loss: 1.2960, data_loss: 1.2960\n",
      "step 2360 , total_loss: 1.2446, data_loss: 1.2446\n",
      "step 2380 , total_loss: 1.3075, data_loss: 1.3075\n",
      "step 2400 , total_loss: 1.4274, data_loss: 1.4274\n",
      "step 2420 , total_loss: 1.3338, data_loss: 1.3338\n",
      "step 2440 , total_loss: 1.3323, data_loss: 1.3323\n",
      "step 2460 , total_loss: 1.2823, data_loss: 1.2823\n",
      "step 2480 , total_loss: 1.2760, data_loss: 1.2760\n",
      "step 2500 , total_loss: 1.3417, data_loss: 1.3417\n",
      "step 2520 , total_loss: 1.3172, data_loss: 1.3172\n",
      "step 2540 , total_loss: 1.2950, data_loss: 1.2950\n",
      "step 2560 , total_loss: 1.3412, data_loss: 1.3412\n",
      "step 2580 , total_loss: 1.2765, data_loss: 1.2765\n",
      "step 2600 , total_loss: 1.2663, data_loss: 1.2663\n",
      "step 2620 , total_loss: 1.3092, data_loss: 1.3092\n",
      "step 2640 , total_loss: 1.2463, data_loss: 1.2463\n",
      "step 2660 , total_loss: 1.2803, data_loss: 1.2803\n",
      "step 2680 , total_loss: 1.2698, data_loss: 1.2698\n",
      "step 2700 , total_loss: 1.2848, data_loss: 1.2848\n",
      "step 2720 , total_loss: 1.2996, data_loss: 1.2996\n",
      "step 2740 , total_loss: 1.3797, data_loss: 1.3797\n",
      "step 2760 , total_loss: 1.2643, data_loss: 1.2643\n",
      "step 2780 , total_loss: 1.3102, data_loss: 1.3102\n",
      "step 2800 , total_loss: 1.3541, data_loss: 1.3541\n",
      "step 2820 , total_loss: 1.3162, data_loss: 1.3162\n",
      "step 2840 , total_loss: 1.3430, data_loss: 1.3430\n",
      "step 2860 , total_loss: 1.3213, data_loss: 1.3213\n",
      "step 2880 , total_loss: 1.2977, data_loss: 1.2977\n",
      "step 2900 , total_loss: 1.3662, data_loss: 1.3662\n",
      "step 2920 , total_loss: 1.2327, data_loss: 1.2327\n",
      "step 2940 , total_loss: 1.2369, data_loss: 1.2369\n",
      "step 2960 , total_loss: 1.3120, data_loss: 1.3120\n",
      "step 2980 , total_loss: 1.2743, data_loss: 1.2743\n",
      "step 3000 , total_loss: 1.3094, data_loss: 1.3094\n",
      "step 3020 , total_loss: 1.3001, data_loss: 1.3001\n",
      "step 3040 , total_loss: 1.3264, data_loss: 1.3264\n",
      "step 3060 , total_loss: 1.3141, data_loss: 1.3141\n",
      "step 3080 , total_loss: 1.2145, data_loss: 1.2145\n",
      "step 3100 , total_loss: 1.2604, data_loss: 1.2604\n",
      "step 3120 , total_loss: 1.2902, data_loss: 1.2902\n",
      "step 3140 , total_loss: 1.2535, data_loss: 1.2535\n",
      "step 3160 , total_loss: 1.2264, data_loss: 1.2264\n",
      "step 3180 , total_loss: 1.2581, data_loss: 1.2581\n",
      "step 3200 , total_loss: 1.2507, data_loss: 1.2507\n",
      "step 3220 , total_loss: 1.2655, data_loss: 1.2655\n",
      "step 3240 , total_loss: 1.2480, data_loss: 1.2480\n",
      "step 3260 , total_loss: 1.2823, data_loss: 1.2823\n",
      "step 3280 , total_loss: 1.2307, data_loss: 1.2307\n",
      "step 3300 , total_loss: 1.2799, data_loss: 1.2799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval valid at epoch 1: auc:0.7566,logloss:0.7411,mean_mrr:0.6879,ndcg@2:0.6409,ndcg@4:0.745,ndcg@6:0.766,group_auc:0.7569\n",
      "step 20 , total_loss: 1.2215, data_loss: 1.2215\n",
      "step 40 , total_loss: 1.1664, data_loss: 1.1664\n",
      "step 60 , total_loss: 1.1949, data_loss: 1.1949\n",
      "step 80 , total_loss: 1.1979, data_loss: 1.1979\n",
      "step 100 , total_loss: 1.2577, data_loss: 1.2577\n",
      "step 120 , total_loss: 1.1915, data_loss: 1.1915\n",
      "step 140 , total_loss: 1.2080, data_loss: 1.2080\n",
      "step 160 , total_loss: 1.2801, data_loss: 1.2801\n",
      "step 180 , total_loss: 1.2241, data_loss: 1.2241\n",
      "step 200 , total_loss: 1.2416, data_loss: 1.2416\n",
      "step 220 , total_loss: 1.2071, data_loss: 1.2071\n",
      "step 240 , total_loss: 1.1806, data_loss: 1.1806\n",
      "step 260 , total_loss: 1.1966, data_loss: 1.1966\n",
      "step 280 , total_loss: 1.1860, data_loss: 1.1860\n",
      "step 300 , total_loss: 1.1688, data_loss: 1.1688\n",
      "step 320 , total_loss: 1.2680, data_loss: 1.2680\n",
      "step 340 , total_loss: 1.1738, data_loss: 1.1738\n",
      "step 360 , total_loss: 1.3408, data_loss: 1.3408\n",
      "step 380 , total_loss: 1.2135, data_loss: 1.2135\n",
      "step 400 , total_loss: 1.1900, data_loss: 1.1900\n",
      "step 420 , total_loss: 1.3292, data_loss: 1.3292\n",
      "step 440 , total_loss: 1.2180, data_loss: 1.2180\n",
      "step 460 , total_loss: 1.1778, data_loss: 1.1778\n",
      "step 480 , total_loss: 1.2230, data_loss: 1.2230\n",
      "step 500 , total_loss: 1.2095, data_loss: 1.2095\n",
      "step 520 , total_loss: 1.2085, data_loss: 1.2085\n",
      "step 540 , total_loss: 1.1243, data_loss: 1.1243\n",
      "step 560 , total_loss: 1.1905, data_loss: 1.1905\n",
      "step 580 , total_loss: 1.2377, data_loss: 1.2377\n",
      "step 600 , total_loss: 1.2658, data_loss: 1.2658\n",
      "step 620 , total_loss: 1.2396, data_loss: 1.2396\n",
      "step 640 , total_loss: 1.2318, data_loss: 1.2318\n",
      "step 660 , total_loss: 1.2203, data_loss: 1.2203\n",
      "step 680 , total_loss: 1.1483, data_loss: 1.1483\n",
      "step 700 , total_loss: 1.2047, data_loss: 1.2047\n",
      "step 720 , total_loss: 1.2002, data_loss: 1.2002\n",
      "step 740 , total_loss: 1.2118, data_loss: 1.2118\n",
      "step 760 , total_loss: 1.1659, data_loss: 1.1659\n",
      "step 780 , total_loss: 1.2769, data_loss: 1.2769\n",
      "step 800 , total_loss: 1.1660, data_loss: 1.1660\n",
      "step 820 , total_loss: 1.2199, data_loss: 1.2199\n",
      "step 840 , total_loss: 1.1635, data_loss: 1.1635\n",
      "step 860 , total_loss: 1.2204, data_loss: 1.2204\n",
      "step 880 , total_loss: 1.2008, data_loss: 1.2008\n",
      "step 900 , total_loss: 1.1564, data_loss: 1.1564\n",
      "step 920 , total_loss: 1.2053, data_loss: 1.2053\n",
      "step 940 , total_loss: 1.1963, data_loss: 1.1963\n",
      "step 960 , total_loss: 1.1240, data_loss: 1.1240\n",
      "step 980 , total_loss: 1.2136, data_loss: 1.2136\n",
      "step 1000 , total_loss: 1.2087, data_loss: 1.2087\n",
      "step 1020 , total_loss: 1.2057, data_loss: 1.2057\n",
      "step 1040 , total_loss: 1.1638, data_loss: 1.1638\n",
      "step 1060 , total_loss: 1.1868, data_loss: 1.1868\n",
      "step 1080 , total_loss: 1.2522, data_loss: 1.2522\n",
      "step 1100 , total_loss: 1.1639, data_loss: 1.1639\n",
      "step 1120 , total_loss: 1.1203, data_loss: 1.1203\n",
      "step 1140 , total_loss: 1.1753, data_loss: 1.1753\n",
      "step 1160 , total_loss: 1.1745, data_loss: 1.1745\n",
      "step 1180 , total_loss: 1.1760, data_loss: 1.1760\n",
      "step 1200 , total_loss: 1.2192, data_loss: 1.2192\n",
      "step 1220 , total_loss: 1.2012, data_loss: 1.2012\n",
      "step 1240 , total_loss: 1.2301, data_loss: 1.2301\n",
      "step 1260 , total_loss: 1.1467, data_loss: 1.1467\n",
      "step 1280 , total_loss: 1.1714, data_loss: 1.1714\n",
      "step 1300 , total_loss: 1.2007, data_loss: 1.2007\n",
      "step 1320 , total_loss: 1.1788, data_loss: 1.1788\n",
      "step 1340 , total_loss: 1.1734, data_loss: 1.1734\n",
      "step 1360 , total_loss: 1.1622, data_loss: 1.1622\n",
      "step 1380 , total_loss: 1.1701, data_loss: 1.1701\n",
      "step 1400 , total_loss: 1.1603, data_loss: 1.1603\n",
      "step 1420 , total_loss: 1.2028, data_loss: 1.2028\n",
      "step 1440 , total_loss: 1.1221, data_loss: 1.1221\n",
      "step 1460 , total_loss: 1.1821, data_loss: 1.1821\n",
      "step 1480 , total_loss: 1.1820, data_loss: 1.1820\n",
      "step 1500 , total_loss: 1.1783, data_loss: 1.1783\n",
      "step 1520 , total_loss: 1.1655, data_loss: 1.1655\n",
      "step 1540 , total_loss: 1.1566, data_loss: 1.1566\n",
      "step 1560 , total_loss: 1.2458, data_loss: 1.2458\n",
      "step 1580 , total_loss: 1.1706, data_loss: 1.1706\n",
      "step 1600 , total_loss: 1.2101, data_loss: 1.2101\n",
      "step 1620 , total_loss: 1.1657, data_loss: 1.1657\n",
      "step 1640 , total_loss: 1.2246, data_loss: 1.2246\n",
      "step 1660 , total_loss: 1.1746, data_loss: 1.1746\n",
      "step 1680 , total_loss: 1.1090, data_loss: 1.1090\n",
      "step 1700 , total_loss: 1.1597, data_loss: 1.1597\n",
      "step 1720 , total_loss: 1.1146, data_loss: 1.1146\n",
      "step 1740 , total_loss: 1.1928, data_loss: 1.1928\n",
      "step 1760 , total_loss: 1.1122, data_loss: 1.1122\n",
      "step 1780 , total_loss: 1.1801, data_loss: 1.1801\n",
      "step 1800 , total_loss: 1.1474, data_loss: 1.1474\n",
      "step 1820 , total_loss: 1.1599, data_loss: 1.1599\n",
      "step 1840 , total_loss: 1.1734, data_loss: 1.1734\n",
      "step 1860 , total_loss: 1.1276, data_loss: 1.1276\n",
      "step 1880 , total_loss: 1.2225, data_loss: 1.2225\n",
      "step 1900 , total_loss: 1.1441, data_loss: 1.1441\n",
      "step 1920 , total_loss: 1.1397, data_loss: 1.1397\n",
      "step 1940 , total_loss: 1.2010, data_loss: 1.2010\n",
      "step 1960 , total_loss: 1.1634, data_loss: 1.1634\n",
      "step 1980 , total_loss: 1.2212, data_loss: 1.2212\n",
      "step 2000 , total_loss: 1.1868, data_loss: 1.1868\n",
      "step 2020 , total_loss: 1.1999, data_loss: 1.1999\n",
      "step 2040 , total_loss: 1.1839, data_loss: 1.1839\n",
      "step 2060 , total_loss: 1.2308, data_loss: 1.2308\n",
      "step 2080 , total_loss: 1.1292, data_loss: 1.1292\n",
      "step 2100 , total_loss: 1.1235, data_loss: 1.1235\n",
      "step 2120 , total_loss: 1.1614, data_loss: 1.1614\n",
      "step 2140 , total_loss: 1.1026, data_loss: 1.1026\n",
      "step 2160 , total_loss: 1.1554, data_loss: 1.1554\n",
      "step 2180 , total_loss: 1.2131, data_loss: 1.2131\n",
      "step 2200 , total_loss: 1.1670, data_loss: 1.1670\n",
      "step 2220 , total_loss: 1.1885, data_loss: 1.1885\n",
      "step 2240 , total_loss: 1.1215, data_loss: 1.1215\n",
      "step 2260 , total_loss: 1.2121, data_loss: 1.2121\n",
      "step 2280 , total_loss: 1.1134, data_loss: 1.1134\n",
      "step 2300 , total_loss: 1.1675, data_loss: 1.1675\n",
      "step 2320 , total_loss: 1.0817, data_loss: 1.0817\n",
      "step 2340 , total_loss: 1.1581, data_loss: 1.1581\n",
      "step 2360 , total_loss: 1.2064, data_loss: 1.2064\n",
      "step 2380 , total_loss: 1.0593, data_loss: 1.0593\n",
      "step 2400 , total_loss: 1.1761, data_loss: 1.1761\n",
      "step 2420 , total_loss: 1.1306, data_loss: 1.1306\n",
      "step 2440 , total_loss: 1.1758, data_loss: 1.1758\n",
      "step 2460 , total_loss: 1.1488, data_loss: 1.1488\n",
      "step 2480 , total_loss: 1.1086, data_loss: 1.1086\n",
      "step 2500 , total_loss: 1.1741, data_loss: 1.1741\n",
      "step 2520 , total_loss: 1.1706, data_loss: 1.1706\n",
      "step 2540 , total_loss: 1.1818, data_loss: 1.1818\n",
      "step 2560 , total_loss: 1.1295, data_loss: 1.1295\n",
      "step 2580 , total_loss: 1.2575, data_loss: 1.2575\n",
      "step 2600 , total_loss: 1.1542, data_loss: 1.1542\n",
      "step 2620 , total_loss: 1.2094, data_loss: 1.2094\n",
      "step 2640 , total_loss: 1.1924, data_loss: 1.1924\n",
      "step 2660 , total_loss: 1.2068, data_loss: 1.2068\n",
      "step 2680 , total_loss: 1.1276, data_loss: 1.1276\n",
      "step 2700 , total_loss: 1.1732, data_loss: 1.1732\n",
      "step 2720 , total_loss: 1.1903, data_loss: 1.1903\n",
      "step 2740 , total_loss: 1.1290, data_loss: 1.1290\n",
      "step 2760 , total_loss: 1.1413, data_loss: 1.1413\n",
      "step 2780 , total_loss: 1.2005, data_loss: 1.2005\n",
      "step 2800 , total_loss: 1.2420, data_loss: 1.2420\n",
      "step 2820 , total_loss: 1.1145, data_loss: 1.1145\n",
      "step 2840 , total_loss: 1.2193, data_loss: 1.2193\n",
      "step 2860 , total_loss: 1.1345, data_loss: 1.1345\n",
      "step 2880 , total_loss: 1.1628, data_loss: 1.1628\n",
      "step 2900 , total_loss: 1.1524, data_loss: 1.1524\n",
      "step 2920 , total_loss: 1.1571, data_loss: 1.1571\n",
      "step 2940 , total_loss: 1.2193, data_loss: 1.2193\n",
      "step 2960 , total_loss: 1.1521, data_loss: 1.1521\n",
      "step 2980 , total_loss: 1.1074, data_loss: 1.1074\n",
      "step 3000 , total_loss: 1.1230, data_loss: 1.1230\n",
      "step 3020 , total_loss: 1.1229, data_loss: 1.1229\n",
      "step 3040 , total_loss: 1.1103, data_loss: 1.1103\n",
      "step 3060 , total_loss: 1.1045, data_loss: 1.1045\n",
      "step 3080 , total_loss: 1.2052, data_loss: 1.2052\n",
      "step 3100 , total_loss: 1.1549, data_loss: 1.1549\n",
      "step 3120 , total_loss: 1.1163, data_loss: 1.1163\n",
      "step 3140 , total_loss: 1.1554, data_loss: 1.1554\n",
      "step 3160 , total_loss: 1.1615, data_loss: 1.1615\n",
      "step 3180 , total_loss: 1.1516, data_loss: 1.1516\n",
      "step 3200 , total_loss: 1.1558, data_loss: 1.1558\n",
      "step 3220 , total_loss: 1.1322, data_loss: 1.1322\n",
      "step 3240 , total_loss: 1.1064, data_loss: 1.1064\n",
      "step 3260 , total_loss: 1.1802, data_loss: 1.1802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3280 , total_loss: 1.1596, data_loss: 1.1596\n",
      "step 3300 , total_loss: 1.1375, data_loss: 1.1375\n",
      "eval valid at epoch 2: auc:0.7879,logloss:0.7763,mean_mrr:0.7236,ndcg@2:0.6864,ndcg@4:0.7763,ndcg@6:0.7929,group_auc:0.7896\n",
      "step 20 , total_loss: 1.1176, data_loss: 1.1176\n",
      "step 40 , total_loss: 1.1296, data_loss: 1.1296\n",
      "step 60 , total_loss: 1.0346, data_loss: 1.0346\n",
      "step 80 , total_loss: 1.1152, data_loss: 1.1152\n",
      "step 100 , total_loss: 1.0457, data_loss: 1.0457\n",
      "step 120 , total_loss: 1.0871, data_loss: 1.0871\n",
      "step 140 , total_loss: 1.1076, data_loss: 1.1076\n",
      "step 160 , total_loss: 1.1007, data_loss: 1.1007\n",
      "step 180 , total_loss: 1.1363, data_loss: 1.1363\n",
      "step 200 , total_loss: 1.0172, data_loss: 1.0172\n",
      "step 220 , total_loss: 1.0851, data_loss: 1.0851\n",
      "step 240 , total_loss: 1.1939, data_loss: 1.1939\n",
      "step 260 , total_loss: 0.9940, data_loss: 0.9940\n",
      "step 280 , total_loss: 1.0794, data_loss: 1.0794\n",
      "step 300 , total_loss: 1.0618, data_loss: 1.0618\n",
      "step 320 , total_loss: 1.0846, data_loss: 1.0846\n",
      "step 340 , total_loss: 1.0510, data_loss: 1.0510\n",
      "step 360 , total_loss: 1.1300, data_loss: 1.1300\n",
      "step 380 , total_loss: 1.1219, data_loss: 1.1219\n",
      "step 400 , total_loss: 1.0556, data_loss: 1.0556\n",
      "step 420 , total_loss: 1.1304, data_loss: 1.1304\n",
      "step 440 , total_loss: 1.0887, data_loss: 1.0887\n",
      "step 460 , total_loss: 1.1748, data_loss: 1.1748\n",
      "step 480 , total_loss: 1.0857, data_loss: 1.0857\n",
      "step 500 , total_loss: 1.1458, data_loss: 1.1458\n",
      "step 520 , total_loss: 1.1061, data_loss: 1.1061\n",
      "step 540 , total_loss: 1.0625, data_loss: 1.0625\n",
      "step 560 , total_loss: 1.0235, data_loss: 1.0235\n",
      "step 580 , total_loss: 1.0963, data_loss: 1.0963\n",
      "step 600 , total_loss: 1.0813, data_loss: 1.0813\n",
      "step 620 , total_loss: 1.1045, data_loss: 1.1045\n",
      "step 640 , total_loss: 1.0972, data_loss: 1.0972\n",
      "step 660 , total_loss: 1.0917, data_loss: 1.0917\n",
      "step 680 , total_loss: 1.0673, data_loss: 1.0673\n",
      "step 700 , total_loss: 1.0600, data_loss: 1.0600\n",
      "step 720 , total_loss: 1.1054, data_loss: 1.1054\n",
      "step 740 , total_loss: 1.0931, data_loss: 1.0931\n",
      "step 760 , total_loss: 1.0979, data_loss: 1.0979\n",
      "step 780 , total_loss: 1.0422, data_loss: 1.0422\n",
      "step 800 , total_loss: 1.0575, data_loss: 1.0575\n",
      "step 820 , total_loss: 1.0701, data_loss: 1.0701\n",
      "step 840 , total_loss: 1.1084, data_loss: 1.1084\n",
      "step 860 , total_loss: 1.0725, data_loss: 1.0725\n",
      "step 880 , total_loss: 1.0808, data_loss: 1.0808\n",
      "step 900 , total_loss: 1.0690, data_loss: 1.0690\n",
      "step 920 , total_loss: 1.0457, data_loss: 1.0457\n",
      "step 940 , total_loss: 1.1407, data_loss: 1.1407\n",
      "step 960 , total_loss: 1.1314, data_loss: 1.1314\n",
      "step 980 , total_loss: 1.1096, data_loss: 1.1096\n",
      "step 1000 , total_loss: 1.0629, data_loss: 1.0629\n",
      "step 1020 , total_loss: 1.0527, data_loss: 1.0527\n",
      "step 1040 , total_loss: 1.1234, data_loss: 1.1234\n",
      "step 1060 , total_loss: 1.1054, data_loss: 1.1054\n",
      "step 1080 , total_loss: 1.0798, data_loss: 1.0798\n",
      "step 1100 , total_loss: 1.0458, data_loss: 1.0458\n",
      "step 1120 , total_loss: 1.0333, data_loss: 1.0333\n",
      "step 1140 , total_loss: 1.0293, data_loss: 1.0293\n",
      "step 1160 , total_loss: 1.0624, data_loss: 1.0624\n",
      "step 1180 , total_loss: 1.0993, data_loss: 1.0993\n",
      "step 1200 , total_loss: 1.1778, data_loss: 1.1778\n",
      "step 1220 , total_loss: 1.0623, data_loss: 1.0623\n",
      "step 1240 , total_loss: 1.1368, data_loss: 1.1368\n",
      "step 1260 , total_loss: 1.0226, data_loss: 1.0226\n",
      "step 1280 , total_loss: 1.1424, data_loss: 1.1424\n",
      "step 1300 , total_loss: 1.1127, data_loss: 1.1127\n",
      "step 1320 , total_loss: 1.1568, data_loss: 1.1568\n",
      "step 1340 , total_loss: 1.0701, data_loss: 1.0701\n",
      "step 1360 , total_loss: 1.0978, data_loss: 1.0978\n",
      "step 1380 , total_loss: 1.0317, data_loss: 1.0317\n",
      "step 1400 , total_loss: 1.0595, data_loss: 1.0595\n",
      "step 1420 , total_loss: 1.0909, data_loss: 1.0909\n",
      "step 1440 , total_loss: 1.0876, data_loss: 1.0876\n",
      "step 1460 , total_loss: 1.0782, data_loss: 1.0782\n",
      "step 1480 , total_loss: 1.1057, data_loss: 1.1057\n",
      "step 1500 , total_loss: 1.1895, data_loss: 1.1895\n",
      "step 1520 , total_loss: 1.0785, data_loss: 1.0785\n",
      "step 1540 , total_loss: 1.1529, data_loss: 1.1529\n",
      "step 1560 , total_loss: 1.0329, data_loss: 1.0329\n",
      "step 1580 , total_loss: 1.0776, data_loss: 1.0776\n",
      "step 1600 , total_loss: 1.1027, data_loss: 1.1027\n",
      "step 1620 , total_loss: 1.1022, data_loss: 1.1022\n",
      "step 1640 , total_loss: 1.0759, data_loss: 1.0759\n",
      "step 1660 , total_loss: 1.0880, data_loss: 1.0880\n",
      "step 1680 , total_loss: 1.0855, data_loss: 1.0855\n",
      "step 1700 , total_loss: 1.1322, data_loss: 1.1322\n",
      "step 1720 , total_loss: 1.0576, data_loss: 1.0576\n",
      "step 1740 , total_loss: 1.0554, data_loss: 1.0554\n",
      "step 1760 , total_loss: 1.0561, data_loss: 1.0561\n",
      "step 1780 , total_loss: 1.0658, data_loss: 1.0658\n",
      "step 1800 , total_loss: 1.0560, data_loss: 1.0560\n",
      "step 1820 , total_loss: 1.0808, data_loss: 1.0808\n",
      "step 1840 , total_loss: 1.0804, data_loss: 1.0804\n",
      "step 1860 , total_loss: 1.0924, data_loss: 1.0924\n",
      "step 1880 , total_loss: 1.0831, data_loss: 1.0831\n",
      "step 1900 , total_loss: 1.1306, data_loss: 1.1306\n",
      "step 1920 , total_loss: 1.0432, data_loss: 1.0432\n",
      "step 1940 , total_loss: 1.0879, data_loss: 1.0879\n",
      "step 1960 , total_loss: 1.1118, data_loss: 1.1118\n",
      "step 1980 , total_loss: 1.0392, data_loss: 1.0392\n",
      "step 2000 , total_loss: 1.0817, data_loss: 1.0817\n",
      "step 2020 , total_loss: 1.0716, data_loss: 1.0716\n",
      "step 2040 , total_loss: 1.0867, data_loss: 1.0867\n",
      "step 2060 , total_loss: 1.0432, data_loss: 1.0432\n",
      "step 2080 , total_loss: 1.0662, data_loss: 1.0662\n",
      "step 2100 , total_loss: 1.0813, data_loss: 1.0813\n",
      "step 2120 , total_loss: 1.0572, data_loss: 1.0572\n",
      "step 2140 , total_loss: 1.1611, data_loss: 1.1611\n",
      "step 2160 , total_loss: 1.1311, data_loss: 1.1311\n",
      "step 2180 , total_loss: 1.1565, data_loss: 1.1565\n",
      "step 2200 , total_loss: 1.1406, data_loss: 1.1406\n",
      "step 2220 , total_loss: 1.0603, data_loss: 1.0603\n",
      "step 2240 , total_loss: 1.1408, data_loss: 1.1408\n",
      "step 2260 , total_loss: 1.0720, data_loss: 1.0720\n",
      "step 2280 , total_loss: 1.1026, data_loss: 1.1026\n",
      "step 2300 , total_loss: 1.1202, data_loss: 1.1202\n",
      "step 2320 , total_loss: 0.9977, data_loss: 0.9977\n",
      "step 2340 , total_loss: 1.0739, data_loss: 1.0739\n",
      "step 2360 , total_loss: 1.0924, data_loss: 1.0924\n",
      "step 2380 , total_loss: 1.0680, data_loss: 1.0680\n",
      "step 2400 , total_loss: 1.1147, data_loss: 1.1147\n",
      "step 2420 , total_loss: 1.1514, data_loss: 1.1514\n",
      "step 2440 , total_loss: 1.0622, data_loss: 1.0622\n",
      "step 2460 , total_loss: 1.0733, data_loss: 1.0733\n",
      "step 2480 , total_loss: 1.0398, data_loss: 1.0398\n",
      "step 2500 , total_loss: 1.0672, data_loss: 1.0672\n",
      "step 2520 , total_loss: 1.0675, data_loss: 1.0675\n",
      "step 2540 , total_loss: 1.0701, data_loss: 1.0701\n",
      "step 2560 , total_loss: 1.0912, data_loss: 1.0912\n",
      "step 2580 , total_loss: 1.0503, data_loss: 1.0503\n",
      "step 2600 , total_loss: 1.0924, data_loss: 1.0924\n",
      "step 2620 , total_loss: 1.0784, data_loss: 1.0784\n",
      "step 2640 , total_loss: 1.0746, data_loss: 1.0746\n",
      "step 2660 , total_loss: 1.0727, data_loss: 1.0727\n",
      "step 2680 , total_loss: 1.1917, data_loss: 1.1917\n",
      "step 2700 , total_loss: 0.9765, data_loss: 0.9765\n",
      "step 2720 , total_loss: 1.0856, data_loss: 1.0856\n",
      "step 2740 , total_loss: 1.0860, data_loss: 1.0860\n",
      "step 2760 , total_loss: 1.1387, data_loss: 1.1387\n",
      "step 2780 , total_loss: 1.0757, data_loss: 1.0757\n",
      "step 2800 , total_loss: 1.0698, data_loss: 1.0698\n",
      "step 2820 , total_loss: 1.0979, data_loss: 1.0979\n",
      "step 2840 , total_loss: 1.0429, data_loss: 1.0429\n",
      "step 2860 , total_loss: 1.0969, data_loss: 1.0969\n",
      "step 2880 , total_loss: 1.2224, data_loss: 1.2224\n",
      "step 2900 , total_loss: 0.9845, data_loss: 0.9845\n",
      "step 2920 , total_loss: 1.0829, data_loss: 1.0829\n",
      "step 2940 , total_loss: 1.0737, data_loss: 1.0737\n",
      "step 2960 , total_loss: 1.1255, data_loss: 1.1255\n",
      "step 2980 , total_loss: 1.0321, data_loss: 1.0321\n",
      "step 3000 , total_loss: 1.0588, data_loss: 1.0588\n",
      "step 3020 , total_loss: 1.0771, data_loss: 1.0771\n",
      "step 3040 , total_loss: 1.0851, data_loss: 1.0851\n",
      "step 3060 , total_loss: 1.0537, data_loss: 1.0537\n",
      "step 3080 , total_loss: 1.1037, data_loss: 1.1037\n",
      "step 3100 , total_loss: 1.0966, data_loss: 1.0966\n",
      "step 3120 , total_loss: 1.0631, data_loss: 1.0631\n",
      "step 3140 , total_loss: 1.1518, data_loss: 1.1518\n",
      "step 3160 , total_loss: 1.0759, data_loss: 1.0759\n",
      "step 3180 , total_loss: 1.0332, data_loss: 1.0332\n",
      "step 3200 , total_loss: 1.0635, data_loss: 1.0635\n",
      "step 3220 , total_loss: 1.1033, data_loss: 1.1033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3240 , total_loss: 1.1154, data_loss: 1.1154\n",
      "step 3260 , total_loss: 1.1155, data_loss: 1.1155\n",
      "step 3280 , total_loss: 1.0868, data_loss: 1.0868\n",
      "step 3300 , total_loss: 1.0711, data_loss: 1.0711\n",
      "eval valid at epoch 3: auc:0.7929,logloss:0.9656,mean_mrr:0.7331,ndcg@2:0.6977,ndcg@4:0.7847,ndcg@6:0.8001,group_auc:0.7976\n",
      "step 20 , total_loss: 0.9759, data_loss: 0.9759\n",
      "step 40 , total_loss: 0.9874, data_loss: 0.9874\n",
      "step 60 , total_loss: 1.0063, data_loss: 1.0063\n",
      "step 80 , total_loss: 1.0108, data_loss: 1.0108\n",
      "step 100 , total_loss: 1.0479, data_loss: 1.0479\n",
      "step 120 , total_loss: 1.0273, data_loss: 1.0273\n",
      "step 140 , total_loss: 0.9958, data_loss: 0.9958\n",
      "step 160 , total_loss: 1.0533, data_loss: 1.0533\n",
      "step 180 , total_loss: 1.0152, data_loss: 1.0152\n",
      "step 200 , total_loss: 1.0000, data_loss: 1.0000\n",
      "step 220 , total_loss: 1.0125, data_loss: 1.0125\n",
      "step 240 , total_loss: 0.9638, data_loss: 0.9638\n",
      "step 260 , total_loss: 1.0101, data_loss: 1.0101\n",
      "step 280 , total_loss: 1.1145, data_loss: 1.1145\n",
      "step 300 , total_loss: 1.1159, data_loss: 1.1159\n",
      "step 320 , total_loss: 1.0268, data_loss: 1.0268\n",
      "step 340 , total_loss: 1.0432, data_loss: 1.0432\n",
      "step 360 , total_loss: 1.0407, data_loss: 1.0407\n",
      "step 380 , total_loss: 1.0288, data_loss: 1.0288\n",
      "step 400 , total_loss: 1.0793, data_loss: 1.0793\n",
      "step 420 , total_loss: 1.0553, data_loss: 1.0553\n",
      "step 440 , total_loss: 1.0010, data_loss: 1.0010\n",
      "step 460 , total_loss: 1.0743, data_loss: 1.0743\n",
      "step 480 , total_loss: 1.0147, data_loss: 1.0147\n",
      "step 500 , total_loss: 1.0641, data_loss: 1.0641\n",
      "step 520 , total_loss: 1.0043, data_loss: 1.0043\n",
      "step 540 , total_loss: 1.0197, data_loss: 1.0197\n",
      "step 560 , total_loss: 1.0020, data_loss: 1.0020\n",
      "step 580 , total_loss: 1.0297, data_loss: 1.0297\n",
      "step 600 , total_loss: 0.9577, data_loss: 0.9577\n",
      "step 620 , total_loss: 1.1380, data_loss: 1.1380\n",
      "step 640 , total_loss: 0.9834, data_loss: 0.9834\n",
      "step 660 , total_loss: 1.0223, data_loss: 1.0223\n",
      "step 680 , total_loss: 1.0199, data_loss: 1.0199\n",
      "step 700 , total_loss: 1.0735, data_loss: 1.0735\n",
      "step 720 , total_loss: 1.0424, data_loss: 1.0424\n",
      "step 740 , total_loss: 0.9795, data_loss: 0.9795\n",
      "step 760 , total_loss: 1.0849, data_loss: 1.0849\n",
      "step 780 , total_loss: 0.9440, data_loss: 0.9440\n",
      "step 800 , total_loss: 1.0412, data_loss: 1.0412\n",
      "step 820 , total_loss: 1.0391, data_loss: 1.0391\n",
      "step 840 , total_loss: 1.0919, data_loss: 1.0919\n",
      "step 860 , total_loss: 1.0363, data_loss: 1.0363\n",
      "step 880 , total_loss: 1.0727, data_loss: 1.0727\n",
      "step 900 , total_loss: 1.0953, data_loss: 1.0953\n",
      "step 920 , total_loss: 0.9880, data_loss: 0.9880\n",
      "step 940 , total_loss: 0.9625, data_loss: 0.9625\n",
      "step 960 , total_loss: 1.0793, data_loss: 1.0793\n",
      "step 980 , total_loss: 1.0717, data_loss: 1.0717\n",
      "step 1000 , total_loss: 1.0760, data_loss: 1.0760\n",
      "step 1020 , total_loss: 1.1299, data_loss: 1.1299\n",
      "step 1040 , total_loss: 0.9921, data_loss: 0.9921\n",
      "step 1060 , total_loss: 1.0310, data_loss: 1.0310\n",
      "step 1080 , total_loss: 1.1227, data_loss: 1.1227\n",
      "step 1100 , total_loss: 1.1090, data_loss: 1.1090\n",
      "step 1120 , total_loss: 1.0165, data_loss: 1.0165\n",
      "step 1140 , total_loss: 0.9516, data_loss: 0.9516\n",
      "step 1160 , total_loss: 1.0175, data_loss: 1.0175\n",
      "step 1180 , total_loss: 1.1184, data_loss: 1.1184\n",
      "step 1200 , total_loss: 0.9784, data_loss: 0.9784\n",
      "step 1220 , total_loss: 1.0450, data_loss: 1.0450\n",
      "step 1240 , total_loss: 1.0717, data_loss: 1.0717\n",
      "step 1260 , total_loss: 1.0031, data_loss: 1.0031\n",
      "step 1280 , total_loss: 0.9896, data_loss: 0.9896\n",
      "step 1300 , total_loss: 1.0408, data_loss: 1.0408\n",
      "step 1320 , total_loss: 1.0038, data_loss: 1.0038\n",
      "step 1340 , total_loss: 1.0390, data_loss: 1.0390\n",
      "step 1360 , total_loss: 1.0575, data_loss: 1.0575\n",
      "step 1380 , total_loss: 1.0589, data_loss: 1.0589\n",
      "step 1400 , total_loss: 1.0446, data_loss: 1.0446\n",
      "step 1420 , total_loss: 1.0480, data_loss: 1.0480\n",
      "step 1440 , total_loss: 0.9667, data_loss: 0.9667\n",
      "step 1460 , total_loss: 1.0902, data_loss: 1.0902\n",
      "step 1480 , total_loss: 0.9934, data_loss: 0.9934\n",
      "step 1500 , total_loss: 1.1109, data_loss: 1.1109\n",
      "step 1520 , total_loss: 1.0045, data_loss: 1.0045\n",
      "step 1540 , total_loss: 1.0114, data_loss: 1.0114\n",
      "step 1560 , total_loss: 1.1021, data_loss: 1.1021\n",
      "step 1580 , total_loss: 1.0928, data_loss: 1.0928\n",
      "step 1600 , total_loss: 0.9872, data_loss: 0.9872\n",
      "step 1620 , total_loss: 1.0901, data_loss: 1.0901\n",
      "step 1640 , total_loss: 1.0477, data_loss: 1.0477\n",
      "step 1660 , total_loss: 1.0136, data_loss: 1.0136\n",
      "step 1680 , total_loss: 1.0275, data_loss: 1.0275\n",
      "step 1700 , total_loss: 1.0576, data_loss: 1.0576\n",
      "step 1720 , total_loss: 1.1013, data_loss: 1.1013\n",
      "step 1740 , total_loss: 1.0245, data_loss: 1.0245\n",
      "step 1760 , total_loss: 1.0164, data_loss: 1.0164\n",
      "step 1780 , total_loss: 1.0630, data_loss: 1.0630\n",
      "step 1800 , total_loss: 0.9427, data_loss: 0.9427\n",
      "step 1820 , total_loss: 0.9897, data_loss: 0.9897\n",
      "step 1840 , total_loss: 1.0060, data_loss: 1.0060\n",
      "step 1860 , total_loss: 1.0529, data_loss: 1.0529\n",
      "step 1880 , total_loss: 1.0705, data_loss: 1.0705\n",
      "step 1900 , total_loss: 1.0410, data_loss: 1.0410\n",
      "step 1920 , total_loss: 1.0139, data_loss: 1.0139\n",
      "step 1940 , total_loss: 1.0619, data_loss: 1.0619\n",
      "step 1960 , total_loss: 0.9943, data_loss: 0.9943\n",
      "step 1980 , total_loss: 1.0355, data_loss: 1.0355\n",
      "step 2000 , total_loss: 1.1100, data_loss: 1.1100\n",
      "step 2020 , total_loss: 1.1157, data_loss: 1.1157\n",
      "step 2040 , total_loss: 1.0920, data_loss: 1.0920\n",
      "step 2060 , total_loss: 1.0314, data_loss: 1.0314\n",
      "step 2080 , total_loss: 1.1002, data_loss: 1.1002\n",
      "step 2100 , total_loss: 1.0995, data_loss: 1.0995\n",
      "step 2120 , total_loss: 1.0478, data_loss: 1.0478\n",
      "step 2140 , total_loss: 1.0103, data_loss: 1.0103\n",
      "step 2160 , total_loss: 1.0488, data_loss: 1.0488\n",
      "step 2180 , total_loss: 1.1142, data_loss: 1.1142\n",
      "step 2200 , total_loss: 1.0324, data_loss: 1.0324\n",
      "step 2220 , total_loss: 0.9855, data_loss: 0.9855\n",
      "step 2240 , total_loss: 1.1121, data_loss: 1.1121\n",
      "step 2260 , total_loss: 1.0500, data_loss: 1.0500\n",
      "step 2280 , total_loss: 1.0722, data_loss: 1.0722\n",
      "step 2300 , total_loss: 1.0771, data_loss: 1.0771\n",
      "step 2320 , total_loss: 1.0277, data_loss: 1.0277\n",
      "step 2340 , total_loss: 1.1307, data_loss: 1.1307\n",
      "step 2360 , total_loss: 1.0001, data_loss: 1.0001\n",
      "step 2380 , total_loss: 1.0270, data_loss: 1.0270\n",
      "step 2400 , total_loss: 1.0389, data_loss: 1.0389\n",
      "step 2420 , total_loss: 1.0587, data_loss: 1.0587\n",
      "step 2440 , total_loss: 1.0221, data_loss: 1.0221\n",
      "step 2460 , total_loss: 1.0362, data_loss: 1.0362\n",
      "step 2480 , total_loss: 1.0756, data_loss: 1.0756\n",
      "step 2500 , total_loss: 0.9790, data_loss: 0.9790\n",
      "step 2520 , total_loss: 1.0404, data_loss: 1.0404\n",
      "step 2540 , total_loss: 1.0661, data_loss: 1.0661\n",
      "step 2560 , total_loss: 1.0782, data_loss: 1.0782\n",
      "step 2580 , total_loss: 1.0171, data_loss: 1.0171\n",
      "step 2600 , total_loss: 1.0336, data_loss: 1.0336\n",
      "step 2620 , total_loss: 1.0942, data_loss: 1.0942\n",
      "step 2640 , total_loss: 1.0409, data_loss: 1.0409\n",
      "step 2660 , total_loss: 0.9910, data_loss: 0.9910\n",
      "step 2680 , total_loss: 1.0498, data_loss: 1.0498\n",
      "step 2700 , total_loss: 1.0183, data_loss: 1.0183\n",
      "step 2720 , total_loss: 1.0766, data_loss: 1.0766\n",
      "step 2740 , total_loss: 1.1484, data_loss: 1.1484\n",
      "step 2760 , total_loss: 1.0253, data_loss: 1.0253\n",
      "step 2780 , total_loss: 1.0200, data_loss: 1.0200\n",
      "step 2800 , total_loss: 1.0290, data_loss: 1.0290\n",
      "step 2820 , total_loss: 1.0467, data_loss: 1.0467\n",
      "step 2840 , total_loss: 1.0356, data_loss: 1.0356\n",
      "step 2860 , total_loss: 1.0368, data_loss: 1.0368\n",
      "step 2880 , total_loss: 1.0228, data_loss: 1.0228\n",
      "step 2900 , total_loss: 1.0103, data_loss: 1.0103\n",
      "step 2920 , total_loss: 1.0426, data_loss: 1.0426\n",
      "step 2940 , total_loss: 1.0064, data_loss: 1.0064\n",
      "step 2960 , total_loss: 0.9967, data_loss: 0.9967\n",
      "step 2980 , total_loss: 1.0793, data_loss: 1.0793\n",
      "step 3000 , total_loss: 1.0657, data_loss: 1.0657\n",
      "step 3020 , total_loss: 1.0457, data_loss: 1.0457\n",
      "step 3040 , total_loss: 0.9895, data_loss: 0.9895\n",
      "step 3060 , total_loss: 1.0090, data_loss: 1.0090\n",
      "step 3080 , total_loss: 1.0529, data_loss: 1.0529\n",
      "step 3100 , total_loss: 1.0354, data_loss: 1.0354\n",
      "step 3120 , total_loss: 1.0155, data_loss: 1.0155\n",
      "step 3140 , total_loss: 1.0605, data_loss: 1.0605\n",
      "step 3160 , total_loss: 1.0148, data_loss: 1.0148\n",
      "step 3180 , total_loss: 0.9714, data_loss: 0.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200 , total_loss: 1.0630, data_loss: 1.0630\n",
      "step 3220 , total_loss: 1.0015, data_loss: 1.0015\n",
      "step 3240 , total_loss: 0.9543, data_loss: 0.9543\n",
      "step 3260 , total_loss: 1.0646, data_loss: 1.0646\n",
      "step 3280 , total_loss: 1.0133, data_loss: 1.0133\n",
      "step 3300 , total_loss: 0.9747, data_loss: 0.9747\n",
      "eval valid at epoch 4: auc:0.8,logloss:1.0104,mean_mrr:0.7412,ndcg@2:0.7074,ndcg@4:0.7911,ndcg@6:0.8062,group_auc:0.8038\n",
      "step 20 , total_loss: 0.9590, data_loss: 0.9590\n",
      "step 40 , total_loss: 1.0819, data_loss: 1.0819\n",
      "step 60 , total_loss: 0.9829, data_loss: 0.9829\n",
      "step 80 , total_loss: 0.9412, data_loss: 0.9412\n",
      "step 100 , total_loss: 1.0827, data_loss: 1.0827\n",
      "step 120 , total_loss: 0.9747, data_loss: 0.9747\n",
      "step 140 , total_loss: 0.9785, data_loss: 0.9785\n",
      "step 160 , total_loss: 0.9957, data_loss: 0.9957\n",
      "step 180 , total_loss: 0.9046, data_loss: 0.9046\n",
      "step 200 , total_loss: 0.9855, data_loss: 0.9855\n",
      "step 220 , total_loss: 0.9411, data_loss: 0.9411\n",
      "step 240 , total_loss: 1.0039, data_loss: 1.0039\n",
      "step 260 , total_loss: 0.9966, data_loss: 0.9966\n",
      "step 280 , total_loss: 0.9831, data_loss: 0.9831\n",
      "step 300 , total_loss: 0.9794, data_loss: 0.9794\n",
      "step 320 , total_loss: 1.0002, data_loss: 1.0002\n",
      "step 340 , total_loss: 1.0194, data_loss: 1.0194\n",
      "step 360 , total_loss: 0.9076, data_loss: 0.9076\n",
      "step 380 , total_loss: 0.9435, data_loss: 0.9435\n",
      "step 400 , total_loss: 0.9413, data_loss: 0.9413\n",
      "step 420 , total_loss: 0.9589, data_loss: 0.9589\n",
      "step 440 , total_loss: 0.9890, data_loss: 0.9890\n",
      "step 460 , total_loss: 0.9308, data_loss: 0.9308\n",
      "step 480 , total_loss: 1.0244, data_loss: 1.0244\n",
      "step 500 , total_loss: 0.9395, data_loss: 0.9395\n",
      "step 520 , total_loss: 0.9686, data_loss: 0.9686\n",
      "step 540 , total_loss: 1.0028, data_loss: 1.0028\n",
      "step 560 , total_loss: 0.9567, data_loss: 0.9567\n",
      "step 580 , total_loss: 0.9835, data_loss: 0.9835\n",
      "step 600 , total_loss: 1.0055, data_loss: 1.0055\n",
      "step 620 , total_loss: 0.8920, data_loss: 0.8920\n",
      "step 640 , total_loss: 0.9359, data_loss: 0.9359\n",
      "step 660 , total_loss: 0.9659, data_loss: 0.9659\n",
      "step 680 , total_loss: 1.0892, data_loss: 1.0892\n",
      "step 700 , total_loss: 1.0446, data_loss: 1.0446\n",
      "step 720 , total_loss: 1.0410, data_loss: 1.0410\n",
      "step 740 , total_loss: 1.0529, data_loss: 1.0529\n",
      "step 760 , total_loss: 0.9693, data_loss: 0.9693\n",
      "step 780 , total_loss: 0.9937, data_loss: 0.9937\n",
      "step 800 , total_loss: 1.0024, data_loss: 1.0024\n",
      "step 820 , total_loss: 1.0341, data_loss: 1.0341\n",
      "step 840 , total_loss: 0.9631, data_loss: 0.9631\n",
      "step 860 , total_loss: 1.0623, data_loss: 1.0623\n",
      "step 880 , total_loss: 1.0038, data_loss: 1.0038\n",
      "step 900 , total_loss: 1.0874, data_loss: 1.0874\n",
      "step 920 , total_loss: 0.9384, data_loss: 0.9384\n",
      "step 940 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 960 , total_loss: 0.8823, data_loss: 0.8823\n",
      "step 980 , total_loss: 1.0547, data_loss: 1.0547\n",
      "step 1000 , total_loss: 1.0167, data_loss: 1.0167\n",
      "step 1020 , total_loss: 1.0129, data_loss: 1.0129\n",
      "step 1040 , total_loss: 1.0403, data_loss: 1.0403\n",
      "step 1060 , total_loss: 0.9601, data_loss: 0.9601\n",
      "step 1080 , total_loss: 0.9425, data_loss: 0.9425\n",
      "step 1100 , total_loss: 1.0646, data_loss: 1.0646\n",
      "step 1120 , total_loss: 0.9463, data_loss: 0.9463\n",
      "step 1140 , total_loss: 1.0349, data_loss: 1.0349\n",
      "step 1160 , total_loss: 1.0126, data_loss: 1.0126\n",
      "step 1180 , total_loss: 0.9774, data_loss: 0.9774\n",
      "step 1200 , total_loss: 0.9432, data_loss: 0.9432\n",
      "step 1220 , total_loss: 0.9598, data_loss: 0.9598\n",
      "step 1240 , total_loss: 0.9584, data_loss: 0.9584\n",
      "step 1260 , total_loss: 0.9743, data_loss: 0.9743\n",
      "step 1280 , total_loss: 0.9792, data_loss: 0.9792\n",
      "step 1300 , total_loss: 0.9989, data_loss: 0.9989\n",
      "step 1320 , total_loss: 0.9674, data_loss: 0.9674\n",
      "step 1340 , total_loss: 0.9627, data_loss: 0.9627\n",
      "step 1360 , total_loss: 1.0224, data_loss: 1.0224\n",
      "step 1380 , total_loss: 0.9560, data_loss: 0.9560\n",
      "step 1400 , total_loss: 0.9105, data_loss: 0.9105\n",
      "step 1420 , total_loss: 1.0581, data_loss: 1.0581\n",
      "step 1440 , total_loss: 0.9890, data_loss: 0.9890\n",
      "step 1460 , total_loss: 1.0160, data_loss: 1.0160\n",
      "step 1480 , total_loss: 0.9706, data_loss: 0.9706\n",
      "step 1500 , total_loss: 0.9965, data_loss: 0.9965\n",
      "step 1520 , total_loss: 0.9842, data_loss: 0.9842\n",
      "step 1540 , total_loss: 0.9348, data_loss: 0.9348\n",
      "step 1560 , total_loss: 0.9591, data_loss: 0.9591\n",
      "step 1580 , total_loss: 1.0596, data_loss: 1.0596\n",
      "step 1600 , total_loss: 1.0283, data_loss: 1.0283\n",
      "step 1620 , total_loss: 1.1083, data_loss: 1.1083\n",
      "step 1640 , total_loss: 0.9802, data_loss: 0.9802\n",
      "step 1660 , total_loss: 0.9353, data_loss: 0.9353\n",
      "step 1680 , total_loss: 1.0081, data_loss: 1.0081\n",
      "step 1700 , total_loss: 0.9846, data_loss: 0.9846\n",
      "step 1720 , total_loss: 0.9446, data_loss: 0.9446\n",
      "step 1740 , total_loss: 1.0515, data_loss: 1.0515\n",
      "step 1760 , total_loss: 0.9944, data_loss: 0.9944\n",
      "step 1780 , total_loss: 0.9744, data_loss: 0.9744\n",
      "step 1800 , total_loss: 0.9487, data_loss: 0.9487\n",
      "step 1820 , total_loss: 1.0642, data_loss: 1.0642\n",
      "step 1840 , total_loss: 0.9902, data_loss: 0.9902\n",
      "step 1860 , total_loss: 0.9766, data_loss: 0.9766\n",
      "step 1880 , total_loss: 1.0321, data_loss: 1.0321\n",
      "step 1900 , total_loss: 1.0738, data_loss: 1.0738\n",
      "step 1920 , total_loss: 0.9431, data_loss: 0.9431\n",
      "step 1940 , total_loss: 0.9809, data_loss: 0.9809\n",
      "step 1960 , total_loss: 0.9265, data_loss: 0.9265\n",
      "step 1980 , total_loss: 0.9397, data_loss: 0.9397\n",
      "step 2000 , total_loss: 0.9627, data_loss: 0.9627\n",
      "step 2020 , total_loss: 0.9751, data_loss: 0.9751\n",
      "step 2040 , total_loss: 0.9690, data_loss: 0.9690\n",
      "step 2060 , total_loss: 0.9792, data_loss: 0.9792\n",
      "step 2080 , total_loss: 0.9605, data_loss: 0.9605\n",
      "step 2100 , total_loss: 1.0400, data_loss: 1.0400\n",
      "step 2120 , total_loss: 0.9854, data_loss: 0.9854\n",
      "step 2140 , total_loss: 0.8831, data_loss: 0.8831\n",
      "step 2160 , total_loss: 1.0150, data_loss: 1.0150\n",
      "step 2180 , total_loss: 0.9883, data_loss: 0.9883\n",
      "step 2200 , total_loss: 1.0258, data_loss: 1.0258\n",
      "step 2220 , total_loss: 1.0019, data_loss: 1.0019\n",
      "step 2240 , total_loss: 1.0319, data_loss: 1.0319\n",
      "step 2260 , total_loss: 1.0130, data_loss: 1.0130\n",
      "step 2280 , total_loss: 0.9559, data_loss: 0.9559\n",
      "step 2300 , total_loss: 1.0460, data_loss: 1.0460\n",
      "step 2320 , total_loss: 0.9384, data_loss: 0.9384\n",
      "step 2340 , total_loss: 0.9989, data_loss: 0.9989\n",
      "step 2360 , total_loss: 0.9613, data_loss: 0.9613\n",
      "step 2380 , total_loss: 1.0058, data_loss: 1.0058\n",
      "step 2400 , total_loss: 1.0538, data_loss: 1.0538\n",
      "step 2420 , total_loss: 0.9634, data_loss: 0.9634\n",
      "step 2440 , total_loss: 1.0149, data_loss: 1.0149\n",
      "step 2460 , total_loss: 0.9901, data_loss: 0.9901\n",
      "step 2480 , total_loss: 1.0327, data_loss: 1.0327\n",
      "step 2500 , total_loss: 1.0263, data_loss: 1.0263\n",
      "step 2520 , total_loss: 1.0237, data_loss: 1.0237\n",
      "step 2540 , total_loss: 1.0241, data_loss: 1.0241\n",
      "step 2560 , total_loss: 0.9812, data_loss: 0.9812\n",
      "step 2580 , total_loss: 0.9550, data_loss: 0.9550\n",
      "step 2600 , total_loss: 0.9997, data_loss: 0.9997\n",
      "step 2620 , total_loss: 1.0515, data_loss: 1.0515\n",
      "step 2640 , total_loss: 0.9529, data_loss: 0.9529\n",
      "step 2660 , total_loss: 1.0281, data_loss: 1.0281\n",
      "step 2680 , total_loss: 0.9698, data_loss: 0.9698\n",
      "step 2700 , total_loss: 0.9986, data_loss: 0.9986\n",
      "step 2720 , total_loss: 0.9837, data_loss: 0.9837\n",
      "step 2740 , total_loss: 1.0194, data_loss: 1.0194\n",
      "step 2760 , total_loss: 1.0145, data_loss: 1.0145\n",
      "step 2780 , total_loss: 1.0334, data_loss: 1.0334\n",
      "step 2800 , total_loss: 0.9671, data_loss: 0.9671\n",
      "step 2820 , total_loss: 1.0746, data_loss: 1.0746\n",
      "step 2840 , total_loss: 1.0158, data_loss: 1.0158\n",
      "step 2860 , total_loss: 0.9641, data_loss: 0.9641\n",
      "step 2880 , total_loss: 0.9809, data_loss: 0.9809\n",
      "step 2900 , total_loss: 1.0214, data_loss: 1.0214\n",
      "step 2920 , total_loss: 0.9616, data_loss: 0.9616\n",
      "step 2940 , total_loss: 1.0028, data_loss: 1.0028\n",
      "step 2960 , total_loss: 1.0084, data_loss: 1.0084\n",
      "step 2980 , total_loss: 1.0411, data_loss: 1.0411\n",
      "step 3000 , total_loss: 1.1261, data_loss: 1.1261\n",
      "step 3020 , total_loss: 1.1049, data_loss: 1.1049\n",
      "step 3040 , total_loss: 1.0557, data_loss: 1.0557\n",
      "step 3060 , total_loss: 0.9647, data_loss: 0.9647\n",
      "step 3080 , total_loss: 1.0186, data_loss: 1.0186\n",
      "step 3100 , total_loss: 1.0066, data_loss: 1.0066\n",
      "step 3120 , total_loss: 0.9651, data_loss: 0.9651\n",
      "step 3140 , total_loss: 1.0411, data_loss: 1.0411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3160 , total_loss: 1.0339, data_loss: 1.0339\n",
      "step 3180 , total_loss: 1.0871, data_loss: 1.0871\n",
      "step 3200 , total_loss: 0.9449, data_loss: 0.9449\n",
      "step 3220 , total_loss: 1.0238, data_loss: 1.0238\n",
      "step 3240 , total_loss: 0.9594, data_loss: 0.9594\n",
      "step 3260 , total_loss: 0.9412, data_loss: 0.9412\n",
      "step 3280 , total_loss: 1.0440, data_loss: 1.0440\n",
      "step 3300 , total_loss: 1.0652, data_loss: 1.0652\n",
      "eval valid at epoch 5: auc:0.8044,logloss:1.1837,mean_mrr:0.7457,ndcg@2:0.7134,ndcg@4:0.7958,ndcg@6:0.8096,group_auc:0.809\n",
      "step 20 , total_loss: 0.9664, data_loss: 0.9664\n",
      "step 40 , total_loss: 0.9381, data_loss: 0.9381\n",
      "step 60 , total_loss: 0.9610, data_loss: 0.9610\n",
      "step 80 , total_loss: 0.9803, data_loss: 0.9803\n",
      "step 100 , total_loss: 0.9352, data_loss: 0.9352\n",
      "step 120 , total_loss: 1.0030, data_loss: 1.0030\n",
      "step 140 , total_loss: 1.0022, data_loss: 1.0022\n",
      "step 160 , total_loss: 0.9401, data_loss: 0.9401\n",
      "step 180 , total_loss: 0.9434, data_loss: 0.9434\n",
      "step 200 , total_loss: 1.0426, data_loss: 1.0426\n",
      "step 220 , total_loss: 0.9033, data_loss: 0.9033\n",
      "step 240 , total_loss: 1.0040, data_loss: 1.0040\n",
      "step 260 , total_loss: 0.9921, data_loss: 0.9921\n",
      "step 280 , total_loss: 0.9225, data_loss: 0.9225\n",
      "step 300 , total_loss: 1.0523, data_loss: 1.0523\n",
      "step 320 , total_loss: 0.9570, data_loss: 0.9570\n",
      "step 340 , total_loss: 1.0025, data_loss: 1.0025\n",
      "step 360 , total_loss: 0.8881, data_loss: 0.8881\n",
      "step 380 , total_loss: 0.8848, data_loss: 0.8848\n",
      "step 400 , total_loss: 0.9471, data_loss: 0.9471\n",
      "step 420 , total_loss: 0.8931, data_loss: 0.8931\n",
      "step 440 , total_loss: 0.9376, data_loss: 0.9376\n",
      "step 460 , total_loss: 0.9344, data_loss: 0.9344\n",
      "step 480 , total_loss: 0.8655, data_loss: 0.8655\n",
      "step 500 , total_loss: 0.9116, data_loss: 0.9116\n",
      "step 520 , total_loss: 0.9051, data_loss: 0.9051\n",
      "step 540 , total_loss: 0.9445, data_loss: 0.9445\n",
      "step 560 , total_loss: 0.9403, data_loss: 0.9403\n",
      "step 580 , total_loss: 0.9413, data_loss: 0.9413\n",
      "step 600 , total_loss: 1.0409, data_loss: 1.0409\n",
      "step 620 , total_loss: 0.9954, data_loss: 0.9954\n",
      "step 640 , total_loss: 1.0110, data_loss: 1.0110\n",
      "step 660 , total_loss: 0.9956, data_loss: 0.9956\n",
      "step 680 , total_loss: 0.9622, data_loss: 0.9622\n",
      "step 700 , total_loss: 0.9672, data_loss: 0.9672\n",
      "step 720 , total_loss: 0.9290, data_loss: 0.9290\n",
      "step 740 , total_loss: 0.9386, data_loss: 0.9386\n",
      "step 760 , total_loss: 0.9176, data_loss: 0.9176\n",
      "step 780 , total_loss: 0.9699, data_loss: 0.9699\n",
      "step 800 , total_loss: 0.9685, data_loss: 0.9685\n",
      "step 820 , total_loss: 0.9752, data_loss: 0.9752\n",
      "step 840 , total_loss: 0.9504, data_loss: 0.9504\n",
      "step 860 , total_loss: 0.9640, data_loss: 0.9640\n",
      "step 880 , total_loss: 0.9107, data_loss: 0.9107\n",
      "step 900 , total_loss: 0.9828, data_loss: 0.9828\n",
      "step 920 , total_loss: 0.9576, data_loss: 0.9576\n",
      "step 940 , total_loss: 0.8939, data_loss: 0.8939\n",
      "step 960 , total_loss: 0.9608, data_loss: 0.9608\n",
      "step 980 , total_loss: 1.0273, data_loss: 1.0273\n",
      "step 1000 , total_loss: 0.9585, data_loss: 0.9585\n",
      "step 1020 , total_loss: 0.9856, data_loss: 0.9856\n",
      "step 1040 , total_loss: 0.9796, data_loss: 0.9796\n",
      "step 1060 , total_loss: 0.9770, data_loss: 0.9770\n",
      "step 1080 , total_loss: 0.9600, data_loss: 0.9600\n",
      "step 1100 , total_loss: 0.9485, data_loss: 0.9485\n",
      "step 1120 , total_loss: 0.9309, data_loss: 0.9309\n",
      "step 1140 , total_loss: 0.9774, data_loss: 0.9774\n",
      "step 1160 , total_loss: 0.9157, data_loss: 0.9157\n",
      "step 1180 , total_loss: 1.0076, data_loss: 1.0076\n",
      "step 1200 , total_loss: 0.9369, data_loss: 0.9369\n",
      "step 1220 , total_loss: 0.9695, data_loss: 0.9695\n",
      "step 1240 , total_loss: 0.8684, data_loss: 0.8684\n",
      "step 1260 , total_loss: 0.9779, data_loss: 0.9779\n",
      "step 1280 , total_loss: 0.9489, data_loss: 0.9489\n",
      "step 1300 , total_loss: 0.9440, data_loss: 0.9440\n",
      "step 1320 , total_loss: 0.9329, data_loss: 0.9329\n",
      "step 1340 , total_loss: 0.9696, data_loss: 0.9696\n",
      "step 1360 , total_loss: 0.9556, data_loss: 0.9556\n",
      "step 1380 , total_loss: 1.0176, data_loss: 1.0176\n",
      "step 1400 , total_loss: 0.9621, data_loss: 0.9621\n",
      "step 1420 , total_loss: 0.9765, data_loss: 0.9765\n",
      "step 1440 , total_loss: 0.9672, data_loss: 0.9672\n",
      "step 1460 , total_loss: 0.9578, data_loss: 0.9578\n",
      "step 1480 , total_loss: 0.9189, data_loss: 0.9189\n",
      "step 1500 , total_loss: 1.0165, data_loss: 1.0165\n",
      "step 1520 , total_loss: 0.9657, data_loss: 0.9657\n",
      "step 1540 , total_loss: 1.0220, data_loss: 1.0220\n",
      "step 1560 , total_loss: 0.9316, data_loss: 0.9316\n",
      "step 1580 , total_loss: 1.0213, data_loss: 1.0213\n",
      "step 1600 , total_loss: 0.9956, data_loss: 0.9956\n",
      "step 1620 , total_loss: 0.9460, data_loss: 0.9460\n",
      "step 1640 , total_loss: 0.9629, data_loss: 0.9629\n",
      "step 1660 , total_loss: 0.9972, data_loss: 0.9972\n",
      "step 1680 , total_loss: 1.0100, data_loss: 1.0100\n",
      "step 1700 , total_loss: 0.9170, data_loss: 0.9170\n",
      "step 1720 , total_loss: 1.0229, data_loss: 1.0229\n",
      "step 1740 , total_loss: 1.0177, data_loss: 1.0177\n",
      "step 1760 , total_loss: 0.9328, data_loss: 0.9328\n",
      "step 1780 , total_loss: 0.9483, data_loss: 0.9483\n",
      "step 1800 , total_loss: 0.9671, data_loss: 0.9671\n",
      "step 1820 , total_loss: 0.9950, data_loss: 0.9950\n",
      "step 1840 , total_loss: 0.9523, data_loss: 0.9523\n",
      "step 1860 , total_loss: 0.9186, data_loss: 0.9186\n",
      "step 1880 , total_loss: 0.9128, data_loss: 0.9128\n",
      "step 1900 , total_loss: 0.9857, data_loss: 0.9857\n",
      "step 1920 , total_loss: 0.9606, data_loss: 0.9606\n",
      "step 1940 , total_loss: 0.9683, data_loss: 0.9683\n",
      "step 1960 , total_loss: 1.0454, data_loss: 1.0454\n",
      "step 1980 , total_loss: 0.9183, data_loss: 0.9183\n",
      "step 2000 , total_loss: 0.9403, data_loss: 0.9403\n",
      "step 2020 , total_loss: 0.9600, data_loss: 0.9600\n",
      "step 2040 , total_loss: 1.0128, data_loss: 1.0128\n",
      "step 2060 , total_loss: 1.0625, data_loss: 1.0625\n",
      "step 2080 , total_loss: 0.9870, data_loss: 0.9870\n",
      "step 2100 , total_loss: 1.0555, data_loss: 1.0555\n",
      "step 2120 , total_loss: 0.9784, data_loss: 0.9784\n",
      "step 2140 , total_loss: 0.9275, data_loss: 0.9275\n",
      "step 2160 , total_loss: 0.9383, data_loss: 0.9383\n",
      "step 2180 , total_loss: 1.0181, data_loss: 1.0181\n",
      "step 2200 , total_loss: 0.9447, data_loss: 0.9447\n",
      "step 2220 , total_loss: 0.8905, data_loss: 0.8905\n",
      "step 2240 , total_loss: 0.9166, data_loss: 0.9166\n",
      "step 2260 , total_loss: 0.9781, data_loss: 0.9781\n",
      "step 2280 , total_loss: 0.9082, data_loss: 0.9082\n",
      "step 2300 , total_loss: 1.0188, data_loss: 1.0188\n",
      "step 2320 , total_loss: 1.0017, data_loss: 1.0017\n",
      "step 2340 , total_loss: 1.0343, data_loss: 1.0343\n",
      "step 2360 , total_loss: 0.9777, data_loss: 0.9777\n",
      "step 2380 , total_loss: 1.0050, data_loss: 1.0050\n",
      "step 2400 , total_loss: 0.9920, data_loss: 0.9920\n",
      "step 2420 , total_loss: 0.9991, data_loss: 0.9991\n",
      "step 2440 , total_loss: 0.9504, data_loss: 0.9504\n",
      "step 2460 , total_loss: 0.8753, data_loss: 0.8753\n",
      "step 2480 , total_loss: 0.9960, data_loss: 0.9960\n",
      "step 2500 , total_loss: 0.9174, data_loss: 0.9174\n",
      "step 2520 , total_loss: 1.0033, data_loss: 1.0033\n",
      "step 2540 , total_loss: 0.9928, data_loss: 0.9928\n",
      "step 2560 , total_loss: 0.9754, data_loss: 0.9754\n",
      "step 2580 , total_loss: 0.9689, data_loss: 0.9689\n",
      "step 2600 , total_loss: 0.9430, data_loss: 0.9430\n",
      "step 2620 , total_loss: 0.9997, data_loss: 0.9997\n",
      "step 2640 , total_loss: 1.0025, data_loss: 1.0025\n",
      "step 2660 , total_loss: 1.0678, data_loss: 1.0678\n",
      "step 2680 , total_loss: 1.1112, data_loss: 1.1112\n",
      "step 2700 , total_loss: 0.9272, data_loss: 0.9272\n",
      "step 2720 , total_loss: 0.9553, data_loss: 0.9553\n",
      "step 2740 , total_loss: 1.0164, data_loss: 1.0164\n",
      "step 2760 , total_loss: 0.9341, data_loss: 0.9341\n",
      "step 2780 , total_loss: 1.0545, data_loss: 1.0545\n",
      "step 2800 , total_loss: 0.9492, data_loss: 0.9492\n",
      "step 2820 , total_loss: 1.0081, data_loss: 1.0081\n",
      "step 2840 , total_loss: 0.9746, data_loss: 0.9746\n",
      "step 2860 , total_loss: 1.0177, data_loss: 1.0177\n",
      "step 2880 , total_loss: 0.9337, data_loss: 0.9337\n",
      "step 2900 , total_loss: 0.8951, data_loss: 0.8951\n",
      "step 2920 , total_loss: 0.9057, data_loss: 0.9057\n",
      "step 2940 , total_loss: 0.9576, data_loss: 0.9576\n",
      "step 2960 , total_loss: 0.9433, data_loss: 0.9433\n",
      "step 2980 , total_loss: 1.0502, data_loss: 1.0502\n",
      "step 3000 , total_loss: 0.9381, data_loss: 0.9381\n",
      "step 3020 , total_loss: 0.9915, data_loss: 0.9915\n",
      "step 3040 , total_loss: 1.0369, data_loss: 1.0369\n",
      "step 3060 , total_loss: 0.9686, data_loss: 0.9686\n",
      "step 3080 , total_loss: 0.9646, data_loss: 0.9646\n",
      "step 3100 , total_loss: 1.0089, data_loss: 1.0089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3120 , total_loss: 0.9828, data_loss: 0.9828\n",
      "step 3140 , total_loss: 0.9495, data_loss: 0.9495\n",
      "step 3160 , total_loss: 0.9434, data_loss: 0.9434\n",
      "step 3180 , total_loss: 1.0014, data_loss: 1.0014\n",
      "step 3200 , total_loss: 0.9909, data_loss: 0.9909\n",
      "step 3220 , total_loss: 1.0077, data_loss: 1.0077\n",
      "step 3240 , total_loss: 0.9226, data_loss: 0.9226\n",
      "step 3260 , total_loss: 0.9461, data_loss: 0.9461\n",
      "step 3280 , total_loss: 0.9963, data_loss: 0.9963\n",
      "step 3300 , total_loss: 0.8964, data_loss: 0.8964\n",
      "eval valid at epoch 6: auc:0.81,logloss:1.3324,mean_mrr:0.7539,ndcg@2:0.7232,ndcg@4:0.8026,ndcg@6:0.8158,group_auc:0.8159\n",
      "step 20 , total_loss: 0.8800, data_loss: 0.8800\n",
      "step 40 , total_loss: 0.8629, data_loss: 0.8629\n",
      "step 60 , total_loss: 0.9532, data_loss: 0.9532\n",
      "step 80 , total_loss: 0.9174, data_loss: 0.9174\n",
      "step 100 , total_loss: 0.9426, data_loss: 0.9426\n",
      "step 120 , total_loss: 0.9869, data_loss: 0.9869\n",
      "step 140 , total_loss: 0.8789, data_loss: 0.8789\n",
      "step 160 , total_loss: 0.8546, data_loss: 0.8546\n",
      "step 180 , total_loss: 0.9072, data_loss: 0.9072\n",
      "step 200 , total_loss: 0.9653, data_loss: 0.9653\n",
      "step 220 , total_loss: 0.9719, data_loss: 0.9719\n",
      "step 240 , total_loss: 0.9732, data_loss: 0.9732\n",
      "step 260 , total_loss: 0.8424, data_loss: 0.8424\n",
      "step 280 , total_loss: 0.8996, data_loss: 0.8996\n",
      "step 300 , total_loss: 0.9309, data_loss: 0.9309\n",
      "step 320 , total_loss: 0.8668, data_loss: 0.8668\n",
      "step 340 , total_loss: 0.8992, data_loss: 0.8992\n",
      "step 360 , total_loss: 0.8905, data_loss: 0.8905\n",
      "step 380 , total_loss: 0.8602, data_loss: 0.8602\n",
      "step 400 , total_loss: 0.9191, data_loss: 0.9191\n",
      "step 420 , total_loss: 0.9674, data_loss: 0.9674\n",
      "step 440 , total_loss: 1.0243, data_loss: 1.0243\n",
      "step 460 , total_loss: 0.8565, data_loss: 0.8565\n",
      "step 480 , total_loss: 0.9342, data_loss: 0.9342\n",
      "step 500 , total_loss: 0.9846, data_loss: 0.9846\n",
      "step 520 , total_loss: 0.9048, data_loss: 0.9048\n",
      "step 540 , total_loss: 0.9407, data_loss: 0.9407\n",
      "step 560 , total_loss: 0.8956, data_loss: 0.8956\n",
      "step 580 , total_loss: 0.9305, data_loss: 0.9305\n",
      "step 600 , total_loss: 0.8725, data_loss: 0.8725\n",
      "step 620 , total_loss: 0.9437, data_loss: 0.9437\n",
      "step 640 , total_loss: 1.0362, data_loss: 1.0362\n",
      "step 660 , total_loss: 0.8724, data_loss: 0.8724\n",
      "step 680 , total_loss: 0.9926, data_loss: 0.9926\n",
      "step 700 , total_loss: 0.9091, data_loss: 0.9091\n",
      "step 720 , total_loss: 0.9422, data_loss: 0.9422\n",
      "step 740 , total_loss: 0.8747, data_loss: 0.8747\n",
      "step 760 , total_loss: 0.9070, data_loss: 0.9070\n",
      "step 780 , total_loss: 0.9262, data_loss: 0.9262\n",
      "step 800 , total_loss: 0.9030, data_loss: 0.9030\n",
      "step 820 , total_loss: 1.0001, data_loss: 1.0001\n",
      "step 840 , total_loss: 0.9195, data_loss: 0.9195\n",
      "step 860 , total_loss: 0.8774, data_loss: 0.8774\n",
      "step 880 , total_loss: 0.9420, data_loss: 0.9420\n",
      "step 900 , total_loss: 0.8628, data_loss: 0.8628\n",
      "step 920 , total_loss: 0.9235, data_loss: 0.9235\n",
      "step 940 , total_loss: 0.9862, data_loss: 0.9862\n",
      "step 960 , total_loss: 0.9753, data_loss: 0.9753\n",
      "step 980 , total_loss: 0.9360, data_loss: 0.9360\n",
      "step 1000 , total_loss: 0.9400, data_loss: 0.9400\n",
      "step 1020 , total_loss: 1.0695, data_loss: 1.0695\n",
      "step 1040 , total_loss: 0.9009, data_loss: 0.9009\n",
      "step 1060 , total_loss: 0.9633, data_loss: 0.9633\n",
      "step 1080 , total_loss: 0.9141, data_loss: 0.9141\n",
      "step 1100 , total_loss: 0.8666, data_loss: 0.8666\n",
      "step 1120 , total_loss: 0.8926, data_loss: 0.8926\n",
      "step 1140 , total_loss: 0.9108, data_loss: 0.9108\n",
      "step 1160 , total_loss: 0.9111, data_loss: 0.9111\n",
      "step 1180 , total_loss: 0.9302, data_loss: 0.9302\n",
      "step 1200 , total_loss: 1.0712, data_loss: 1.0712\n",
      "step 1220 , total_loss: 0.9390, data_loss: 0.9390\n",
      "step 1240 , total_loss: 1.0188, data_loss: 1.0188\n",
      "step 1260 , total_loss: 1.0334, data_loss: 1.0334\n",
      "step 1280 , total_loss: 0.9618, data_loss: 0.9618\n",
      "step 1300 , total_loss: 0.9315, data_loss: 0.9315\n",
      "step 1320 , total_loss: 0.9169, data_loss: 0.9169\n",
      "step 1340 , total_loss: 0.9665, data_loss: 0.9665\n",
      "step 1360 , total_loss: 0.9610, data_loss: 0.9610\n",
      "step 1380 , total_loss: 0.9422, data_loss: 0.9422\n",
      "step 1400 , total_loss: 0.8989, data_loss: 0.8989\n",
      "step 1420 , total_loss: 0.9054, data_loss: 0.9054\n",
      "step 1440 , total_loss: 0.9602, data_loss: 0.9602\n",
      "step 1460 , total_loss: 0.8570, data_loss: 0.8570\n",
      "step 1480 , total_loss: 0.8774, data_loss: 0.8774\n",
      "step 1500 , total_loss: 0.9835, data_loss: 0.9835\n",
      "step 1520 , total_loss: 0.8656, data_loss: 0.8656\n",
      "step 1540 , total_loss: 0.9635, data_loss: 0.9635\n",
      "step 1560 , total_loss: 1.0437, data_loss: 1.0437\n",
      "step 1580 , total_loss: 0.9234, data_loss: 0.9234\n",
      "step 1600 , total_loss: 0.9566, data_loss: 0.9566\n",
      "step 1620 , total_loss: 1.0107, data_loss: 1.0107\n",
      "step 1640 , total_loss: 0.9972, data_loss: 0.9972\n",
      "step 1660 , total_loss: 1.0047, data_loss: 1.0047\n",
      "step 1680 , total_loss: 0.9500, data_loss: 0.9500\n",
      "step 1700 , total_loss: 0.9897, data_loss: 0.9897\n",
      "step 1720 , total_loss: 0.9505, data_loss: 0.9505\n",
      "step 1740 , total_loss: 0.9508, data_loss: 0.9508\n",
      "step 1760 , total_loss: 0.9555, data_loss: 0.9555\n",
      "step 1780 , total_loss: 0.9281, data_loss: 0.9281\n",
      "step 1800 , total_loss: 0.9005, data_loss: 0.9005\n",
      "step 1820 , total_loss: 0.9983, data_loss: 0.9983\n",
      "step 1840 , total_loss: 0.9535, data_loss: 0.9535\n",
      "step 1860 , total_loss: 0.9074, data_loss: 0.9074\n",
      "step 1880 , total_loss: 0.9113, data_loss: 0.9113\n",
      "step 1900 , total_loss: 0.9115, data_loss: 0.9115\n",
      "step 1920 , total_loss: 0.9833, data_loss: 0.9833\n",
      "step 1940 , total_loss: 0.9754, data_loss: 0.9754\n",
      "step 1960 , total_loss: 0.8421, data_loss: 0.8421\n",
      "step 1980 , total_loss: 1.0382, data_loss: 1.0382\n",
      "step 2000 , total_loss: 0.8976, data_loss: 0.8976\n",
      "step 2020 , total_loss: 0.9144, data_loss: 0.9144\n",
      "step 2040 , total_loss: 0.9515, data_loss: 0.9515\n",
      "step 2060 , total_loss: 0.9568, data_loss: 0.9568\n",
      "step 2080 , total_loss: 0.9320, data_loss: 0.9320\n",
      "step 2100 , total_loss: 0.9946, data_loss: 0.9946\n",
      "step 2120 , total_loss: 0.9422, data_loss: 0.9422\n",
      "step 2140 , total_loss: 1.0273, data_loss: 1.0273\n",
      "step 2160 , total_loss: 0.9039, data_loss: 0.9039\n",
      "step 2180 , total_loss: 0.9777, data_loss: 0.9777\n",
      "step 2200 , total_loss: 1.0545, data_loss: 1.0545\n",
      "step 2220 , total_loss: 0.9764, data_loss: 0.9764\n",
      "step 2240 , total_loss: 0.9520, data_loss: 0.9520\n",
      "step 2260 , total_loss: 0.9888, data_loss: 0.9888\n",
      "step 2280 , total_loss: 0.9623, data_loss: 0.9623\n",
      "step 2300 , total_loss: 0.9254, data_loss: 0.9254\n",
      "step 2320 , total_loss: 0.8753, data_loss: 0.8753\n",
      "step 2340 , total_loss: 0.9587, data_loss: 0.9587\n",
      "step 2360 , total_loss: 0.9183, data_loss: 0.9183\n",
      "step 2380 , total_loss: 0.8886, data_loss: 0.8886\n",
      "step 2400 , total_loss: 0.8683, data_loss: 0.8683\n",
      "step 2420 , total_loss: 1.0146, data_loss: 1.0146\n",
      "step 2440 , total_loss: 1.0637, data_loss: 1.0637\n",
      "step 2460 , total_loss: 0.8408, data_loss: 0.8408\n",
      "step 2480 , total_loss: 1.0105, data_loss: 1.0105\n",
      "step 2500 , total_loss: 0.9495, data_loss: 0.9495\n",
      "step 2520 , total_loss: 0.9230, data_loss: 0.9230\n",
      "step 2540 , total_loss: 0.9701, data_loss: 0.9701\n",
      "step 2560 , total_loss: 0.9363, data_loss: 0.9363\n",
      "step 2580 , total_loss: 0.9814, data_loss: 0.9814\n",
      "step 2600 , total_loss: 0.9110, data_loss: 0.9110\n",
      "step 2620 , total_loss: 0.9546, data_loss: 0.9546\n",
      "step 2640 , total_loss: 0.9187, data_loss: 0.9187\n",
      "step 2660 , total_loss: 0.9331, data_loss: 0.9331\n",
      "step 2680 , total_loss: 0.9407, data_loss: 0.9407\n",
      "step 2700 , total_loss: 1.0056, data_loss: 1.0056\n",
      "step 2720 , total_loss: 0.9050, data_loss: 0.9050\n",
      "step 2740 , total_loss: 1.0315, data_loss: 1.0315\n",
      "step 2760 , total_loss: 1.0109, data_loss: 1.0109\n",
      "step 2780 , total_loss: 1.0034, data_loss: 1.0034\n",
      "step 2800 , total_loss: 1.0069, data_loss: 1.0069\n",
      "step 2820 , total_loss: 0.9858, data_loss: 0.9858\n",
      "step 2840 , total_loss: 0.8921, data_loss: 0.8921\n",
      "step 2860 , total_loss: 0.9652, data_loss: 0.9652\n",
      "step 2880 , total_loss: 0.9500, data_loss: 0.9500\n",
      "step 2900 , total_loss: 0.8842, data_loss: 0.8842\n",
      "step 2920 , total_loss: 0.9581, data_loss: 0.9581\n",
      "step 2940 , total_loss: 0.8832, data_loss: 0.8832\n",
      "step 2960 , total_loss: 0.9164, data_loss: 0.9164\n",
      "step 2980 , total_loss: 0.9624, data_loss: 0.9624\n",
      "step 3000 , total_loss: 0.9335, data_loss: 0.9335\n",
      "step 3020 , total_loss: 0.8984, data_loss: 0.8984\n",
      "step 3040 , total_loss: 0.9205, data_loss: 0.9205\n",
      "step 3060 , total_loss: 0.9903, data_loss: 0.9903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3080 , total_loss: 0.9671, data_loss: 0.9671\n",
      "step 3100 , total_loss: 1.0599, data_loss: 1.0599\n",
      "step 3120 , total_loss: 1.0481, data_loss: 1.0481\n",
      "step 3140 , total_loss: 0.9371, data_loss: 0.9371\n",
      "step 3160 , total_loss: 1.0204, data_loss: 1.0204\n",
      "step 3180 , total_loss: 0.9472, data_loss: 0.9472\n",
      "step 3200 , total_loss: 0.9728, data_loss: 0.9728\n",
      "step 3220 , total_loss: 0.9194, data_loss: 0.9194\n",
      "step 3240 , total_loss: 0.9749, data_loss: 0.9749\n",
      "step 3260 , total_loss: 0.9861, data_loss: 0.9861\n",
      "step 3280 , total_loss: 0.9691, data_loss: 0.9691\n",
      "step 3300 , total_loss: 0.9799, data_loss: 0.9799\n",
      "eval valid at epoch 7: auc:0.8089,logloss:1.4253,mean_mrr:0.7545,ndcg@2:0.7234,ndcg@4:0.8027,ndcg@6:0.8162,group_auc:0.8155\n",
      "step 20 , total_loss: 0.8501, data_loss: 0.8501\n",
      "step 40 , total_loss: 0.8616, data_loss: 0.8616\n",
      "step 60 , total_loss: 0.8679, data_loss: 0.8679\n",
      "step 80 , total_loss: 0.8696, data_loss: 0.8696\n",
      "step 100 , total_loss: 0.8833, data_loss: 0.8833\n",
      "step 120 , total_loss: 0.9562, data_loss: 0.9562\n",
      "step 140 , total_loss: 0.8814, data_loss: 0.8814\n",
      "step 160 , total_loss: 0.8689, data_loss: 0.8689\n",
      "step 180 , total_loss: 0.8647, data_loss: 0.8647\n",
      "step 200 , total_loss: 0.8873, data_loss: 0.8873\n",
      "step 220 , total_loss: 0.8709, data_loss: 0.8709\n",
      "step 240 , total_loss: 0.9470, data_loss: 0.9470\n",
      "step 260 , total_loss: 0.8606, data_loss: 0.8606\n",
      "step 280 , total_loss: 0.8257, data_loss: 0.8257\n",
      "step 300 , total_loss: 0.9628, data_loss: 0.9628\n",
      "step 320 , total_loss: 0.9437, data_loss: 0.9437\n",
      "step 340 , total_loss: 0.8438, data_loss: 0.8438\n",
      "step 360 , total_loss: 0.8737, data_loss: 0.8737\n",
      "step 380 , total_loss: 0.9782, data_loss: 0.9782\n",
      "step 400 , total_loss: 0.9512, data_loss: 0.9512\n",
      "step 420 , total_loss: 0.9799, data_loss: 0.9799\n",
      "step 440 , total_loss: 0.9483, data_loss: 0.9483\n",
      "step 460 , total_loss: 0.7894, data_loss: 0.7894\n",
      "step 480 , total_loss: 0.8292, data_loss: 0.8292\n",
      "step 500 , total_loss: 0.9106, data_loss: 0.9106\n",
      "step 520 , total_loss: 0.8628, data_loss: 0.8628\n",
      "step 540 , total_loss: 1.0075, data_loss: 1.0075\n",
      "step 560 , total_loss: 0.9179, data_loss: 0.9179\n",
      "step 580 , total_loss: 0.8587, data_loss: 0.8587\n",
      "step 600 , total_loss: 0.8928, data_loss: 0.8928\n",
      "step 620 , total_loss: 0.8018, data_loss: 0.8018\n",
      "step 640 , total_loss: 0.9502, data_loss: 0.9502\n",
      "step 660 , total_loss: 0.8666, data_loss: 0.8666\n",
      "step 680 , total_loss: 0.8966, data_loss: 0.8966\n",
      "step 700 , total_loss: 0.9046, data_loss: 0.9046\n",
      "step 720 , total_loss: 0.8940, data_loss: 0.8940\n",
      "step 740 , total_loss: 0.9110, data_loss: 0.9110\n",
      "step 760 , total_loss: 0.8913, data_loss: 0.8913\n",
      "step 780 , total_loss: 0.8765, data_loss: 0.8765\n",
      "step 800 , total_loss: 0.9033, data_loss: 0.9033\n",
      "step 820 , total_loss: 0.9160, data_loss: 0.9160\n",
      "step 840 , total_loss: 0.9907, data_loss: 0.9907\n",
      "step 860 , total_loss: 0.9764, data_loss: 0.9764\n",
      "step 880 , total_loss: 0.9181, data_loss: 0.9181\n",
      "step 900 , total_loss: 0.8944, data_loss: 0.8944\n",
      "step 920 , total_loss: 0.9406, data_loss: 0.9406\n",
      "step 940 , total_loss: 0.9650, data_loss: 0.9650\n",
      "step 960 , total_loss: 0.9374, data_loss: 0.9374\n",
      "step 980 , total_loss: 0.8880, data_loss: 0.8880\n",
      "step 1000 , total_loss: 0.9396, data_loss: 0.9396\n",
      "step 1020 , total_loss: 0.8401, data_loss: 0.8401\n",
      "step 1040 , total_loss: 0.9467, data_loss: 0.9467\n",
      "step 1060 , total_loss: 0.9761, data_loss: 0.9761\n",
      "step 1080 , total_loss: 0.9016, data_loss: 0.9016\n",
      "step 1100 , total_loss: 0.8750, data_loss: 0.8750\n",
      "step 1120 , total_loss: 0.9674, data_loss: 0.9674\n",
      "step 1140 , total_loss: 0.8914, data_loss: 0.8914\n",
      "step 1160 , total_loss: 0.8938, data_loss: 0.8938\n",
      "step 1180 , total_loss: 0.9095, data_loss: 0.9095\n",
      "step 1200 , total_loss: 0.9648, data_loss: 0.9648\n",
      "step 1220 , total_loss: 0.9104, data_loss: 0.9104\n",
      "step 1240 , total_loss: 0.8720, data_loss: 0.8720\n",
      "step 1260 , total_loss: 0.9725, data_loss: 0.9725\n",
      "step 1280 , total_loss: 0.9202, data_loss: 0.9202\n",
      "step 1300 , total_loss: 0.8620, data_loss: 0.8620\n",
      "step 1320 , total_loss: 0.8979, data_loss: 0.8979\n",
      "step 1340 , total_loss: 0.9636, data_loss: 0.9636\n",
      "step 1360 , total_loss: 0.9912, data_loss: 0.9912\n",
      "step 1380 , total_loss: 0.8949, data_loss: 0.8949\n",
      "step 1400 , total_loss: 0.9143, data_loss: 0.9143\n",
      "step 1420 , total_loss: 0.9765, data_loss: 0.9765\n",
      "step 1440 , total_loss: 0.9408, data_loss: 0.9408\n",
      "step 1460 , total_loss: 1.0678, data_loss: 1.0678\n",
      "step 1480 , total_loss: 0.9775, data_loss: 0.9775\n",
      "step 1500 , total_loss: 0.8542, data_loss: 0.8542\n",
      "step 1520 , total_loss: 0.9232, data_loss: 0.9232\n",
      "step 1540 , total_loss: 0.9813, data_loss: 0.9813\n",
      "step 1560 , total_loss: 0.9868, data_loss: 0.9868\n",
      "step 1580 , total_loss: 1.0525, data_loss: 1.0525\n",
      "step 1600 , total_loss: 0.9347, data_loss: 0.9347\n",
      "step 1620 , total_loss: 0.9453, data_loss: 0.9453\n",
      "step 1640 , total_loss: 0.9506, data_loss: 0.9506\n",
      "step 1660 , total_loss: 0.9386, data_loss: 0.9386\n",
      "step 1680 , total_loss: 0.8956, data_loss: 0.8956\n",
      "step 1700 , total_loss: 0.8268, data_loss: 0.8268\n",
      "step 1720 , total_loss: 1.0012, data_loss: 1.0012\n",
      "step 1740 , total_loss: 0.8937, data_loss: 0.8937\n",
      "step 1760 , total_loss: 0.9144, data_loss: 0.9144\n",
      "step 1780 , total_loss: 0.9562, data_loss: 0.9562\n",
      "step 1800 , total_loss: 0.9644, data_loss: 0.9644\n",
      "step 1820 , total_loss: 0.9319, data_loss: 0.9319\n",
      "step 1840 , total_loss: 0.8908, data_loss: 0.8908\n",
      "step 1860 , total_loss: 0.9082, data_loss: 0.9082\n",
      "step 1880 , total_loss: 0.8321, data_loss: 0.8321\n",
      "step 1900 , total_loss: 0.8499, data_loss: 0.8499\n",
      "step 1920 , total_loss: 0.9187, data_loss: 0.9187\n",
      "step 1940 , total_loss: 0.9458, data_loss: 0.9458\n",
      "step 1960 , total_loss: 0.9799, data_loss: 0.9799\n",
      "step 1980 , total_loss: 0.8898, data_loss: 0.8898\n",
      "step 2000 , total_loss: 1.0846, data_loss: 1.0846\n",
      "step 2020 , total_loss: 0.9047, data_loss: 0.9047\n",
      "step 2040 , total_loss: 0.8924, data_loss: 0.8924\n",
      "step 2060 , total_loss: 0.9082, data_loss: 0.9082\n",
      "step 2080 , total_loss: 0.9753, data_loss: 0.9753\n",
      "step 2100 , total_loss: 0.9175, data_loss: 0.9175\n",
      "step 2120 , total_loss: 0.9137, data_loss: 0.9137\n",
      "step 2140 , total_loss: 0.9288, data_loss: 0.9288\n",
      "step 2160 , total_loss: 0.8414, data_loss: 0.8414\n",
      "step 2180 , total_loss: 0.9127, data_loss: 0.9127\n",
      "step 2200 , total_loss: 0.8460, data_loss: 0.8460\n",
      "step 2220 , total_loss: 0.9062, data_loss: 0.9062\n",
      "step 2240 , total_loss: 0.8762, data_loss: 0.8762\n",
      "step 2260 , total_loss: 0.9345, data_loss: 0.9345\n",
      "step 2280 , total_loss: 0.9595, data_loss: 0.9595\n",
      "step 2300 , total_loss: 0.8899, data_loss: 0.8899\n",
      "step 2320 , total_loss: 1.0233, data_loss: 1.0233\n",
      "step 2340 , total_loss: 0.9267, data_loss: 0.9267\n",
      "step 2360 , total_loss: 0.8353, data_loss: 0.8353\n",
      "step 2380 , total_loss: 0.9278, data_loss: 0.9278\n",
      "step 2400 , total_loss: 0.9432, data_loss: 0.9432\n",
      "step 2420 , total_loss: 0.9330, data_loss: 0.9330\n",
      "step 2440 , total_loss: 0.9593, data_loss: 0.9593\n",
      "step 2460 , total_loss: 1.0209, data_loss: 1.0209\n",
      "step 2480 , total_loss: 0.8655, data_loss: 0.8655\n",
      "step 2500 , total_loss: 0.9519, data_loss: 0.9519\n",
      "step 2520 , total_loss: 0.9908, data_loss: 0.9908\n",
      "step 2540 , total_loss: 0.9343, data_loss: 0.9343\n",
      "step 2560 , total_loss: 0.9456, data_loss: 0.9456\n",
      "step 2580 , total_loss: 0.8838, data_loss: 0.8838\n",
      "step 2600 , total_loss: 0.9565, data_loss: 0.9565\n",
      "step 2620 , total_loss: 1.0153, data_loss: 1.0153\n",
      "step 2640 , total_loss: 0.9415, data_loss: 0.9415\n",
      "step 2660 , total_loss: 0.9469, data_loss: 0.9469\n",
      "step 2680 , total_loss: 0.8647, data_loss: 0.8647\n",
      "step 2700 , total_loss: 0.9093, data_loss: 0.9093\n",
      "step 2720 , total_loss: 0.8271, data_loss: 0.8271\n",
      "step 2740 , total_loss: 0.9121, data_loss: 0.9121\n",
      "step 2760 , total_loss: 0.9209, data_loss: 0.9209\n",
      "step 2780 , total_loss: 0.9480, data_loss: 0.9480\n",
      "step 2800 , total_loss: 0.9191, data_loss: 0.9191\n",
      "step 2820 , total_loss: 0.8854, data_loss: 0.8854\n",
      "step 2840 , total_loss: 0.9293, data_loss: 0.9293\n",
      "step 2860 , total_loss: 0.9704, data_loss: 0.9704\n",
      "step 2880 , total_loss: 0.8693, data_loss: 0.8693\n",
      "step 2900 , total_loss: 0.9374, data_loss: 0.9374\n",
      "step 2920 , total_loss: 0.9419, data_loss: 0.9419\n",
      "step 2940 , total_loss: 0.9501, data_loss: 0.9501\n",
      "step 2960 , total_loss: 0.8928, data_loss: 0.8928\n",
      "step 2980 , total_loss: 0.8778, data_loss: 0.8778\n",
      "step 3000 , total_loss: 0.9894, data_loss: 0.9894\n",
      "step 3020 , total_loss: 0.8939, data_loss: 0.8939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3040 , total_loss: 0.9578, data_loss: 0.9578\n",
      "step 3060 , total_loss: 0.9595, data_loss: 0.9595\n",
      "step 3080 , total_loss: 0.9307, data_loss: 0.9307\n",
      "step 3100 , total_loss: 0.8914, data_loss: 0.8914\n",
      "step 3120 , total_loss: 0.9273, data_loss: 0.9273\n",
      "step 3140 , total_loss: 0.9879, data_loss: 0.9879\n",
      "step 3160 , total_loss: 0.9552, data_loss: 0.9552\n",
      "step 3180 , total_loss: 1.0188, data_loss: 1.0188\n",
      "step 3200 , total_loss: 0.9291, data_loss: 0.9291\n",
      "step 3220 , total_loss: 0.9185, data_loss: 0.9185\n",
      "step 3240 , total_loss: 0.9704, data_loss: 0.9704\n",
      "step 3260 , total_loss: 0.8537, data_loss: 0.8537\n",
      "step 3280 , total_loss: 0.9777, data_loss: 0.9777\n",
      "step 3300 , total_loss: 0.9842, data_loss: 0.9842\n",
      "eval valid at epoch 8: auc:0.811,logloss:1.5735,mean_mrr:0.7555,ndcg@2:0.725,ndcg@4:0.8032,ndcg@6:0.8169,group_auc:0.8164\n",
      "step 20 , total_loss: 0.9404, data_loss: 0.9404\n",
      "step 40 , total_loss: 0.8516, data_loss: 0.8516\n",
      "step 60 , total_loss: 0.8964, data_loss: 0.8964\n",
      "step 80 , total_loss: 0.8826, data_loss: 0.8826\n",
      "step 100 , total_loss: 0.9151, data_loss: 0.9151\n",
      "step 120 , total_loss: 0.9001, data_loss: 0.9001\n",
      "step 140 , total_loss: 0.9528, data_loss: 0.9528\n",
      "step 160 , total_loss: 0.8142, data_loss: 0.8142\n",
      "step 180 , total_loss: 0.9483, data_loss: 0.9483\n",
      "step 200 , total_loss: 0.8720, data_loss: 0.8720\n",
      "step 220 , total_loss: 0.9336, data_loss: 0.9336\n",
      "step 240 , total_loss: 0.8796, data_loss: 0.8796\n",
      "step 260 , total_loss: 0.8799, data_loss: 0.8799\n",
      "step 280 , total_loss: 0.8538, data_loss: 0.8538\n",
      "step 300 , total_loss: 0.8638, data_loss: 0.8638\n",
      "step 320 , total_loss: 0.8198, data_loss: 0.8198\n",
      "step 340 , total_loss: 0.9927, data_loss: 0.9927\n",
      "step 360 , total_loss: 0.8503, data_loss: 0.8503\n",
      "step 380 , total_loss: 0.8228, data_loss: 0.8228\n",
      "step 400 , total_loss: 0.8720, data_loss: 0.8720\n",
      "step 420 , total_loss: 0.8416, data_loss: 0.8416\n",
      "step 440 , total_loss: 0.8587, data_loss: 0.8587\n",
      "step 460 , total_loss: 0.9266, data_loss: 0.9266\n",
      "step 480 , total_loss: 0.8795, data_loss: 0.8795\n",
      "step 500 , total_loss: 0.9147, data_loss: 0.9147\n",
      "step 520 , total_loss: 0.9124, data_loss: 0.9124\n",
      "step 540 , total_loss: 0.9478, data_loss: 0.9478\n",
      "step 560 , total_loss: 0.8793, data_loss: 0.8793\n",
      "step 580 , total_loss: 0.8725, data_loss: 0.8725\n",
      "step 600 , total_loss: 0.9700, data_loss: 0.9700\n",
      "step 620 , total_loss: 0.8571, data_loss: 0.8571\n",
      "step 640 , total_loss: 0.9081, data_loss: 0.9081\n",
      "step 660 , total_loss: 0.9175, data_loss: 0.9175\n",
      "step 680 , total_loss: 0.9278, data_loss: 0.9278\n",
      "step 700 , total_loss: 0.8953, data_loss: 0.8953\n",
      "step 720 , total_loss: 0.8702, data_loss: 0.8702\n",
      "step 740 , total_loss: 0.8583, data_loss: 0.8583\n",
      "step 760 , total_loss: 0.8874, data_loss: 0.8874\n",
      "step 780 , total_loss: 0.8991, data_loss: 0.8991\n",
      "step 800 , total_loss: 0.8940, data_loss: 0.8940\n",
      "step 820 , total_loss: 0.8531, data_loss: 0.8531\n",
      "step 840 , total_loss: 0.8159, data_loss: 0.8159\n",
      "step 860 , total_loss: 0.8720, data_loss: 0.8720\n",
      "step 880 , total_loss: 0.8896, data_loss: 0.8896\n",
      "step 900 , total_loss: 0.9237, data_loss: 0.9237\n",
      "step 920 , total_loss: 0.8913, data_loss: 0.8913\n",
      "step 940 , total_loss: 0.8383, data_loss: 0.8383\n",
      "step 960 , total_loss: 0.8793, data_loss: 0.8793\n",
      "step 980 , total_loss: 0.9199, data_loss: 0.9199\n",
      "step 1000 , total_loss: 0.9164, data_loss: 0.9164\n",
      "step 1020 , total_loss: 0.8568, data_loss: 0.8568\n",
      "step 1040 , total_loss: 0.8108, data_loss: 0.8108\n",
      "step 1060 , total_loss: 0.9156, data_loss: 0.9156\n",
      "step 1080 , total_loss: 0.9404, data_loss: 0.9404\n",
      "step 1100 , total_loss: 0.8662, data_loss: 0.8662\n",
      "step 1120 , total_loss: 0.8773, data_loss: 0.8773\n",
      "step 1140 , total_loss: 0.9418, data_loss: 0.9418\n",
      "step 1160 , total_loss: 0.9085, data_loss: 0.9085\n",
      "step 1180 , total_loss: 0.8445, data_loss: 0.8445\n",
      "step 1200 , total_loss: 0.8447, data_loss: 0.8447\n",
      "step 1220 , total_loss: 0.8627, data_loss: 0.8627\n",
      "step 1240 , total_loss: 0.9081, data_loss: 0.9081\n",
      "step 1260 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 1280 , total_loss: 0.9454, data_loss: 0.9454\n",
      "step 1300 , total_loss: 0.9013, data_loss: 0.9013\n",
      "step 1320 , total_loss: 0.8765, data_loss: 0.8765\n",
      "step 1340 , total_loss: 0.8835, data_loss: 0.8835\n",
      "step 1360 , total_loss: 0.9013, data_loss: 0.9013\n",
      "step 1380 , total_loss: 0.8952, data_loss: 0.8952\n",
      "step 1400 , total_loss: 0.9345, data_loss: 0.9345\n",
      "step 1420 , total_loss: 0.9241, data_loss: 0.9241\n",
      "step 1440 , total_loss: 0.9125, data_loss: 0.9125\n",
      "step 1460 , total_loss: 0.8598, data_loss: 0.8598\n",
      "step 1480 , total_loss: 0.8554, data_loss: 0.8554\n",
      "step 1500 , total_loss: 0.9131, data_loss: 0.9131\n",
      "step 1520 , total_loss: 0.9436, data_loss: 0.9436\n",
      "step 1540 , total_loss: 0.9217, data_loss: 0.9217\n",
      "step 1560 , total_loss: 0.8915, data_loss: 0.8915\n",
      "step 1580 , total_loss: 0.9455, data_loss: 0.9455\n",
      "step 1600 , total_loss: 0.9909, data_loss: 0.9909\n",
      "step 1620 , total_loss: 0.8407, data_loss: 0.8407\n",
      "step 1640 , total_loss: 0.9210, data_loss: 0.9210\n",
      "step 1660 , total_loss: 0.8979, data_loss: 0.8979\n",
      "step 1680 , total_loss: 0.9096, data_loss: 0.9096\n",
      "step 1700 , total_loss: 0.9448, data_loss: 0.9448\n",
      "step 1720 , total_loss: 0.8553, data_loss: 0.8553\n",
      "step 1740 , total_loss: 0.8574, data_loss: 0.8574\n",
      "step 1760 , total_loss: 0.9125, data_loss: 0.9125\n",
      "step 1780 , total_loss: 0.9068, data_loss: 0.9068\n",
      "step 1800 , total_loss: 0.9413, data_loss: 0.9413\n",
      "step 1820 , total_loss: 0.8823, data_loss: 0.8823\n",
      "step 1840 , total_loss: 0.9355, data_loss: 0.9355\n",
      "step 1860 , total_loss: 0.9441, data_loss: 0.9441\n",
      "step 1880 , total_loss: 0.8723, data_loss: 0.8723\n",
      "step 1900 , total_loss: 0.8867, data_loss: 0.8867\n",
      "step 1920 , total_loss: 0.9167, data_loss: 0.9167\n",
      "step 1940 , total_loss: 0.9239, data_loss: 0.9239\n",
      "step 1960 , total_loss: 0.9722, data_loss: 0.9722\n",
      "step 1980 , total_loss: 0.9460, data_loss: 0.9460\n",
      "step 2000 , total_loss: 0.9282, data_loss: 0.9282\n",
      "step 2020 , total_loss: 0.9662, data_loss: 0.9662\n",
      "step 2040 , total_loss: 0.8645, data_loss: 0.8645\n",
      "step 2060 , total_loss: 0.9648, data_loss: 0.9648\n",
      "step 2080 , total_loss: 0.9734, data_loss: 0.9734\n",
      "step 2100 , total_loss: 0.9127, data_loss: 0.9127\n",
      "step 2120 , total_loss: 0.8628, data_loss: 0.8628\n",
      "step 2140 , total_loss: 0.9180, data_loss: 0.9180\n",
      "step 2160 , total_loss: 0.8475, data_loss: 0.8475\n",
      "step 2180 , total_loss: 0.8835, data_loss: 0.8835\n",
      "step 2200 , total_loss: 0.9605, data_loss: 0.9605\n",
      "step 2220 , total_loss: 0.9307, data_loss: 0.9307\n",
      "step 2240 , total_loss: 0.9439, data_loss: 0.9439\n",
      "step 2260 , total_loss: 0.9202, data_loss: 0.9202\n",
      "step 2280 , total_loss: 0.9172, data_loss: 0.9172\n",
      "step 2300 , total_loss: 0.7872, data_loss: 0.7872\n",
      "step 2320 , total_loss: 1.0289, data_loss: 1.0289\n",
      "step 2340 , total_loss: 0.9794, data_loss: 0.9794\n",
      "step 2360 , total_loss: 0.8509, data_loss: 0.8509\n",
      "step 2380 , total_loss: 0.9199, data_loss: 0.9199\n",
      "step 2400 , total_loss: 0.8863, data_loss: 0.8863\n",
      "step 2420 , total_loss: 1.0029, data_loss: 1.0029\n",
      "step 2440 , total_loss: 0.9194, data_loss: 0.9194\n",
      "step 2460 , total_loss: 0.8627, data_loss: 0.8627\n",
      "step 2480 , total_loss: 0.8552, data_loss: 0.8552\n",
      "step 2500 , total_loss: 0.9257, data_loss: 0.9257\n",
      "step 2520 , total_loss: 0.8666, data_loss: 0.8666\n",
      "step 2540 , total_loss: 0.9162, data_loss: 0.9162\n",
      "step 2560 , total_loss: 0.8414, data_loss: 0.8414\n",
      "step 2580 , total_loss: 0.8677, data_loss: 0.8677\n",
      "step 2600 , total_loss: 0.8752, data_loss: 0.8752\n",
      "step 2620 , total_loss: 0.8375, data_loss: 0.8375\n",
      "step 2640 , total_loss: 0.8609, data_loss: 0.8609\n",
      "step 2660 , total_loss: 0.9174, data_loss: 0.9174\n",
      "step 2680 , total_loss: 0.9637, data_loss: 0.9637\n",
      "step 2700 , total_loss: 0.9312, data_loss: 0.9312\n",
      "step 2720 , total_loss: 0.8837, data_loss: 0.8837\n",
      "step 2740 , total_loss: 0.8840, data_loss: 0.8840\n",
      "step 2760 , total_loss: 0.9252, data_loss: 0.9252\n",
      "step 2780 , total_loss: 0.9557, data_loss: 0.9557\n",
      "step 2800 , total_loss: 0.8690, data_loss: 0.8690\n",
      "step 2820 , total_loss: 0.9043, data_loss: 0.9043\n",
      "step 2840 , total_loss: 0.8923, data_loss: 0.8923\n",
      "step 2860 , total_loss: 0.9683, data_loss: 0.9683\n",
      "step 2880 , total_loss: 0.9397, data_loss: 0.9397\n",
      "step 2900 , total_loss: 0.9422, data_loss: 0.9422\n",
      "step 2920 , total_loss: 0.9272, data_loss: 0.9272\n",
      "step 2940 , total_loss: 0.9376, data_loss: 0.9376\n",
      "step 2960 , total_loss: 0.9639, data_loss: 0.9639\n",
      "step 2980 , total_loss: 0.9145, data_loss: 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000 , total_loss: 0.9006, data_loss: 0.9006\n",
      "step 3020 , total_loss: 0.9393, data_loss: 0.9393\n",
      "step 3040 , total_loss: 0.9517, data_loss: 0.9517\n",
      "step 3060 , total_loss: 0.9323, data_loss: 0.9323\n",
      "step 3080 , total_loss: 0.8451, data_loss: 0.8451\n",
      "step 3100 , total_loss: 0.9581, data_loss: 0.9581\n",
      "step 3120 , total_loss: 0.8958, data_loss: 0.8958\n",
      "step 3140 , total_loss: 0.9085, data_loss: 0.9085\n",
      "step 3160 , total_loss: 0.9454, data_loss: 0.9454\n",
      "step 3180 , total_loss: 0.9023, data_loss: 0.9023\n",
      "step 3200 , total_loss: 0.9170, data_loss: 0.9170\n",
      "step 3220 , total_loss: 0.8802, data_loss: 0.8802\n",
      "step 3240 , total_loss: 0.9812, data_loss: 0.9812\n",
      "step 3260 , total_loss: 0.9194, data_loss: 0.9194\n",
      "step 3280 , total_loss: 0.9143, data_loss: 0.9143\n",
      "step 3300 , total_loss: 0.9519, data_loss: 0.9519\n",
      "eval valid at epoch 9: auc:0.8157,logloss:1.6467,mean_mrr:0.7598,ndcg@2:0.7306,ndcg@4:0.8073,ndcg@6:0.8202,group_auc:0.8206\n",
      "step 20 , total_loss: 0.7696, data_loss: 0.7696\n",
      "step 40 , total_loss: 0.8752, data_loss: 0.8752\n",
      "step 60 , total_loss: 0.8471, data_loss: 0.8471\n",
      "step 80 , total_loss: 0.8403, data_loss: 0.8403\n",
      "step 100 , total_loss: 0.8190, data_loss: 0.8190\n",
      "step 120 , total_loss: 0.8685, data_loss: 0.8685\n",
      "step 140 , total_loss: 0.8361, data_loss: 0.8361\n",
      "step 160 , total_loss: 0.9188, data_loss: 0.9188\n",
      "step 180 , total_loss: 0.9798, data_loss: 0.9798\n",
      "step 200 , total_loss: 0.8627, data_loss: 0.8627\n",
      "step 220 , total_loss: 0.8736, data_loss: 0.8736\n",
      "step 240 , total_loss: 0.8684, data_loss: 0.8684\n",
      "step 260 , total_loss: 0.8517, data_loss: 0.8517\n",
      "step 280 , total_loss: 0.8646, data_loss: 0.8646\n",
      "step 300 , total_loss: 0.8734, data_loss: 0.8734\n",
      "step 320 , total_loss: 0.9157, data_loss: 0.9157\n",
      "step 340 , total_loss: 0.9116, data_loss: 0.9116\n",
      "step 360 , total_loss: 0.9027, data_loss: 0.9027\n",
      "step 380 , total_loss: 0.8147, data_loss: 0.8147\n",
      "step 400 , total_loss: 0.9471, data_loss: 0.9471\n",
      "step 420 , total_loss: 0.8316, data_loss: 0.8316\n",
      "step 440 , total_loss: 0.8924, data_loss: 0.8924\n",
      "step 460 , total_loss: 0.8959, data_loss: 0.8959\n",
      "step 480 , total_loss: 0.8669, data_loss: 0.8669\n",
      "step 500 , total_loss: 0.9013, data_loss: 0.9013\n",
      "step 520 , total_loss: 0.9354, data_loss: 0.9354\n",
      "step 540 , total_loss: 0.9220, data_loss: 0.9220\n",
      "step 560 , total_loss: 0.8793, data_loss: 0.8793\n",
      "step 580 , total_loss: 0.8789, data_loss: 0.8789\n",
      "step 600 , total_loss: 0.8599, data_loss: 0.8599\n",
      "step 620 , total_loss: 0.8722, data_loss: 0.8722\n",
      "step 640 , total_loss: 0.8665, data_loss: 0.8665\n",
      "step 660 , total_loss: 0.8710, data_loss: 0.8710\n",
      "step 680 , total_loss: 0.9071, data_loss: 0.9071\n",
      "step 700 , total_loss: 0.8210, data_loss: 0.8210\n",
      "step 720 , total_loss: 0.8649, data_loss: 0.8649\n",
      "step 740 , total_loss: 0.8008, data_loss: 0.8008\n",
      "step 760 , total_loss: 0.8879, data_loss: 0.8879\n",
      "step 780 , total_loss: 0.8792, data_loss: 0.8792\n",
      "step 800 , total_loss: 0.8337, data_loss: 0.8337\n",
      "step 820 , total_loss: 0.8807, data_loss: 0.8807\n",
      "step 840 , total_loss: 0.8274, data_loss: 0.8274\n",
      "step 860 , total_loss: 0.9460, data_loss: 0.9460\n",
      "step 880 , total_loss: 0.8880, data_loss: 0.8880\n",
      "step 900 , total_loss: 0.8680, data_loss: 0.8680\n",
      "step 920 , total_loss: 0.8501, data_loss: 0.8501\n",
      "step 940 , total_loss: 0.8678, data_loss: 0.8678\n",
      "step 960 , total_loss: 0.9140, data_loss: 0.9140\n",
      "step 980 , total_loss: 0.8652, data_loss: 0.8652\n",
      "step 1000 , total_loss: 0.8836, data_loss: 0.8836\n",
      "step 1020 , total_loss: 0.9617, data_loss: 0.9617\n",
      "step 1040 , total_loss: 0.8515, data_loss: 0.8515\n",
      "step 1060 , total_loss: 0.8817, data_loss: 0.8817\n",
      "step 1080 , total_loss: 0.9115, data_loss: 0.9115\n",
      "step 1100 , total_loss: 0.9521, data_loss: 0.9521\n",
      "step 1120 , total_loss: 0.9091, data_loss: 0.9091\n",
      "step 1140 , total_loss: 0.8235, data_loss: 0.8235\n",
      "step 1160 , total_loss: 0.9021, data_loss: 0.9021\n",
      "step 1180 , total_loss: 0.9142, data_loss: 0.9142\n",
      "step 1200 , total_loss: 0.8529, data_loss: 0.8529\n",
      "step 1220 , total_loss: 0.9319, data_loss: 0.9319\n",
      "step 1240 , total_loss: 0.8819, data_loss: 0.8819\n",
      "step 1260 , total_loss: 0.7992, data_loss: 0.7992\n",
      "step 1280 , total_loss: 0.9464, data_loss: 0.9464\n",
      "step 1300 , total_loss: 0.9302, data_loss: 0.9302\n",
      "step 1320 , total_loss: 0.8665, data_loss: 0.8665\n",
      "step 1340 , total_loss: 0.8224, data_loss: 0.8224\n",
      "step 1360 , total_loss: 0.8435, data_loss: 0.8435\n",
      "step 1380 , total_loss: 0.8769, data_loss: 0.8769\n",
      "step 1400 , total_loss: 0.9594, data_loss: 0.9594\n",
      "step 1420 , total_loss: 0.9266, data_loss: 0.9266\n",
      "step 1440 , total_loss: 0.9304, data_loss: 0.9304\n",
      "step 1460 , total_loss: 0.9226, data_loss: 0.9226\n",
      "step 1480 , total_loss: 0.8422, data_loss: 0.8422\n",
      "step 1500 , total_loss: 0.8839, data_loss: 0.8839\n",
      "step 1520 , total_loss: 0.9697, data_loss: 0.9697\n",
      "step 1540 , total_loss: 0.8861, data_loss: 0.8861\n",
      "step 1560 , total_loss: 0.9075, data_loss: 0.9075\n",
      "step 1580 , total_loss: 0.9319, data_loss: 0.9319\n",
      "step 1600 , total_loss: 0.8015, data_loss: 0.8015\n",
      "step 1620 , total_loss: 0.9410, data_loss: 0.9410\n",
      "step 1640 , total_loss: 0.8432, data_loss: 0.8432\n",
      "step 1660 , total_loss: 0.9835, data_loss: 0.9835\n",
      "step 1680 , total_loss: 0.8219, data_loss: 0.8219\n",
      "step 1700 , total_loss: 0.8622, data_loss: 0.8622\n",
      "step 1720 , total_loss: 0.8778, data_loss: 0.8778\n",
      "step 1740 , total_loss: 0.9052, data_loss: 0.9052\n",
      "step 1760 , total_loss: 0.9285, data_loss: 0.9285\n",
      "step 1780 , total_loss: 0.8363, data_loss: 0.8363\n",
      "step 1800 , total_loss: 0.8630, data_loss: 0.8630\n",
      "step 1820 , total_loss: 0.9862, data_loss: 0.9862\n",
      "step 1840 , total_loss: 0.9052, data_loss: 0.9052\n",
      "step 1860 , total_loss: 0.8985, data_loss: 0.8985\n",
      "step 1880 , total_loss: 0.8808, data_loss: 0.8808\n",
      "step 1900 , total_loss: 0.8873, data_loss: 0.8873\n",
      "step 1920 , total_loss: 0.9081, data_loss: 0.9081\n",
      "step 1940 , total_loss: 0.8842, data_loss: 0.8842\n",
      "step 1960 , total_loss: 0.8452, data_loss: 0.8452\n",
      "step 1980 , total_loss: 0.9921, data_loss: 0.9921\n",
      "step 2000 , total_loss: 0.8697, data_loss: 0.8697\n",
      "step 2020 , total_loss: 0.8884, data_loss: 0.8884\n",
      "step 2040 , total_loss: 0.9334, data_loss: 0.9334\n",
      "step 2060 , total_loss: 0.9211, data_loss: 0.9211\n",
      "step 2080 , total_loss: 0.8827, data_loss: 0.8827\n",
      "step 2100 , total_loss: 0.9038, data_loss: 0.9038\n",
      "step 2120 , total_loss: 0.8606, data_loss: 0.8606\n",
      "step 2140 , total_loss: 0.8321, data_loss: 0.8321\n",
      "step 2160 , total_loss: 0.8495, data_loss: 0.8495\n",
      "step 2180 , total_loss: 0.8541, data_loss: 0.8541\n",
      "step 2200 , total_loss: 0.8478, data_loss: 0.8478\n",
      "step 2220 , total_loss: 0.9362, data_loss: 0.9362\n",
      "step 2240 , total_loss: 0.8958, data_loss: 0.8958\n",
      "step 2260 , total_loss: 0.9016, data_loss: 0.9016\n",
      "step 2280 , total_loss: 0.9140, data_loss: 0.9140\n",
      "step 2300 , total_loss: 0.8294, data_loss: 0.8294\n",
      "step 2320 , total_loss: 0.9760, data_loss: 0.9760\n",
      "step 2340 , total_loss: 0.9512, data_loss: 0.9512\n",
      "step 2360 , total_loss: 0.9001, data_loss: 0.9001\n",
      "step 2380 , total_loss: 0.9644, data_loss: 0.9644\n",
      "step 2400 , total_loss: 0.9033, data_loss: 0.9033\n",
      "step 2420 , total_loss: 0.8900, data_loss: 0.8900\n",
      "step 2440 , total_loss: 0.8503, data_loss: 0.8503\n",
      "step 2460 , total_loss: 0.8462, data_loss: 0.8462\n",
      "step 2480 , total_loss: 0.9350, data_loss: 0.9350\n",
      "step 2500 , total_loss: 0.9417, data_loss: 0.9417\n",
      "step 2520 , total_loss: 0.8777, data_loss: 0.8777\n",
      "step 2540 , total_loss: 0.8897, data_loss: 0.8897\n",
      "step 2560 , total_loss: 0.9268, data_loss: 0.9268\n",
      "step 2580 , total_loss: 0.8774, data_loss: 0.8774\n",
      "step 2600 , total_loss: 0.8727, data_loss: 0.8727\n",
      "step 2620 , total_loss: 0.9229, data_loss: 0.9229\n",
      "step 2640 , total_loss: 0.9749, data_loss: 0.9749\n",
      "step 2660 , total_loss: 0.8616, data_loss: 0.8616\n",
      "step 2680 , total_loss: 0.8548, data_loss: 0.8548\n",
      "step 2700 , total_loss: 0.9590, data_loss: 0.9590\n",
      "step 2720 , total_loss: 0.8360, data_loss: 0.8360\n",
      "step 2740 , total_loss: 0.9173, data_loss: 0.9173\n",
      "step 2760 , total_loss: 0.8407, data_loss: 0.8407\n",
      "step 2780 , total_loss: 0.8655, data_loss: 0.8655\n",
      "step 2800 , total_loss: 0.8393, data_loss: 0.8393\n",
      "step 2820 , total_loss: 0.9162, data_loss: 0.9162\n",
      "step 2840 , total_loss: 0.9064, data_loss: 0.9064\n",
      "step 2860 , total_loss: 0.9741, data_loss: 0.9741\n",
      "step 2880 , total_loss: 0.9072, data_loss: 0.9072\n",
      "step 2900 , total_loss: 0.9300, data_loss: 0.9300\n",
      "step 2920 , total_loss: 0.8736, data_loss: 0.8736\n",
      "step 2940 , total_loss: 0.8616, data_loss: 0.8616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2960 , total_loss: 0.8998, data_loss: 0.8998\n",
      "step 2980 , total_loss: 0.9157, data_loss: 0.9157\n",
      "step 3000 , total_loss: 0.8758, data_loss: 0.8758\n",
      "step 3020 , total_loss: 0.9353, data_loss: 0.9353\n",
      "step 3040 , total_loss: 0.9480, data_loss: 0.9480\n",
      "step 3060 , total_loss: 0.9077, data_loss: 0.9077\n",
      "step 3080 , total_loss: 0.8816, data_loss: 0.8816\n",
      "step 3100 , total_loss: 0.9274, data_loss: 0.9274\n",
      "step 3120 , total_loss: 0.8664, data_loss: 0.8664\n",
      "step 3140 , total_loss: 0.8699, data_loss: 0.8699\n",
      "step 3160 , total_loss: 0.9956, data_loss: 0.9956\n",
      "step 3180 , total_loss: 0.8816, data_loss: 0.8816\n",
      "step 3200 , total_loss: 0.9471, data_loss: 0.9471\n",
      "step 3220 , total_loss: 0.8502, data_loss: 0.8502\n",
      "step 3240 , total_loss: 0.9317, data_loss: 0.9317\n",
      "step 3260 , total_loss: 0.9315, data_loss: 0.9315\n",
      "step 3280 , total_loss: 0.8923, data_loss: 0.8923\n",
      "step 3300 , total_loss: 0.9480, data_loss: 0.9480\n",
      "eval valid at epoch 10: auc:0.8104,logloss:1.8416,mean_mrr:0.7564,ndcg@2:0.7253,ndcg@4:0.8039,ndcg@6:0.8176,group_auc:0.8165\n",
      "[(1, {'auc': 0.7566, 'logloss': 0.7411, 'mean_mrr': 0.6879, 'ndcg@2': 0.6409, 'ndcg@4': 0.745, 'ndcg@6': 0.766, 'group_auc': 0.7569}), (2, {'auc': 0.7879, 'logloss': 0.7763, 'mean_mrr': 0.7236, 'ndcg@2': 0.6864, 'ndcg@4': 0.7763, 'ndcg@6': 0.7929, 'group_auc': 0.7896}), (3, {'auc': 0.7929, 'logloss': 0.9656, 'mean_mrr': 0.7331, 'ndcg@2': 0.6977, 'ndcg@4': 0.7847, 'ndcg@6': 0.8001, 'group_auc': 0.7976}), (4, {'auc': 0.8, 'logloss': 1.0104, 'mean_mrr': 0.7412, 'ndcg@2': 0.7074, 'ndcg@4': 0.7911, 'ndcg@6': 0.8062, 'group_auc': 0.8038}), (5, {'auc': 0.8044, 'logloss': 1.1837, 'mean_mrr': 0.7457, 'ndcg@2': 0.7134, 'ndcg@4': 0.7958, 'ndcg@6': 0.8096, 'group_auc': 0.809}), (6, {'auc': 0.81, 'logloss': 1.3324, 'mean_mrr': 0.7539, 'ndcg@2': 0.7232, 'ndcg@4': 0.8026, 'ndcg@6': 0.8158, 'group_auc': 0.8159}), (7, {'auc': 0.8089, 'logloss': 1.4253, 'mean_mrr': 0.7545, 'ndcg@2': 0.7234, 'ndcg@4': 0.8027, 'ndcg@6': 0.8162, 'group_auc': 0.8155}), (8, {'auc': 0.811, 'logloss': 1.5735, 'mean_mrr': 0.7555, 'ndcg@2': 0.725, 'ndcg@4': 0.8032, 'ndcg@6': 0.8169, 'group_auc': 0.8164}), (9, {'auc': 0.8157, 'logloss': 1.6467, 'mean_mrr': 0.7598, 'ndcg@2': 0.7306, 'ndcg@4': 0.8073, 'ndcg@6': 0.8202, 'group_auc': 0.8206}), (10, {'auc': 0.8104, 'logloss': 1.8416, 'mean_mrr': 0.7564, 'ndcg@2': 0.7253, 'ndcg@4': 0.8039, 'ndcg@6': 0.8176, 'group_auc': 0.8165})]\n",
      "best epoch: 9\n",
      "Time cost for training is 66.13 mins\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e41391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.7947, 'logloss': 2.0921, 'mean_mrr': 0.6151, 'ndcg@2': 0.5539, 'ndcg@4': 0.6377, 'ndcg@6': 0.6744, 'group_auc': 0.8}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved model in ../../tests/resources/deeprec/slirec/model/din_sum_pooling/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 10:05:09.329131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 10:05:09.329315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 10:05:09.329434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 10:05:09.329581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 10:05:09.329697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-04 10:05:09.329775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9796 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.8058,\n",
       " 'logloss': 1.8566,\n",
       " 'mean_mrr': 0.6262,\n",
       " 'ndcg@2': 0.5672,\n",
       " 'ndcg@4': 0.6511,\n",
       " 'ndcg@6': 0.6862,\n",
       " 'group_auc': 0.8103}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
