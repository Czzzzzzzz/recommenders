{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6852f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "    alphas = tf.get_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/din_op/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/din_op/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "                          attention_mode=\"outer_product\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['relu', 'relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'attention_mod': 'inner_product',\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/din_op/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/din_op/',\n",
       " 'attention_mode': 'outer_product'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6a1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist_input Tensor(\"sequential/din/concat:0\", shape=(None, 50, 40), dtype=float32)\n",
      "boolean_mask Tensor(\"sequential/din/target_attention/Equal:0\", shape=(None, 50), dtype=bool)\n",
      "attention_logits Tensor(\"sequential/din/target_attention/Squeeze:0\", shape=(None, 50), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/base_model.py:701: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-05 14:10:34.222796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:34.228483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:34.228709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.184633: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-05 14:10:35.185616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.185790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.185905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.486876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.487033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.487142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 14:10:35.487234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 886 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-05 14:10:35.501769: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_496722/2719419441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_num_ngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_num_ngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# valid_num_ngs is the number of negative lines after each positive line in your valid_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/models/sequential/sequential_base_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_file, valid_file, valid_num_ngs, eval_metric)\u001b[0m\n\u001b[1;32m    130\u001b[0m             )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_data_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mstep_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/io/sequential_iterator.py\u001b[0m in \u001b[0;36mload_data_from_file\u001b[0;34m(self, infile, batch_num_ngs, min_seq_length)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/io/sequential_iterator.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(self, input_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_one_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/io/sequential_iterator.py\u001b[0m in \u001b[0;36mparser_one_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtime_history_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtime_history_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_history_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtime_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3600\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/recommenders-1.1.0-py3.9.egg/recommenders/models/deeprec/io/sequential_iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtime_history_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtime_history_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_history_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtime_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3600\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e41391",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
