{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/din_dice/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/din_dice/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "                          attention_mode=\"outer_product\",\n",
    "                          activation=['dice', 'dice']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['dice', 'dice'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'attention_mod': 'inner_product',\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/din_dice/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/din_dice/',\n",
       " 'attention_mode': 'outer_product'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6a1733",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/Desktop/code/recommenders/examples/00_quick_start/../../recommenders/models/deeprec/models/base_model.py:724: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-05 17:10:59.805629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:10:59.810128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:10:59.810307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:00.978662: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-05 17:11:00.979716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:00.979888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:00.980004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:01.289369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:01.289532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:01.289642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 17:11:01.289736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9634 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-05 17:11:01.309647: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-06-05 17:12:49.800518: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.5636, data_loss: 1.5636\n",
      "step 40 , total_loss: 1.5630, data_loss: 1.5630\n",
      "step 60 , total_loss: 1.5526, data_loss: 1.5526\n",
      "step 80 , total_loss: 1.4973, data_loss: 1.4973\n",
      "step 100 , total_loss: 1.5177, data_loss: 1.5177\n",
      "step 120 , total_loss: 1.5538, data_loss: 1.5538\n",
      "step 140 , total_loss: 1.5166, data_loss: 1.5166\n",
      "step 160 , total_loss: 1.5119, data_loss: 1.5119\n",
      "step 180 , total_loss: 1.5292, data_loss: 1.5292\n",
      "step 200 , total_loss: 1.5130, data_loss: 1.5130\n",
      "step 220 , total_loss: 1.4745, data_loss: 1.4745\n",
      "step 240 , total_loss: 1.4867, data_loss: 1.4867\n",
      "step 260 , total_loss: 1.4845, data_loss: 1.4845\n",
      "step 280 , total_loss: 1.4967, data_loss: 1.4967\n",
      "step 300 , total_loss: 1.4348, data_loss: 1.4348\n",
      "step 320 , total_loss: 1.3992, data_loss: 1.3992\n",
      "step 340 , total_loss: 1.4690, data_loss: 1.4690\n",
      "step 360 , total_loss: 1.4061, data_loss: 1.4061\n",
      "step 380 , total_loss: 1.4192, data_loss: 1.4192\n",
      "step 400 , total_loss: 1.4326, data_loss: 1.4326\n",
      "step 420 , total_loss: 1.4494, data_loss: 1.4494\n",
      "step 440 , total_loss: 1.4464, data_loss: 1.4464\n",
      "step 460 , total_loss: 1.3662, data_loss: 1.3662\n",
      "step 480 , total_loss: 1.3835, data_loss: 1.3835\n",
      "step 500 , total_loss: 1.3984, data_loss: 1.3984\n",
      "step 520 , total_loss: 1.3490, data_loss: 1.3490\n",
      "step 540 , total_loss: 1.3334, data_loss: 1.3334\n",
      "step 560 , total_loss: 1.3615, data_loss: 1.3615\n",
      "step 580 , total_loss: 1.4189, data_loss: 1.4189\n",
      "step 600 , total_loss: 1.3422, data_loss: 1.3422\n",
      "step 620 , total_loss: 1.3253, data_loss: 1.3253\n",
      "step 640 , total_loss: 1.3609, data_loss: 1.3609\n",
      "step 660 , total_loss: 1.3010, data_loss: 1.3010\n",
      "step 680 , total_loss: 1.3099, data_loss: 1.3099\n",
      "step 700 , total_loss: 1.2538, data_loss: 1.2538\n",
      "step 720 , total_loss: 1.3036, data_loss: 1.3036\n",
      "step 740 , total_loss: 1.2664, data_loss: 1.2664\n",
      "step 760 , total_loss: 1.3377, data_loss: 1.3377\n",
      "step 780 , total_loss: 1.3049, data_loss: 1.3049\n",
      "step 800 , total_loss: 1.2696, data_loss: 1.2696\n",
      "step 820 , total_loss: 1.2804, data_loss: 1.2804\n",
      "step 840 , total_loss: 1.3257, data_loss: 1.3257\n",
      "step 860 , total_loss: 1.2039, data_loss: 1.2039\n",
      "step 880 , total_loss: 1.2346, data_loss: 1.2346\n",
      "step 900 , total_loss: 1.2173, data_loss: 1.2173\n",
      "step 920 , total_loss: 1.2946, data_loss: 1.2946\n",
      "step 940 , total_loss: 1.1869, data_loss: 1.1869\n",
      "step 960 , total_loss: 1.2588, data_loss: 1.2588\n",
      "step 980 , total_loss: 1.1850, data_loss: 1.1850\n",
      "step 1000 , total_loss: 1.2845, data_loss: 1.2845\n",
      "step 1020 , total_loss: 1.2216, data_loss: 1.2216\n",
      "step 1040 , total_loss: 1.2548, data_loss: 1.2548\n",
      "step 1060 , total_loss: 1.1984, data_loss: 1.1984\n",
      "step 1080 , total_loss: 1.2196, data_loss: 1.2196\n",
      "step 1100 , total_loss: 1.2160, data_loss: 1.2160\n",
      "step 1120 , total_loss: 1.1708, data_loss: 1.1708\n",
      "step 1140 , total_loss: 1.1818, data_loss: 1.1818\n",
      "step 1160 , total_loss: 1.1845, data_loss: 1.1845\n",
      "step 1180 , total_loss: 1.1362, data_loss: 1.1362\n",
      "step 1200 , total_loss: 1.2363, data_loss: 1.2363\n",
      "step 1220 , total_loss: 1.1867, data_loss: 1.1867\n",
      "step 1240 , total_loss: 1.1924, data_loss: 1.1924\n",
      "step 1260 , total_loss: 1.1949, data_loss: 1.1949\n",
      "step 1280 , total_loss: 1.1914, data_loss: 1.1914\n",
      "step 1300 , total_loss: 1.1573, data_loss: 1.1573\n",
      "step 1320 , total_loss: 1.2343, data_loss: 1.2343\n",
      "step 1340 , total_loss: 1.1389, data_loss: 1.1389\n",
      "step 1360 , total_loss: 1.1712, data_loss: 1.1712\n",
      "step 1380 , total_loss: 1.1707, data_loss: 1.1707\n",
      "step 1400 , total_loss: 1.1487, data_loss: 1.1487\n",
      "step 1420 , total_loss: 1.1964, data_loss: 1.1964\n",
      "step 1440 , total_loss: 1.0554, data_loss: 1.0554\n",
      "step 1460 , total_loss: 1.1789, data_loss: 1.1789\n",
      "step 1480 , total_loss: 1.0393, data_loss: 1.0393\n",
      "step 1500 , total_loss: 1.1243, data_loss: 1.1243\n",
      "step 1520 , total_loss: 1.2097, data_loss: 1.2097\n",
      "step 1540 , total_loss: 1.2108, data_loss: 1.2108\n",
      "step 1560 , total_loss: 1.1619, data_loss: 1.1619\n",
      "step 1580 , total_loss: 1.2492, data_loss: 1.2492\n",
      "step 1600 , total_loss: 1.2067, data_loss: 1.2067\n",
      "step 1620 , total_loss: 1.2109, data_loss: 1.2109\n",
      "step 1640 , total_loss: 1.0855, data_loss: 1.0855\n",
      "step 1660 , total_loss: 1.1623, data_loss: 1.1623\n",
      "step 1680 , total_loss: 1.1817, data_loss: 1.1817\n",
      "step 1700 , total_loss: 1.1744, data_loss: 1.1744\n",
      "step 1720 , total_loss: 1.1799, data_loss: 1.1799\n",
      "step 1740 , total_loss: 1.1563, data_loss: 1.1563\n",
      "step 1760 , total_loss: 1.1585, data_loss: 1.1585\n",
      "step 1780 , total_loss: 1.1736, data_loss: 1.1736\n",
      "step 1800 , total_loss: 1.1357, data_loss: 1.1357\n",
      "step 1820 , total_loss: 1.1311, data_loss: 1.1311\n",
      "step 1840 , total_loss: 1.0764, data_loss: 1.0764\n",
      "step 1860 , total_loss: 1.0353, data_loss: 1.0353\n",
      "step 1880 , total_loss: 1.0966, data_loss: 1.0966\n",
      "step 1900 , total_loss: 1.1596, data_loss: 1.1596\n",
      "step 1920 , total_loss: 1.0735, data_loss: 1.0735\n",
      "step 1940 , total_loss: 1.1492, data_loss: 1.1492\n",
      "step 1960 , total_loss: 1.0935, data_loss: 1.0935\n",
      "step 1980 , total_loss: 1.1084, data_loss: 1.1084\n",
      "step 2000 , total_loss: 1.1493, data_loss: 1.1493\n",
      "step 2020 , total_loss: 1.0602, data_loss: 1.0602\n",
      "step 2040 , total_loss: 1.1126, data_loss: 1.1126\n",
      "step 2060 , total_loss: 1.0970, data_loss: 1.0970\n",
      "step 2080 , total_loss: 1.1090, data_loss: 1.1090\n",
      "step 2100 , total_loss: 1.0797, data_loss: 1.0797\n",
      "step 2120 , total_loss: 1.0936, data_loss: 1.0936\n",
      "step 2140 , total_loss: 1.1170, data_loss: 1.1170\n",
      "step 2160 , total_loss: 1.0138, data_loss: 1.0138\n",
      "step 2180 , total_loss: 1.0349, data_loss: 1.0349\n",
      "step 2200 , total_loss: 1.0748, data_loss: 1.0748\n",
      "step 2220 , total_loss: 1.0772, data_loss: 1.0772\n",
      "step 2240 , total_loss: 1.0923, data_loss: 1.0923\n",
      "step 2260 , total_loss: 1.0801, data_loss: 1.0801\n",
      "step 2280 , total_loss: 1.1004, data_loss: 1.1004\n",
      "step 2300 , total_loss: 1.1797, data_loss: 1.1797\n",
      "step 2320 , total_loss: 1.0914, data_loss: 1.0914\n",
      "step 2340 , total_loss: 1.0819, data_loss: 1.0819\n",
      "step 2360 , total_loss: 1.0591, data_loss: 1.0591\n",
      "step 2380 , total_loss: 1.1178, data_loss: 1.1178\n",
      "step 2400 , total_loss: 1.2294, data_loss: 1.2294\n",
      "step 2420 , total_loss: 1.1140, data_loss: 1.1140\n",
      "step 2440 , total_loss: 1.0829, data_loss: 1.0829\n",
      "step 2460 , total_loss: 1.0539, data_loss: 1.0539\n",
      "step 2480 , total_loss: 0.9535, data_loss: 0.9535\n",
      "step 2500 , total_loss: 1.1342, data_loss: 1.1342\n",
      "step 2520 , total_loss: 1.0775, data_loss: 1.0775\n",
      "step 2540 , total_loss: 1.0290, data_loss: 1.0290\n",
      "step 2560 , total_loss: 1.1424, data_loss: 1.1424\n",
      "step 2580 , total_loss: 1.0956, data_loss: 1.0956\n",
      "step 2600 , total_loss: 1.0128, data_loss: 1.0128\n",
      "step 2620 , total_loss: 1.0872, data_loss: 1.0872\n",
      "step 2640 , total_loss: 1.0023, data_loss: 1.0023\n",
      "step 2660 , total_loss: 1.1249, data_loss: 1.1249\n",
      "step 2680 , total_loss: 1.0059, data_loss: 1.0059\n",
      "step 2700 , total_loss: 1.0660, data_loss: 1.0660\n",
      "step 2720 , total_loss: 0.9973, data_loss: 0.9973\n",
      "step 2740 , total_loss: 1.0966, data_loss: 1.0966\n",
      "step 2760 , total_loss: 1.0060, data_loss: 1.0060\n",
      "step 2780 , total_loss: 1.0589, data_loss: 1.0589\n",
      "step 2800 , total_loss: 1.0875, data_loss: 1.0875\n",
      "step 2820 , total_loss: 1.0799, data_loss: 1.0799\n",
      "step 2840 , total_loss: 1.1651, data_loss: 1.1651\n",
      "step 2860 , total_loss: 1.1108, data_loss: 1.1108\n",
      "step 2880 , total_loss: 1.0263, data_loss: 1.0263\n",
      "step 2900 , total_loss: 1.1054, data_loss: 1.1054\n",
      "step 2920 , total_loss: 1.0130, data_loss: 1.0130\n",
      "step 2940 , total_loss: 1.1022, data_loss: 1.1022\n",
      "step 2960 , total_loss: 1.0104, data_loss: 1.0104\n",
      "step 2980 , total_loss: 0.9857, data_loss: 0.9857\n",
      "step 3000 , total_loss: 0.9960, data_loss: 0.9960\n",
      "step 3020 , total_loss: 1.1004, data_loss: 1.1004\n",
      "step 3040 , total_loss: 1.0515, data_loss: 1.0515\n",
      "step 3060 , total_loss: 1.0546, data_loss: 1.0546\n",
      "step 3080 , total_loss: 0.9798, data_loss: 0.9798\n",
      "step 3100 , total_loss: 1.0371, data_loss: 1.0371\n",
      "step 3120 , total_loss: 1.0286, data_loss: 1.0286\n",
      "step 3140 , total_loss: 1.0228, data_loss: 1.0228\n",
      "step 3160 , total_loss: 0.9852, data_loss: 0.9852\n",
      "step 3180 , total_loss: 1.0345, data_loss: 1.0345\n",
      "step 3200 , total_loss: 1.0269, data_loss: 1.0269\n",
      "step 3220 , total_loss: 0.9865, data_loss: 0.9865\n",
      "step 3240 , total_loss: 1.0096, data_loss: 1.0096\n",
      "step 3260 , total_loss: 0.9881, data_loss: 0.9881\n",
      "step 3280 , total_loss: 1.0337, data_loss: 1.0337\n",
      "step 3300 , total_loss: 1.0624, data_loss: 1.0624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval valid at epoch 1: auc:0.793,logloss:0.6768,mean_mrr:0.734,ndcg@2:0.6958,ndcg@4:0.7835,ndcg@6:0.8007,group_auc:0.7947\n",
      "step 20 , total_loss: 0.9912, data_loss: 0.9912\n",
      "step 40 , total_loss: 0.9293, data_loss: 0.9293\n",
      "step 60 , total_loss: 0.9967, data_loss: 0.9967\n",
      "step 80 , total_loss: 0.9945, data_loss: 0.9945\n",
      "step 100 , total_loss: 1.0453, data_loss: 1.0453\n",
      "step 120 , total_loss: 0.9906, data_loss: 0.9906\n",
      "step 140 , total_loss: 0.9479, data_loss: 0.9479\n",
      "step 160 , total_loss: 1.0351, data_loss: 1.0351\n",
      "step 180 , total_loss: 0.9580, data_loss: 0.9580\n",
      "step 200 , total_loss: 1.0197, data_loss: 1.0197\n",
      "step 220 , total_loss: 0.9481, data_loss: 0.9481\n",
      "step 240 , total_loss: 0.9707, data_loss: 0.9707\n",
      "step 260 , total_loss: 1.0223, data_loss: 1.0223\n",
      "step 280 , total_loss: 0.9398, data_loss: 0.9398\n",
      "step 300 , total_loss: 0.9515, data_loss: 0.9515\n",
      "step 320 , total_loss: 0.9649, data_loss: 0.9649\n",
      "step 340 , total_loss: 0.9810, data_loss: 0.9810\n",
      "step 360 , total_loss: 0.9883, data_loss: 0.9883\n",
      "step 380 , total_loss: 1.0033, data_loss: 1.0033\n",
      "step 400 , total_loss: 0.9953, data_loss: 0.9953\n",
      "step 420 , total_loss: 1.0841, data_loss: 1.0841\n",
      "step 440 , total_loss: 0.9843, data_loss: 0.9843\n",
      "step 460 , total_loss: 0.9353, data_loss: 0.9353\n",
      "step 480 , total_loss: 0.9838, data_loss: 0.9838\n",
      "step 500 , total_loss: 0.9792, data_loss: 0.9792\n",
      "step 520 , total_loss: 0.9765, data_loss: 0.9765\n",
      "step 540 , total_loss: 0.9456, data_loss: 0.9456\n",
      "step 560 , total_loss: 0.9766, data_loss: 0.9766\n",
      "step 580 , total_loss: 1.0229, data_loss: 1.0229\n",
      "step 600 , total_loss: 1.0098, data_loss: 1.0098\n",
      "step 620 , total_loss: 1.0577, data_loss: 1.0577\n",
      "step 640 , total_loss: 0.9694, data_loss: 0.9694\n",
      "step 660 , total_loss: 0.9744, data_loss: 0.9744\n",
      "step 680 , total_loss: 0.8789, data_loss: 0.8789\n",
      "step 700 , total_loss: 0.9767, data_loss: 0.9767\n",
      "step 720 , total_loss: 0.9244, data_loss: 0.9244\n",
      "step 740 , total_loss: 1.0671, data_loss: 1.0671\n",
      "step 760 , total_loss: 0.9514, data_loss: 0.9514\n",
      "step 780 , total_loss: 0.9499, data_loss: 0.9499\n",
      "step 800 , total_loss: 0.9368, data_loss: 0.9368\n",
      "step 820 , total_loss: 0.9808, data_loss: 0.9808\n",
      "step 840 , total_loss: 0.9168, data_loss: 0.9168\n",
      "step 860 , total_loss: 0.9579, data_loss: 0.9579\n",
      "step 880 , total_loss: 0.9690, data_loss: 0.9690\n",
      "step 900 , total_loss: 0.9324, data_loss: 0.9324\n",
      "step 920 , total_loss: 0.9859, data_loss: 0.9859\n",
      "step 940 , total_loss: 0.9600, data_loss: 0.9600\n",
      "step 960 , total_loss: 0.8968, data_loss: 0.8968\n",
      "step 980 , total_loss: 0.9328, data_loss: 0.9328\n",
      "step 1000 , total_loss: 1.0052, data_loss: 1.0052\n",
      "step 1020 , total_loss: 1.0585, data_loss: 1.0585\n",
      "step 1040 , total_loss: 0.9934, data_loss: 0.9934\n",
      "step 1060 , total_loss: 0.9598, data_loss: 0.9598\n",
      "step 1080 , total_loss: 1.0849, data_loss: 1.0849\n",
      "step 1100 , total_loss: 0.9479, data_loss: 0.9479\n",
      "step 1120 , total_loss: 0.9242, data_loss: 0.9242\n",
      "step 1140 , total_loss: 0.9044, data_loss: 0.9044\n",
      "step 1160 , total_loss: 0.9289, data_loss: 0.9289\n",
      "step 1180 , total_loss: 0.9670, data_loss: 0.9670\n",
      "step 1200 , total_loss: 0.9992, data_loss: 0.9992\n",
      "step 1220 , total_loss: 0.9444, data_loss: 0.9444\n",
      "step 1240 , total_loss: 1.0537, data_loss: 1.0537\n",
      "step 1260 , total_loss: 0.9152, data_loss: 0.9152\n",
      "step 1280 , total_loss: 0.8979, data_loss: 0.8979\n",
      "step 1300 , total_loss: 1.0456, data_loss: 1.0456\n",
      "step 1320 , total_loss: 0.9558, data_loss: 0.9558\n",
      "step 1340 , total_loss: 0.9346, data_loss: 0.9346\n",
      "step 1360 , total_loss: 0.9106, data_loss: 0.9106\n",
      "step 1380 , total_loss: 0.9869, data_loss: 0.9869\n",
      "step 1400 , total_loss: 0.9512, data_loss: 0.9512\n",
      "step 1420 , total_loss: 0.9466, data_loss: 0.9466\n",
      "step 1440 , total_loss: 0.8711, data_loss: 0.8711\n",
      "step 1460 , total_loss: 0.9557, data_loss: 0.9557\n",
      "step 1480 , total_loss: 0.9073, data_loss: 0.9073\n",
      "step 1500 , total_loss: 0.8739, data_loss: 0.8739\n",
      "step 1520 , total_loss: 0.9118, data_loss: 0.9118\n",
      "step 1540 , total_loss: 0.9519, data_loss: 0.9519\n",
      "step 1560 , total_loss: 1.1227, data_loss: 1.1227\n",
      "step 1580 , total_loss: 0.9678, data_loss: 0.9678\n",
      "step 1600 , total_loss: 0.9176, data_loss: 0.9176\n",
      "step 1620 , total_loss: 0.9392, data_loss: 0.9392\n",
      "step 1640 , total_loss: 1.0366, data_loss: 1.0366\n",
      "step 1660 , total_loss: 0.8992, data_loss: 0.8992\n",
      "step 1680 , total_loss: 0.8961, data_loss: 0.8961\n",
      "step 1700 , total_loss: 0.9913, data_loss: 0.9913\n",
      "step 1720 , total_loss: 0.9130, data_loss: 0.9130\n",
      "step 1740 , total_loss: 0.9339, data_loss: 0.9339\n",
      "step 1760 , total_loss: 0.9052, data_loss: 0.9052\n",
      "step 1780 , total_loss: 0.9567, data_loss: 0.9567\n",
      "step 1800 , total_loss: 0.9762, data_loss: 0.9762\n",
      "step 1820 , total_loss: 0.9471, data_loss: 0.9471\n",
      "step 1840 , total_loss: 0.9223, data_loss: 0.9223\n",
      "step 1860 , total_loss: 0.8635, data_loss: 0.8635\n",
      "step 1880 , total_loss: 0.9461, data_loss: 0.9461\n",
      "step 1900 , total_loss: 0.9779, data_loss: 0.9779\n",
      "step 1920 , total_loss: 0.9046, data_loss: 0.9046\n",
      "step 1940 , total_loss: 0.9985, data_loss: 0.9985\n",
      "step 1960 , total_loss: 0.8797, data_loss: 0.8797\n",
      "step 1980 , total_loss: 1.0291, data_loss: 1.0291\n",
      "step 2000 , total_loss: 0.9373, data_loss: 0.9373\n",
      "step 2020 , total_loss: 0.9125, data_loss: 0.9125\n",
      "step 2040 , total_loss: 0.8948, data_loss: 0.8948\n",
      "step 2060 , total_loss: 1.0017, data_loss: 1.0017\n",
      "step 2080 , total_loss: 0.8973, data_loss: 0.8973\n",
      "step 2100 , total_loss: 0.8859, data_loss: 0.8859\n",
      "step 2120 , total_loss: 0.9432, data_loss: 0.9432\n",
      "step 2140 , total_loss: 0.9508, data_loss: 0.9508\n",
      "step 2160 , total_loss: 0.9046, data_loss: 0.9046\n",
      "step 2180 , total_loss: 1.0142, data_loss: 1.0142\n",
      "step 2200 , total_loss: 0.9557, data_loss: 0.9557\n",
      "step 2220 , total_loss: 0.8899, data_loss: 0.8899\n",
      "step 2240 , total_loss: 0.8748, data_loss: 0.8748\n",
      "step 2260 , total_loss: 0.9300, data_loss: 0.9300\n",
      "step 2280 , total_loss: 0.8792, data_loss: 0.8792\n",
      "step 2300 , total_loss: 0.9952, data_loss: 0.9952\n",
      "step 2320 , total_loss: 0.9473, data_loss: 0.9473\n",
      "step 2340 , total_loss: 0.9139, data_loss: 0.9139\n",
      "step 2360 , total_loss: 1.0217, data_loss: 1.0217\n",
      "step 2380 , total_loss: 0.8479, data_loss: 0.8479\n",
      "step 2400 , total_loss: 0.9167, data_loss: 0.9167\n",
      "step 2420 , total_loss: 0.8728, data_loss: 0.8728\n",
      "step 2440 , total_loss: 1.0000, data_loss: 1.0000\n",
      "step 2460 , total_loss: 0.9330, data_loss: 0.9330\n",
      "step 2480 , total_loss: 0.8783, data_loss: 0.8783\n",
      "step 2500 , total_loss: 0.9529, data_loss: 0.9529\n",
      "step 2520 , total_loss: 0.8916, data_loss: 0.8916\n",
      "step 2540 , total_loss: 0.9771, data_loss: 0.9771\n",
      "step 2560 , total_loss: 0.8863, data_loss: 0.8863\n",
      "step 2580 , total_loss: 1.0052, data_loss: 1.0052\n",
      "step 2600 , total_loss: 0.9211, data_loss: 0.9211\n",
      "step 2620 , total_loss: 0.9369, data_loss: 0.9369\n",
      "step 2640 , total_loss: 0.8665, data_loss: 0.8665\n",
      "step 2660 , total_loss: 0.9774, data_loss: 0.9774\n",
      "step 2680 , total_loss: 0.9079, data_loss: 0.9079\n",
      "step 2700 , total_loss: 0.8514, data_loss: 0.8514\n",
      "step 2720 , total_loss: 0.9959, data_loss: 0.9959\n",
      "step 2740 , total_loss: 0.9139, data_loss: 0.9139\n",
      "step 2760 , total_loss: 0.8912, data_loss: 0.8912\n",
      "step 2780 , total_loss: 0.9319, data_loss: 0.9319\n",
      "step 2800 , total_loss: 0.9675, data_loss: 0.9675\n",
      "step 2820 , total_loss: 0.8891, data_loss: 0.8891\n",
      "step 2840 , total_loss: 0.9124, data_loss: 0.9124\n",
      "step 2860 , total_loss: 0.9283, data_loss: 0.9283\n",
      "step 2880 , total_loss: 0.8930, data_loss: 0.8930\n",
      "step 2900 , total_loss: 0.9113, data_loss: 0.9113\n",
      "step 2920 , total_loss: 0.9817, data_loss: 0.9817\n",
      "step 2940 , total_loss: 0.9377, data_loss: 0.9377\n",
      "step 2960 , total_loss: 0.8371, data_loss: 0.8371\n",
      "step 2980 , total_loss: 0.8728, data_loss: 0.8728\n",
      "step 3000 , total_loss: 0.9562, data_loss: 0.9562\n",
      "step 3020 , total_loss: 0.9324, data_loss: 0.9324\n",
      "step 3040 , total_loss: 0.8569, data_loss: 0.8569\n",
      "step 3060 , total_loss: 0.9666, data_loss: 0.9666\n",
      "step 3080 , total_loss: 0.8939, data_loss: 0.8939\n",
      "step 3100 , total_loss: 0.8838, data_loss: 0.8838\n",
      "step 3120 , total_loss: 0.8716, data_loss: 0.8716\n",
      "step 3140 , total_loss: 0.8641, data_loss: 0.8641\n",
      "step 3160 , total_loss: 0.9169, data_loss: 0.9169\n",
      "step 3180 , total_loss: 0.9453, data_loss: 0.9453\n",
      "step 3200 , total_loss: 0.9772, data_loss: 0.9772\n",
      "step 3220 , total_loss: 0.9301, data_loss: 0.9301\n",
      "step 3240 , total_loss: 0.8809, data_loss: 0.8809\n",
      "step 3260 , total_loss: 0.9525, data_loss: 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3280 , total_loss: 0.8924, data_loss: 0.8924\n",
      "step 3300 , total_loss: 0.9699, data_loss: 0.9699\n",
      "eval valid at epoch 2: auc:0.8189,logloss:0.633,mean_mrr:0.7634,ndcg@2:0.7326,ndcg@4:0.8091,ndcg@6:0.8228,group_auc:0.8209\n",
      "step 20 , total_loss: 0.8894, data_loss: 0.8894\n",
      "step 40 , total_loss: 0.8872, data_loss: 0.8872\n",
      "step 60 , total_loss: 0.8528, data_loss: 0.8528\n",
      "step 80 , total_loss: 0.8498, data_loss: 0.8498\n",
      "step 100 , total_loss: 0.8382, data_loss: 0.8382\n",
      "step 120 , total_loss: 0.8428, data_loss: 0.8428\n",
      "step 140 , total_loss: 0.8226, data_loss: 0.8226\n",
      "step 160 , total_loss: 0.8480, data_loss: 0.8480\n",
      "step 180 , total_loss: 0.9250, data_loss: 0.9250\n",
      "step 200 , total_loss: 0.8606, data_loss: 0.8606\n",
      "step 220 , total_loss: 0.9100, data_loss: 0.9100\n",
      "step 240 , total_loss: 0.9012, data_loss: 0.9012\n",
      "step 260 , total_loss: 0.7607, data_loss: 0.7607\n",
      "step 280 , total_loss: 0.8310, data_loss: 0.8310\n",
      "step 300 , total_loss: 0.7963, data_loss: 0.7963\n",
      "step 320 , total_loss: 0.7325, data_loss: 0.7325\n",
      "step 340 , total_loss: 0.8255, data_loss: 0.8255\n",
      "step 360 , total_loss: 0.8500, data_loss: 0.8500\n",
      "step 380 , total_loss: 0.9890, data_loss: 0.9890\n",
      "step 400 , total_loss: 0.8552, data_loss: 0.8552\n",
      "step 420 , total_loss: 0.8441, data_loss: 0.8441\n",
      "step 440 , total_loss: 0.8263, data_loss: 0.8263\n",
      "step 460 , total_loss: 0.9687, data_loss: 0.9687\n",
      "step 480 , total_loss: 0.8165, data_loss: 0.8165\n",
      "step 500 , total_loss: 0.8486, data_loss: 0.8486\n",
      "step 520 , total_loss: 0.8392, data_loss: 0.8392\n",
      "step 540 , total_loss: 0.8756, data_loss: 0.8756\n",
      "step 560 , total_loss: 0.7810, data_loss: 0.7810\n",
      "step 580 , total_loss: 0.8970, data_loss: 0.8970\n",
      "step 600 , total_loss: 0.8490, data_loss: 0.8490\n",
      "step 620 , total_loss: 0.8993, data_loss: 0.8993\n",
      "step 640 , total_loss: 0.8384, data_loss: 0.8384\n",
      "step 660 , total_loss: 0.9035, data_loss: 0.9035\n",
      "step 680 , total_loss: 0.8439, data_loss: 0.8439\n",
      "step 700 , total_loss: 0.8553, data_loss: 0.8553\n",
      "step 720 , total_loss: 0.8133, data_loss: 0.8133\n",
      "step 740 , total_loss: 0.9010, data_loss: 0.9010\n",
      "step 760 , total_loss: 0.7861, data_loss: 0.7861\n",
      "step 780 , total_loss: 0.8153, data_loss: 0.8153\n",
      "step 800 , total_loss: 0.8678, data_loss: 0.8678\n",
      "step 820 , total_loss: 0.8958, data_loss: 0.8958\n",
      "step 840 , total_loss: 0.8701, data_loss: 0.8701\n",
      "step 860 , total_loss: 0.8569, data_loss: 0.8569\n",
      "step 880 , total_loss: 0.8698, data_loss: 0.8698\n",
      "step 900 , total_loss: 0.8395, data_loss: 0.8395\n",
      "step 920 , total_loss: 0.8086, data_loss: 0.8086\n",
      "step 940 , total_loss: 0.8335, data_loss: 0.8335\n",
      "step 960 , total_loss: 0.9051, data_loss: 0.9051\n",
      "step 980 , total_loss: 0.8219, data_loss: 0.8219\n",
      "step 1000 , total_loss: 0.8384, data_loss: 0.8384\n",
      "step 1020 , total_loss: 0.8000, data_loss: 0.8000\n",
      "step 1040 , total_loss: 0.8161, data_loss: 0.8161\n",
      "step 1060 , total_loss: 0.8557, data_loss: 0.8557\n",
      "step 1080 , total_loss: 0.8725, data_loss: 0.8725\n",
      "step 1100 , total_loss: 0.7843, data_loss: 0.7843\n",
      "step 1120 , total_loss: 0.8660, data_loss: 0.8660\n",
      "step 1140 , total_loss: 0.8472, data_loss: 0.8472\n",
      "step 1160 , total_loss: 0.8488, data_loss: 0.8488\n",
      "step 1180 , total_loss: 0.8041, data_loss: 0.8041\n",
      "step 1200 , total_loss: 0.9168, data_loss: 0.9168\n",
      "step 1220 , total_loss: 0.7718, data_loss: 0.7718\n",
      "step 1240 , total_loss: 0.8747, data_loss: 0.8747\n",
      "step 1260 , total_loss: 0.7539, data_loss: 0.7539\n",
      "step 1280 , total_loss: 0.9039, data_loss: 0.9039\n",
      "step 1300 , total_loss: 0.9150, data_loss: 0.9150\n",
      "step 1320 , total_loss: 0.8848, data_loss: 0.8848\n",
      "step 1340 , total_loss: 0.9294, data_loss: 0.9294\n",
      "step 1360 , total_loss: 0.8381, data_loss: 0.8381\n",
      "step 1380 , total_loss: 0.8281, data_loss: 0.8281\n",
      "step 1400 , total_loss: 0.8835, data_loss: 0.8835\n",
      "step 1420 , total_loss: 0.8879, data_loss: 0.8879\n",
      "step 1440 , total_loss: 0.8626, data_loss: 0.8626\n",
      "step 1460 , total_loss: 0.8816, data_loss: 0.8816\n",
      "step 1480 , total_loss: 0.8597, data_loss: 0.8597\n",
      "step 1500 , total_loss: 0.8974, data_loss: 0.8974\n",
      "step 1520 , total_loss: 0.8500, data_loss: 0.8500\n",
      "step 1540 , total_loss: 0.9109, data_loss: 0.9109\n",
      "step 1560 , total_loss: 0.7836, data_loss: 0.7836\n",
      "step 1580 , total_loss: 0.8371, data_loss: 0.8371\n",
      "step 1600 , total_loss: 0.8459, data_loss: 0.8459\n",
      "step 1620 , total_loss: 0.9312, data_loss: 0.9312\n",
      "step 1640 , total_loss: 0.8501, data_loss: 0.8501\n",
      "step 1660 , total_loss: 0.8601, data_loss: 0.8601\n",
      "step 1680 , total_loss: 0.8677, data_loss: 0.8677\n",
      "step 1700 , total_loss: 0.8625, data_loss: 0.8625\n",
      "step 1720 , total_loss: 0.8300, data_loss: 0.8300\n",
      "step 1740 , total_loss: 0.8620, data_loss: 0.8620\n",
      "step 1760 , total_loss: 0.8521, data_loss: 0.8521\n",
      "step 1780 , total_loss: 0.8339, data_loss: 0.8339\n",
      "step 1800 , total_loss: 0.7942, data_loss: 0.7942\n",
      "step 1820 , total_loss: 0.7989, data_loss: 0.7989\n",
      "step 1840 , total_loss: 0.8836, data_loss: 0.8836\n",
      "step 1860 , total_loss: 0.8469, data_loss: 0.8469\n",
      "step 1880 , total_loss: 0.8578, data_loss: 0.8578\n",
      "step 1900 , total_loss: 0.8034, data_loss: 0.8034\n",
      "step 1920 , total_loss: 0.7742, data_loss: 0.7742\n",
      "step 1940 , total_loss: 0.8662, data_loss: 0.8662\n",
      "step 1960 , total_loss: 0.9341, data_loss: 0.9341\n",
      "step 1980 , total_loss: 0.8139, data_loss: 0.8139\n",
      "step 2000 , total_loss: 0.9170, data_loss: 0.9170\n",
      "step 2020 , total_loss: 0.8279, data_loss: 0.8279\n",
      "step 2040 , total_loss: 0.8595, data_loss: 0.8595\n",
      "step 2060 , total_loss: 0.7728, data_loss: 0.7728\n",
      "step 2080 , total_loss: 0.8524, data_loss: 0.8524\n",
      "step 2100 , total_loss: 0.8936, data_loss: 0.8936\n",
      "step 2120 , total_loss: 0.8383, data_loss: 0.8383\n",
      "step 2140 , total_loss: 0.8981, data_loss: 0.8981\n",
      "step 2160 , total_loss: 0.8562, data_loss: 0.8562\n",
      "step 2180 , total_loss: 0.8803, data_loss: 0.8803\n",
      "step 2200 , total_loss: 0.8966, data_loss: 0.8966\n",
      "step 2220 , total_loss: 0.7542, data_loss: 0.7542\n",
      "step 2240 , total_loss: 0.9548, data_loss: 0.9548\n",
      "step 2260 , total_loss: 0.8931, data_loss: 0.8931\n",
      "step 2280 , total_loss: 0.8701, data_loss: 0.8701\n",
      "step 2300 , total_loss: 0.9115, data_loss: 0.9115\n",
      "step 2320 , total_loss: 0.7772, data_loss: 0.7772\n",
      "step 2340 , total_loss: 0.8394, data_loss: 0.8394\n",
      "step 2360 , total_loss: 0.8753, data_loss: 0.8753\n",
      "step 2380 , total_loss: 0.8032, data_loss: 0.8032\n",
      "step 2400 , total_loss: 0.8562, data_loss: 0.8562\n",
      "step 2420 , total_loss: 0.9825, data_loss: 0.9825\n",
      "step 2440 , total_loss: 0.8386, data_loss: 0.8386\n",
      "step 2460 , total_loss: 0.8500, data_loss: 0.8500\n",
      "step 2480 , total_loss: 0.8323, data_loss: 0.8323\n",
      "step 2500 , total_loss: 0.8682, data_loss: 0.8682\n",
      "step 2520 , total_loss: 0.8301, data_loss: 0.8301\n",
      "step 2540 , total_loss: 0.8436, data_loss: 0.8436\n",
      "step 2560 , total_loss: 0.8330, data_loss: 0.8330\n",
      "step 2580 , total_loss: 0.8204, data_loss: 0.8204\n",
      "step 2600 , total_loss: 0.8427, data_loss: 0.8427\n",
      "step 2620 , total_loss: 0.8127, data_loss: 0.8127\n",
      "step 2640 , total_loss: 0.7944, data_loss: 0.7944\n",
      "step 2660 , total_loss: 0.8380, data_loss: 0.8380\n",
      "step 2680 , total_loss: 0.9496, data_loss: 0.9496\n",
      "step 2700 , total_loss: 0.8159, data_loss: 0.8159\n",
      "step 2720 , total_loss: 0.8796, data_loss: 0.8796\n",
      "step 2740 , total_loss: 0.8139, data_loss: 0.8139\n",
      "step 2760 , total_loss: 0.9287, data_loss: 0.9287\n",
      "step 2780 , total_loss: 0.8347, data_loss: 0.8347\n",
      "step 2800 , total_loss: 0.8757, data_loss: 0.8757\n",
      "step 2820 , total_loss: 0.8606, data_loss: 0.8606\n",
      "step 2840 , total_loss: 0.8593, data_loss: 0.8593\n",
      "step 2860 , total_loss: 0.8378, data_loss: 0.8378\n",
      "step 2880 , total_loss: 1.0162, data_loss: 1.0162\n",
      "step 2900 , total_loss: 0.7864, data_loss: 0.7864\n",
      "step 2920 , total_loss: 0.8545, data_loss: 0.8545\n",
      "step 2940 , total_loss: 0.8368, data_loss: 0.8368\n",
      "step 2960 , total_loss: 0.8905, data_loss: 0.8905\n",
      "step 2980 , total_loss: 0.8500, data_loss: 0.8500\n",
      "step 3000 , total_loss: 0.8431, data_loss: 0.8431\n",
      "step 3020 , total_loss: 0.9101, data_loss: 0.9101\n",
      "step 3040 , total_loss: 0.8942, data_loss: 0.8942\n",
      "step 3060 , total_loss: 0.8400, data_loss: 0.8400\n",
      "step 3080 , total_loss: 0.8351, data_loss: 0.8351\n",
      "step 3100 , total_loss: 0.8531, data_loss: 0.8531\n",
      "step 3120 , total_loss: 0.8284, data_loss: 0.8284\n",
      "step 3140 , total_loss: 0.8604, data_loss: 0.8604\n",
      "step 3160 , total_loss: 0.9020, data_loss: 0.9020\n",
      "step 3180 , total_loss: 0.8873, data_loss: 0.8873\n",
      "step 3200 , total_loss: 0.8154, data_loss: 0.8154\n",
      "step 3220 , total_loss: 0.8403, data_loss: 0.8403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3240 , total_loss: 0.8808, data_loss: 0.8808\n",
      "step 3260 , total_loss: 0.8749, data_loss: 0.8749\n",
      "step 3280 , total_loss: 0.9057, data_loss: 0.9057\n",
      "step 3300 , total_loss: 0.8635, data_loss: 0.8635\n",
      "eval valid at epoch 3: auc:0.8284,logloss:0.6861,mean_mrr:0.775,ndcg@2:0.7465,ndcg@4:0.8188,ndcg@6:0.8315,group_auc:0.8306\n",
      "step 20 , total_loss: 0.7927, data_loss: 0.7927\n",
      "step 40 , total_loss: 0.8111, data_loss: 0.8111\n",
      "step 60 , total_loss: 0.8140, data_loss: 0.8140\n",
      "step 80 , total_loss: 0.8524, data_loss: 0.8524\n",
      "step 100 , total_loss: 0.7806, data_loss: 0.7806\n",
      "step 120 , total_loss: 0.7855, data_loss: 0.7855\n",
      "step 140 , total_loss: 0.7536, data_loss: 0.7536\n",
      "step 160 , total_loss: 0.8252, data_loss: 0.8252\n",
      "step 180 , total_loss: 0.7574, data_loss: 0.7574\n",
      "step 200 , total_loss: 0.7774, data_loss: 0.7774\n",
      "step 220 , total_loss: 0.7672, data_loss: 0.7672\n",
      "step 240 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 260 , total_loss: 0.7725, data_loss: 0.7725\n",
      "step 280 , total_loss: 0.7997, data_loss: 0.7997\n",
      "step 300 , total_loss: 0.8416, data_loss: 0.8416\n",
      "step 320 , total_loss: 0.8192, data_loss: 0.8192\n",
      "step 340 , total_loss: 0.7511, data_loss: 0.7511\n",
      "step 360 , total_loss: 0.7873, data_loss: 0.7873\n",
      "step 380 , total_loss: 0.7588, data_loss: 0.7588\n",
      "step 400 , total_loss: 0.8215, data_loss: 0.8215\n",
      "step 420 , total_loss: 0.8530, data_loss: 0.8530\n",
      "step 440 , total_loss: 0.7637, data_loss: 0.7637\n",
      "step 460 , total_loss: 0.7902, data_loss: 0.7902\n",
      "step 480 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 500 , total_loss: 0.8091, data_loss: 0.8091\n",
      "step 520 , total_loss: 0.8157, data_loss: 0.8157\n",
      "step 540 , total_loss: 0.7580, data_loss: 0.7580\n",
      "step 560 , total_loss: 0.7646, data_loss: 0.7646\n",
      "step 580 , total_loss: 0.8200, data_loss: 0.8200\n",
      "step 600 , total_loss: 0.7407, data_loss: 0.7407\n",
      "step 620 , total_loss: 0.8146, data_loss: 0.8146\n",
      "step 640 , total_loss: 0.7667, data_loss: 0.7667\n",
      "step 660 , total_loss: 0.8135, data_loss: 0.8135\n",
      "step 680 , total_loss: 0.7432, data_loss: 0.7432\n",
      "step 700 , total_loss: 0.9066, data_loss: 0.9066\n",
      "step 720 , total_loss: 0.7558, data_loss: 0.7558\n",
      "step 740 , total_loss: 0.7881, data_loss: 0.7881\n",
      "step 760 , total_loss: 0.8593, data_loss: 0.8593\n",
      "step 780 , total_loss: 0.6973, data_loss: 0.6973\n",
      "step 800 , total_loss: 0.8281, data_loss: 0.8281\n",
      "step 820 , total_loss: 0.7650, data_loss: 0.7650\n",
      "step 840 , total_loss: 0.8093, data_loss: 0.8093\n",
      "step 860 , total_loss: 0.7838, data_loss: 0.7838\n",
      "step 880 , total_loss: 0.8427, data_loss: 0.8427\n",
      "step 900 , total_loss: 0.8956, data_loss: 0.8956\n",
      "step 920 , total_loss: 0.7521, data_loss: 0.7521\n",
      "step 940 , total_loss: 0.6858, data_loss: 0.6858\n",
      "step 960 , total_loss: 0.8555, data_loss: 0.8555\n",
      "step 980 , total_loss: 0.8791, data_loss: 0.8791\n",
      "step 1000 , total_loss: 0.8442, data_loss: 0.8442\n",
      "step 1020 , total_loss: 0.8420, data_loss: 0.8420\n",
      "step 1040 , total_loss: 0.7852, data_loss: 0.7852\n",
      "step 1060 , total_loss: 0.8091, data_loss: 0.8091\n",
      "step 1080 , total_loss: 0.7760, data_loss: 0.7760\n",
      "step 1100 , total_loss: 0.8307, data_loss: 0.8307\n",
      "step 1120 , total_loss: 0.7569, data_loss: 0.7569\n",
      "step 1140 , total_loss: 0.7720, data_loss: 0.7720\n",
      "step 1160 , total_loss: 0.7453, data_loss: 0.7453\n",
      "step 1180 , total_loss: 0.8989, data_loss: 0.8989\n",
      "step 1200 , total_loss: 0.7390, data_loss: 0.7390\n",
      "step 1220 , total_loss: 0.8684, data_loss: 0.8684\n",
      "step 1240 , total_loss: 0.8805, data_loss: 0.8805\n",
      "step 1260 , total_loss: 0.7549, data_loss: 0.7549\n",
      "step 1280 , total_loss: 0.7983, data_loss: 0.7983\n",
      "step 1300 , total_loss: 0.8150, data_loss: 0.8150\n",
      "step 1320 , total_loss: 0.7422, data_loss: 0.7422\n",
      "step 1340 , total_loss: 0.8425, data_loss: 0.8425\n",
      "step 1360 , total_loss: 0.8242, data_loss: 0.8242\n",
      "step 1380 , total_loss: 0.8361, data_loss: 0.8361\n",
      "step 1400 , total_loss: 0.7826, data_loss: 0.7826\n",
      "step 1420 , total_loss: 0.8270, data_loss: 0.8270\n",
      "step 1440 , total_loss: 0.7457, data_loss: 0.7457\n",
      "step 1460 , total_loss: 0.7733, data_loss: 0.7733\n",
      "step 1480 , total_loss: 0.8029, data_loss: 0.8029\n",
      "step 1500 , total_loss: 0.8839, data_loss: 0.8839\n",
      "step 1520 , total_loss: 0.7393, data_loss: 0.7393\n",
      "step 1540 , total_loss: 0.7706, data_loss: 0.7706\n",
      "step 1560 , total_loss: 0.8279, data_loss: 0.8279\n",
      "step 1580 , total_loss: 0.9061, data_loss: 0.9061\n",
      "step 1600 , total_loss: 0.7687, data_loss: 0.7687\n",
      "step 1620 , total_loss: 0.8347, data_loss: 0.8347\n",
      "step 1640 , total_loss: 0.9362, data_loss: 0.9362\n",
      "step 1660 , total_loss: 0.8020, data_loss: 0.8020\n",
      "step 1680 , total_loss: 0.7817, data_loss: 0.7817\n",
      "step 1700 , total_loss: 0.7923, data_loss: 0.7923\n",
      "step 1720 , total_loss: 0.9099, data_loss: 0.9099\n",
      "step 1740 , total_loss: 0.8206, data_loss: 0.8206\n",
      "step 1760 , total_loss: 0.7303, data_loss: 0.7303\n",
      "step 1780 , total_loss: 0.8277, data_loss: 0.8277\n",
      "step 1800 , total_loss: 0.6493, data_loss: 0.6493\n",
      "step 1820 , total_loss: 0.7734, data_loss: 0.7734\n",
      "step 1840 , total_loss: 0.7651, data_loss: 0.7651\n",
      "step 1860 , total_loss: 0.7949, data_loss: 0.7949\n",
      "step 1880 , total_loss: 0.8277, data_loss: 0.8277\n",
      "step 1900 , total_loss: 0.7891, data_loss: 0.7891\n",
      "step 1920 , total_loss: 0.7318, data_loss: 0.7318\n",
      "step 1940 , total_loss: 0.8305, data_loss: 0.8305\n",
      "step 1960 , total_loss: 0.8069, data_loss: 0.8069\n",
      "step 1980 , total_loss: 0.7674, data_loss: 0.7674\n",
      "step 2000 , total_loss: 0.7990, data_loss: 0.7990\n",
      "step 2020 , total_loss: 0.8811, data_loss: 0.8811\n",
      "step 2040 , total_loss: 0.8934, data_loss: 0.8934\n",
      "step 2060 , total_loss: 0.7845, data_loss: 0.7845\n",
      "step 2080 , total_loss: 0.8294, data_loss: 0.8294\n",
      "step 2100 , total_loss: 0.8007, data_loss: 0.8007\n",
      "step 2120 , total_loss: 0.8376, data_loss: 0.8376\n",
      "step 2140 , total_loss: 0.8242, data_loss: 0.8242\n",
      "step 2160 , total_loss: 0.7897, data_loss: 0.7897\n",
      "step 2180 , total_loss: 0.8185, data_loss: 0.8185\n",
      "step 2200 , total_loss: 0.7889, data_loss: 0.7889\n",
      "step 2220 , total_loss: 0.7443, data_loss: 0.7443\n",
      "step 2240 , total_loss: 0.8505, data_loss: 0.8505\n",
      "step 2260 , total_loss: 0.7477, data_loss: 0.7477\n",
      "step 2280 , total_loss: 0.7932, data_loss: 0.7932\n",
      "step 2300 , total_loss: 0.8034, data_loss: 0.8034\n",
      "step 2320 , total_loss: 0.7238, data_loss: 0.7238\n",
      "step 2340 , total_loss: 0.9125, data_loss: 0.9125\n",
      "step 2360 , total_loss: 0.7962, data_loss: 0.7962\n",
      "step 2380 , total_loss: 0.8141, data_loss: 0.8141\n",
      "step 2400 , total_loss: 0.9141, data_loss: 0.9141\n",
      "step 2420 , total_loss: 0.8768, data_loss: 0.8768\n",
      "step 2440 , total_loss: 0.7255, data_loss: 0.7255\n",
      "step 2460 , total_loss: 0.8208, data_loss: 0.8208\n",
      "step 2480 , total_loss: 0.8680, data_loss: 0.8680\n",
      "step 2500 , total_loss: 0.8011, data_loss: 0.8011\n",
      "step 2520 , total_loss: 0.8487, data_loss: 0.8487\n",
      "step 2540 , total_loss: 0.8045, data_loss: 0.8045\n",
      "step 2560 , total_loss: 0.9422, data_loss: 0.9422\n",
      "step 2580 , total_loss: 0.7674, data_loss: 0.7674\n",
      "step 2600 , total_loss: 0.8070, data_loss: 0.8070\n",
      "step 2620 , total_loss: 0.8499, data_loss: 0.8499\n",
      "step 2640 , total_loss: 0.7989, data_loss: 0.7989\n",
      "step 2660 , total_loss: 0.7263, data_loss: 0.7263\n",
      "step 2680 , total_loss: 0.8072, data_loss: 0.8072\n",
      "step 2700 , total_loss: 0.8006, data_loss: 0.8006\n",
      "step 2720 , total_loss: 0.8287, data_loss: 0.8287\n",
      "step 2740 , total_loss: 0.9216, data_loss: 0.9216\n",
      "step 2760 , total_loss: 0.8456, data_loss: 0.8456\n",
      "step 2780 , total_loss: 0.8633, data_loss: 0.8633\n",
      "step 2800 , total_loss: 0.8584, data_loss: 0.8584\n",
      "step 2820 , total_loss: 0.8354, data_loss: 0.8354\n",
      "step 2840 , total_loss: 0.8934, data_loss: 0.8934\n",
      "step 2860 , total_loss: 0.7955, data_loss: 0.7955\n",
      "step 2880 , total_loss: 0.8396, data_loss: 0.8396\n",
      "step 2900 , total_loss: 0.8002, data_loss: 0.8002\n",
      "step 2920 , total_loss: 0.8159, data_loss: 0.8159\n",
      "step 2940 , total_loss: 0.8010, data_loss: 0.8010\n",
      "step 2960 , total_loss: 0.7412, data_loss: 0.7412\n",
      "step 2980 , total_loss: 0.8578, data_loss: 0.8578\n",
      "step 3000 , total_loss: 0.8658, data_loss: 0.8658\n",
      "step 3020 , total_loss: 0.8181, data_loss: 0.8181\n",
      "step 3040 , total_loss: 0.7267, data_loss: 0.7267\n",
      "step 3060 , total_loss: 0.7958, data_loss: 0.7958\n",
      "step 3080 , total_loss: 0.7999, data_loss: 0.7999\n",
      "step 3100 , total_loss: 0.7929, data_loss: 0.7929\n",
      "step 3120 , total_loss: 0.8187, data_loss: 0.8187\n",
      "step 3140 , total_loss: 0.8241, data_loss: 0.8241\n",
      "step 3160 , total_loss: 0.7706, data_loss: 0.7706\n",
      "step 3180 , total_loss: 0.8059, data_loss: 0.8059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200 , total_loss: 0.8625, data_loss: 0.8625\n",
      "step 3220 , total_loss: 0.8175, data_loss: 0.8175\n",
      "step 3240 , total_loss: 0.7610, data_loss: 0.7610\n",
      "step 3260 , total_loss: 0.8304, data_loss: 0.8304\n",
      "step 3280 , total_loss: 0.7645, data_loss: 0.7645\n",
      "step 3300 , total_loss: 0.7734, data_loss: 0.7734\n",
      "eval valid at epoch 4: auc:0.832,logloss:0.7149,mean_mrr:0.7789,ndcg@2:0.7514,ndcg@4:0.8221,ndcg@6:0.8344,group_auc:0.834\n",
      "step 20 , total_loss: 0.7493, data_loss: 0.7493\n",
      "step 40 , total_loss: 0.7910, data_loss: 0.7910\n",
      "step 60 , total_loss: 0.7796, data_loss: 0.7796\n",
      "step 80 , total_loss: 0.7057, data_loss: 0.7057\n",
      "step 100 , total_loss: 0.8894, data_loss: 0.8894\n",
      "step 120 , total_loss: 0.7343, data_loss: 0.7343\n",
      "step 140 , total_loss: 0.6884, data_loss: 0.6884\n",
      "step 160 , total_loss: 0.8078, data_loss: 0.8078\n",
      "step 180 , total_loss: 0.7010, data_loss: 0.7010\n",
      "step 200 , total_loss: 0.7718, data_loss: 0.7718\n",
      "step 220 , total_loss: 0.8263, data_loss: 0.8263\n",
      "step 240 , total_loss: 0.7276, data_loss: 0.7276\n",
      "step 260 , total_loss: 0.7418, data_loss: 0.7418\n",
      "step 280 , total_loss: 0.7699, data_loss: 0.7699\n",
      "step 300 , total_loss: 0.6750, data_loss: 0.6750\n",
      "step 320 , total_loss: 0.7198, data_loss: 0.7198\n",
      "step 340 , total_loss: 0.7502, data_loss: 0.7502\n",
      "step 360 , total_loss: 0.7292, data_loss: 0.7292\n",
      "step 380 , total_loss: 0.7421, data_loss: 0.7421\n",
      "step 400 , total_loss: 0.7355, data_loss: 0.7355\n",
      "step 420 , total_loss: 0.7626, data_loss: 0.7626\n",
      "step 440 , total_loss: 0.8066, data_loss: 0.8066\n",
      "step 460 , total_loss: 0.7304, data_loss: 0.7304\n",
      "step 480 , total_loss: 0.7190, data_loss: 0.7190\n",
      "step 500 , total_loss: 0.6987, data_loss: 0.6987\n",
      "step 520 , total_loss: 0.7752, data_loss: 0.7752\n",
      "step 540 , total_loss: 0.7555, data_loss: 0.7555\n",
      "step 560 , total_loss: 0.7194, data_loss: 0.7194\n",
      "step 580 , total_loss: 0.7020, data_loss: 0.7020\n",
      "step 600 , total_loss: 0.7530, data_loss: 0.7530\n",
      "step 620 , total_loss: 0.7508, data_loss: 0.7508\n",
      "step 640 , total_loss: 0.8032, data_loss: 0.8032\n",
      "step 660 , total_loss: 0.6539, data_loss: 0.6539\n",
      "step 680 , total_loss: 0.8295, data_loss: 0.8295\n",
      "step 700 , total_loss: 0.8218, data_loss: 0.8218\n",
      "step 720 , total_loss: 0.8107, data_loss: 0.8107\n",
      "step 740 , total_loss: 0.8261, data_loss: 0.8261\n",
      "step 760 , total_loss: 0.7146, data_loss: 0.7146\n",
      "step 780 , total_loss: 0.6601, data_loss: 0.6601\n",
      "step 800 , total_loss: 0.8331, data_loss: 0.8331\n",
      "step 820 , total_loss: 0.7198, data_loss: 0.7198\n",
      "step 840 , total_loss: 0.7532, data_loss: 0.7532\n",
      "step 860 , total_loss: 0.8815, data_loss: 0.8815\n",
      "step 880 , total_loss: 0.7808, data_loss: 0.7808\n",
      "step 900 , total_loss: 0.7584, data_loss: 0.7584\n",
      "step 920 , total_loss: 0.7432, data_loss: 0.7432\n",
      "step 940 , total_loss: 0.7048, data_loss: 0.7048\n",
      "step 960 , total_loss: 0.6517, data_loss: 0.6517\n",
      "step 980 , total_loss: 0.7336, data_loss: 0.7336\n",
      "step 1000 , total_loss: 0.7324, data_loss: 0.7324\n",
      "step 1020 , total_loss: 0.7292, data_loss: 0.7292\n",
      "step 1040 , total_loss: 0.7815, data_loss: 0.7815\n",
      "step 1060 , total_loss: 0.7823, data_loss: 0.7823\n",
      "step 1080 , total_loss: 0.6767, data_loss: 0.6767\n",
      "step 1100 , total_loss: 0.8143, data_loss: 0.8143\n",
      "step 1120 , total_loss: 0.7236, data_loss: 0.7236\n",
      "step 1140 , total_loss: 0.8224, data_loss: 0.8224\n",
      "step 1160 , total_loss: 0.8232, data_loss: 0.8232\n",
      "step 1180 , total_loss: 0.7492, data_loss: 0.7492\n",
      "step 1200 , total_loss: 0.7769, data_loss: 0.7769\n",
      "step 1220 , total_loss: 0.6789, data_loss: 0.6789\n",
      "step 1240 , total_loss: 0.8682, data_loss: 0.8682\n",
      "step 1260 , total_loss: 0.6922, data_loss: 0.6922\n",
      "step 1280 , total_loss: 0.7533, data_loss: 0.7533\n",
      "step 1300 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 1320 , total_loss: 0.7555, data_loss: 0.7555\n",
      "step 1340 , total_loss: 0.6854, data_loss: 0.6854\n",
      "step 1360 , total_loss: 0.8270, data_loss: 0.8270\n",
      "step 1380 , total_loss: 0.7209, data_loss: 0.7209\n",
      "step 1400 , total_loss: 0.6744, data_loss: 0.6744\n",
      "step 1420 , total_loss: 0.7383, data_loss: 0.7383\n",
      "step 1440 , total_loss: 0.7367, data_loss: 0.7367\n",
      "step 1460 , total_loss: 0.7503, data_loss: 0.7503\n",
      "step 1480 , total_loss: 0.7766, data_loss: 0.7766\n",
      "step 1500 , total_loss: 0.7120, data_loss: 0.7120\n",
      "step 1520 , total_loss: 0.7433, data_loss: 0.7433\n",
      "step 1540 , total_loss: 0.7513, data_loss: 0.7513\n",
      "step 1560 , total_loss: 0.6529, data_loss: 0.6529\n",
      "step 1580 , total_loss: 0.7723, data_loss: 0.7723\n",
      "step 1600 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 1620 , total_loss: 0.8714, data_loss: 0.8714\n",
      "step 1640 , total_loss: 0.7713, data_loss: 0.7713\n",
      "step 1660 , total_loss: 0.7434, data_loss: 0.7434\n",
      "step 1680 , total_loss: 0.8349, data_loss: 0.8349\n",
      "step 1700 , total_loss: 0.7393, data_loss: 0.7393\n",
      "step 1720 , total_loss: 0.7277, data_loss: 0.7277\n",
      "step 1740 , total_loss: 0.8363, data_loss: 0.8363\n",
      "step 1760 , total_loss: 0.8181, data_loss: 0.8181\n",
      "step 1780 , total_loss: 0.7175, data_loss: 0.7175\n",
      "step 1800 , total_loss: 0.7214, data_loss: 0.7214\n",
      "step 1820 , total_loss: 0.8456, data_loss: 0.8456\n",
      "step 1840 , total_loss: 0.7311, data_loss: 0.7311\n",
      "step 1860 , total_loss: 0.7275, data_loss: 0.7275\n",
      "step 1880 , total_loss: 0.8102, data_loss: 0.8102\n",
      "step 1900 , total_loss: 0.8201, data_loss: 0.8201\n",
      "step 1920 , total_loss: 0.6904, data_loss: 0.6904\n",
      "step 1940 , total_loss: 0.7358, data_loss: 0.7358\n",
      "step 1960 , total_loss: 0.7120, data_loss: 0.7120\n",
      "step 1980 , total_loss: 0.7121, data_loss: 0.7121\n",
      "step 2000 , total_loss: 0.7704, data_loss: 0.7704\n",
      "step 2020 , total_loss: 0.8248, data_loss: 0.8248\n",
      "step 2040 , total_loss: 0.7363, data_loss: 0.7363\n",
      "step 2060 , total_loss: 0.7596, data_loss: 0.7596\n",
      "step 2080 , total_loss: 0.7660, data_loss: 0.7660\n",
      "step 2100 , total_loss: 0.8107, data_loss: 0.8107\n",
      "step 2120 , total_loss: 0.8069, data_loss: 0.8069\n",
      "step 2140 , total_loss: 0.6906, data_loss: 0.6906\n",
      "step 2160 , total_loss: 0.8326, data_loss: 0.8326\n",
      "step 2180 , total_loss: 0.7623, data_loss: 0.7623\n",
      "step 2200 , total_loss: 0.8206, data_loss: 0.8206\n",
      "step 2220 , total_loss: 0.7655, data_loss: 0.7655\n",
      "step 2240 , total_loss: 0.8367, data_loss: 0.8367\n",
      "step 2260 , total_loss: 0.7961, data_loss: 0.7961\n",
      "step 2280 , total_loss: 0.7334, data_loss: 0.7334\n",
      "step 2300 , total_loss: 0.8009, data_loss: 0.8009\n",
      "step 2320 , total_loss: 0.6826, data_loss: 0.6826\n",
      "step 2340 , total_loss: 0.8104, data_loss: 0.8104\n",
      "step 2360 , total_loss: 0.7363, data_loss: 0.7363\n",
      "step 2380 , total_loss: 0.7239, data_loss: 0.7239\n",
      "step 2400 , total_loss: 0.8131, data_loss: 0.8131\n",
      "step 2420 , total_loss: 0.7550, data_loss: 0.7550\n",
      "step 2440 , total_loss: 0.7430, data_loss: 0.7430\n",
      "step 2460 , total_loss: 0.7685, data_loss: 0.7685\n",
      "step 2480 , total_loss: 0.8265, data_loss: 0.8265\n",
      "step 2500 , total_loss: 0.8115, data_loss: 0.8115\n",
      "step 2520 , total_loss: 0.8025, data_loss: 0.8025\n",
      "step 2540 , total_loss: 0.7684, data_loss: 0.7684\n",
      "step 2560 , total_loss: 0.7533, data_loss: 0.7533\n",
      "step 2580 , total_loss: 0.6951, data_loss: 0.6951\n",
      "step 2600 , total_loss: 0.7714, data_loss: 0.7714\n",
      "step 2620 , total_loss: 0.7860, data_loss: 0.7860\n",
      "step 2640 , total_loss: 0.7252, data_loss: 0.7252\n",
      "step 2660 , total_loss: 0.7671, data_loss: 0.7671\n",
      "step 2680 , total_loss: 0.7333, data_loss: 0.7333\n",
      "step 2700 , total_loss: 0.7506, data_loss: 0.7506\n",
      "step 2720 , total_loss: 0.7771, data_loss: 0.7771\n",
      "step 2740 , total_loss: 0.8379, data_loss: 0.8379\n",
      "step 2760 , total_loss: 0.8700, data_loss: 0.8700\n",
      "step 2780 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 2800 , total_loss: 0.7997, data_loss: 0.7997\n",
      "step 2820 , total_loss: 0.8365, data_loss: 0.8365\n",
      "step 2840 , total_loss: 0.8109, data_loss: 0.8109\n",
      "step 2860 , total_loss: 0.7779, data_loss: 0.7779\n",
      "step 2880 , total_loss: 0.7501, data_loss: 0.7501\n",
      "step 2900 , total_loss: 0.7687, data_loss: 0.7687\n",
      "step 2920 , total_loss: 0.7326, data_loss: 0.7326\n",
      "step 2940 , total_loss: 0.8175, data_loss: 0.8175\n",
      "step 2960 , total_loss: 0.7437, data_loss: 0.7437\n",
      "step 2980 , total_loss: 0.8359, data_loss: 0.8359\n",
      "step 3000 , total_loss: 0.8131, data_loss: 0.8131\n",
      "step 3020 , total_loss: 0.8450, data_loss: 0.8450\n",
      "step 3040 , total_loss: 0.7863, data_loss: 0.7863\n",
      "step 3060 , total_loss: 0.7385, data_loss: 0.7385\n",
      "step 3080 , total_loss: 0.8137, data_loss: 0.8137\n",
      "step 3100 , total_loss: 0.7588, data_loss: 0.7588\n",
      "step 3120 , total_loss: 0.7143, data_loss: 0.7143\n",
      "step 3140 , total_loss: 0.8300, data_loss: 0.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3160 , total_loss: 0.7798, data_loss: 0.7798\n",
      "step 3180 , total_loss: 0.8600, data_loss: 0.8600\n",
      "step 3200 , total_loss: 0.8350, data_loss: 0.8350\n",
      "step 3220 , total_loss: 0.7944, data_loss: 0.7944\n",
      "step 3240 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 3260 , total_loss: 0.6693, data_loss: 0.6693\n",
      "step 3280 , total_loss: 0.8777, data_loss: 0.8777\n",
      "step 3300 , total_loss: 0.8384, data_loss: 0.8384\n",
      "eval valid at epoch 5: auc:0.8336,logloss:0.7827,mean_mrr:0.78,ndcg@2:0.7528,ndcg@4:0.8233,ndcg@6:0.8353,group_auc:0.8351\n",
      "step 20 , total_loss: 0.7311, data_loss: 0.7311\n",
      "step 40 , total_loss: 0.6350, data_loss: 0.6350\n",
      "step 60 , total_loss: 0.7554, data_loss: 0.7554\n",
      "step 80 , total_loss: 0.7180, data_loss: 0.7180\n",
      "step 100 , total_loss: 0.7174, data_loss: 0.7174\n",
      "step 120 , total_loss: 0.7038, data_loss: 0.7038\n",
      "step 140 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 160 , total_loss: 0.6906, data_loss: 0.6906\n",
      "step 180 , total_loss: 0.7529, data_loss: 0.7529\n",
      "step 200 , total_loss: 0.7543, data_loss: 0.7543\n",
      "step 220 , total_loss: 0.6761, data_loss: 0.6761\n",
      "step 240 , total_loss: 0.7424, data_loss: 0.7424\n",
      "step 260 , total_loss: 0.7626, data_loss: 0.7626\n",
      "step 280 , total_loss: 0.6323, data_loss: 0.6323\n",
      "step 300 , total_loss: 0.7790, data_loss: 0.7790\n",
      "step 320 , total_loss: 0.7609, data_loss: 0.7609\n",
      "step 340 , total_loss: 0.7995, data_loss: 0.7995\n",
      "step 360 , total_loss: 0.6692, data_loss: 0.6692\n",
      "step 380 , total_loss: 0.6321, data_loss: 0.6321\n",
      "step 400 , total_loss: 0.7239, data_loss: 0.7239\n",
      "step 420 , total_loss: 0.6355, data_loss: 0.6355\n",
      "step 440 , total_loss: 0.7257, data_loss: 0.7257\n",
      "step 460 , total_loss: 0.6857, data_loss: 0.6857\n",
      "step 480 , total_loss: 0.6119, data_loss: 0.6119\n",
      "step 500 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 520 , total_loss: 0.6837, data_loss: 0.6837\n",
      "step 540 , total_loss: 0.6709, data_loss: 0.6709\n",
      "step 560 , total_loss: 0.6529, data_loss: 0.6529\n",
      "step 580 , total_loss: 0.6146, data_loss: 0.6146\n",
      "step 600 , total_loss: 0.7407, data_loss: 0.7407\n",
      "step 620 , total_loss: 0.7378, data_loss: 0.7378\n",
      "step 640 , total_loss: 0.7486, data_loss: 0.7486\n",
      "step 660 , total_loss: 0.7770, data_loss: 0.7770\n",
      "step 680 , total_loss: 0.7291, data_loss: 0.7291\n",
      "step 700 , total_loss: 0.7288, data_loss: 0.7288\n",
      "step 720 , total_loss: 0.7440, data_loss: 0.7440\n",
      "step 740 , total_loss: 0.6753, data_loss: 0.6753\n",
      "step 760 , total_loss: 0.6155, data_loss: 0.6155\n",
      "step 780 , total_loss: 0.6848, data_loss: 0.6848\n",
      "step 800 , total_loss: 0.7988, data_loss: 0.7988\n",
      "step 820 , total_loss: 0.7345, data_loss: 0.7345\n",
      "step 840 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 860 , total_loss: 0.7468, data_loss: 0.7468\n",
      "step 880 , total_loss: 0.6945, data_loss: 0.6945\n",
      "step 900 , total_loss: 0.6482, data_loss: 0.6482\n",
      "step 920 , total_loss: 0.7443, data_loss: 0.7443\n",
      "step 940 , total_loss: 0.7386, data_loss: 0.7386\n",
      "step 960 , total_loss: 0.6943, data_loss: 0.6943\n",
      "step 980 , total_loss: 0.8207, data_loss: 0.8207\n",
      "step 1000 , total_loss: 0.7473, data_loss: 0.7473\n",
      "step 1020 , total_loss: 0.7530, data_loss: 0.7530\n",
      "step 1040 , total_loss: 0.7729, data_loss: 0.7729\n",
      "step 1060 , total_loss: 0.7563, data_loss: 0.7563\n",
      "step 1080 , total_loss: 0.7282, data_loss: 0.7282\n",
      "step 1100 , total_loss: 0.7059, data_loss: 0.7059\n",
      "step 1120 , total_loss: 0.7434, data_loss: 0.7434\n",
      "step 1140 , total_loss: 0.7047, data_loss: 0.7047\n",
      "step 1160 , total_loss: 0.7242, data_loss: 0.7242\n",
      "step 1180 , total_loss: 0.7184, data_loss: 0.7184\n",
      "step 1200 , total_loss: 0.7057, data_loss: 0.7057\n",
      "step 1220 , total_loss: 0.7257, data_loss: 0.7257\n",
      "step 1240 , total_loss: 0.6535, data_loss: 0.6535\n",
      "step 1260 , total_loss: 0.6936, data_loss: 0.6936\n",
      "step 1280 , total_loss: 0.6879, data_loss: 0.6879\n",
      "step 1300 , total_loss: 0.7184, data_loss: 0.7184\n",
      "step 1320 , total_loss: 0.6803, data_loss: 0.6803\n",
      "step 1340 , total_loss: 0.8133, data_loss: 0.8133\n",
      "step 1360 , total_loss: 0.6926, data_loss: 0.6926\n",
      "step 1380 , total_loss: 0.8049, data_loss: 0.8049\n",
      "step 1400 , total_loss: 0.7549, data_loss: 0.7549\n",
      "step 1420 , total_loss: 0.7252, data_loss: 0.7252\n",
      "step 1440 , total_loss: 0.7706, data_loss: 0.7706\n",
      "step 1460 , total_loss: 0.6995, data_loss: 0.6995\n",
      "step 1480 , total_loss: 0.6894, data_loss: 0.6894\n",
      "step 1500 , total_loss: 0.7296, data_loss: 0.7296\n",
      "step 1520 , total_loss: 0.7028, data_loss: 0.7028\n",
      "step 1540 , total_loss: 0.8030, data_loss: 0.8030\n",
      "step 1560 , total_loss: 0.6918, data_loss: 0.6918\n",
      "step 1580 , total_loss: 0.8365, data_loss: 0.8365\n",
      "step 1600 , total_loss: 0.7398, data_loss: 0.7398\n",
      "step 1620 , total_loss: 0.7695, data_loss: 0.7695\n",
      "step 1640 , total_loss: 0.7785, data_loss: 0.7785\n",
      "step 1660 , total_loss: 0.7288, data_loss: 0.7288\n",
      "step 1680 , total_loss: 0.7647, data_loss: 0.7647\n",
      "step 1700 , total_loss: 0.7267, data_loss: 0.7267\n",
      "step 1720 , total_loss: 0.7757, data_loss: 0.7757\n",
      "step 1740 , total_loss: 0.6772, data_loss: 0.6772\n",
      "step 1760 , total_loss: 0.6910, data_loss: 0.6910\n",
      "step 1780 , total_loss: 0.7297, data_loss: 0.7297\n",
      "step 1800 , total_loss: 0.7668, data_loss: 0.7668\n",
      "step 1820 , total_loss: 0.7477, data_loss: 0.7477\n",
      "step 1840 , total_loss: 0.7564, data_loss: 0.7564\n",
      "step 1860 , total_loss: 0.7092, data_loss: 0.7092\n",
      "step 1880 , total_loss: 0.7364, data_loss: 0.7364\n",
      "step 1900 , total_loss: 0.7455, data_loss: 0.7455\n",
      "step 1920 , total_loss: 0.8151, data_loss: 0.8151\n",
      "step 1940 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 1960 , total_loss: 0.8880, data_loss: 0.8880\n",
      "step 1980 , total_loss: 0.6893, data_loss: 0.6893\n",
      "step 2000 , total_loss: 0.7048, data_loss: 0.7048\n",
      "step 2020 , total_loss: 0.7484, data_loss: 0.7484\n",
      "step 2040 , total_loss: 0.7291, data_loss: 0.7291\n",
      "step 2060 , total_loss: 0.7810, data_loss: 0.7810\n",
      "step 2080 , total_loss: 0.7260, data_loss: 0.7260\n",
      "step 2100 , total_loss: 0.8115, data_loss: 0.8115\n",
      "step 2120 , total_loss: 0.7629, data_loss: 0.7629\n",
      "step 2140 , total_loss: 0.6769, data_loss: 0.6769\n",
      "step 2160 , total_loss: 0.7446, data_loss: 0.7446\n",
      "step 2180 , total_loss: 0.8075, data_loss: 0.8075\n",
      "step 2200 , total_loss: 0.7570, data_loss: 0.7570\n",
      "step 2220 , total_loss: 0.6955, data_loss: 0.6955\n",
      "step 2240 , total_loss: 0.7616, data_loss: 0.7616\n",
      "step 2260 , total_loss: 0.7476, data_loss: 0.7476\n",
      "step 2280 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 2300 , total_loss: 0.7872, data_loss: 0.7872\n",
      "step 2320 , total_loss: 0.7347, data_loss: 0.7347\n",
      "step 2340 , total_loss: 0.8369, data_loss: 0.8369\n",
      "step 2360 , total_loss: 0.7839, data_loss: 0.7839\n",
      "step 2380 , total_loss: 0.7730, data_loss: 0.7730\n",
      "step 2400 , total_loss: 0.7606, data_loss: 0.7606\n",
      "step 2420 , total_loss: 0.7305, data_loss: 0.7305\n",
      "step 2440 , total_loss: 0.7345, data_loss: 0.7345\n",
      "step 2460 , total_loss: 0.7013, data_loss: 0.7013\n",
      "step 2480 , total_loss: 0.6903, data_loss: 0.6903\n",
      "step 2500 , total_loss: 0.6860, data_loss: 0.6860\n",
      "step 2520 , total_loss: 0.7199, data_loss: 0.7199\n",
      "step 2540 , total_loss: 0.8012, data_loss: 0.8012\n",
      "step 2560 , total_loss: 0.6816, data_loss: 0.6816\n",
      "step 2580 , total_loss: 0.7979, data_loss: 0.7979\n",
      "step 2600 , total_loss: 0.7115, data_loss: 0.7115\n",
      "step 2620 , total_loss: 0.8105, data_loss: 0.8105\n",
      "step 2640 , total_loss: 0.8230, data_loss: 0.8230\n",
      "step 2660 , total_loss: 0.7744, data_loss: 0.7744\n",
      "step 2680 , total_loss: 0.8699, data_loss: 0.8699\n",
      "step 2700 , total_loss: 0.7168, data_loss: 0.7168\n",
      "step 2720 , total_loss: 0.7268, data_loss: 0.7268\n",
      "step 2740 , total_loss: 0.7272, data_loss: 0.7272\n",
      "step 2760 , total_loss: 0.7201, data_loss: 0.7201\n",
      "step 2780 , total_loss: 0.7958, data_loss: 0.7958\n",
      "step 2800 , total_loss: 0.7619, data_loss: 0.7619\n",
      "step 2820 , total_loss: 0.8116, data_loss: 0.8116\n",
      "step 2840 , total_loss: 0.7087, data_loss: 0.7087\n",
      "step 2860 , total_loss: 0.7188, data_loss: 0.7188\n",
      "step 2880 , total_loss: 0.6743, data_loss: 0.6743\n",
      "step 2900 , total_loss: 0.6683, data_loss: 0.6683\n",
      "step 2920 , total_loss: 0.7688, data_loss: 0.7688\n",
      "step 2940 , total_loss: 0.6661, data_loss: 0.6661\n",
      "step 2960 , total_loss: 0.7720, data_loss: 0.7720\n",
      "step 2980 , total_loss: 0.7276, data_loss: 0.7276\n",
      "step 3000 , total_loss: 0.7989, data_loss: 0.7989\n",
      "step 3020 , total_loss: 0.7405, data_loss: 0.7405\n",
      "step 3040 , total_loss: 0.8036, data_loss: 0.8036\n",
      "step 3060 , total_loss: 0.7488, data_loss: 0.7488\n",
      "step 3080 , total_loss: 0.7821, data_loss: 0.7821\n",
      "step 3100 , total_loss: 0.7874, data_loss: 0.7874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3120 , total_loss: 0.7823, data_loss: 0.7823\n",
      "step 3140 , total_loss: 0.7075, data_loss: 0.7075\n",
      "step 3160 , total_loss: 0.7513, data_loss: 0.7513\n",
      "step 3180 , total_loss: 0.8149, data_loss: 0.8149\n",
      "step 3200 , total_loss: 0.8097, data_loss: 0.8097\n",
      "step 3220 , total_loss: 0.7766, data_loss: 0.7766\n",
      "step 3240 , total_loss: 0.6717, data_loss: 0.6717\n",
      "step 3260 , total_loss: 0.6865, data_loss: 0.6865\n",
      "step 3280 , total_loss: 0.8010, data_loss: 0.8010\n",
      "step 3300 , total_loss: 0.6573, data_loss: 0.6573\n",
      "eval valid at epoch 6: auc:0.8347,logloss:0.8523,mean_mrr:0.7807,ndcg@2:0.7536,ndcg@4:0.8238,ndcg@6:0.8358,group_auc:0.8353\n",
      "step 20 , total_loss: 0.6111, data_loss: 0.6111\n",
      "step 40 , total_loss: 0.6161, data_loss: 0.6161\n",
      "step 60 , total_loss: 0.6822, data_loss: 0.6822\n",
      "step 80 , total_loss: 0.6491, data_loss: 0.6491\n",
      "step 100 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 120 , total_loss: 0.7866, data_loss: 0.7866\n",
      "step 140 , total_loss: 0.6583, data_loss: 0.6583\n",
      "step 160 , total_loss: 0.6598, data_loss: 0.6598\n",
      "step 180 , total_loss: 0.7371, data_loss: 0.7371\n",
      "step 200 , total_loss: 0.7026, data_loss: 0.7026\n",
      "step 220 , total_loss: 0.6904, data_loss: 0.6904\n",
      "step 240 , total_loss: 0.6965, data_loss: 0.6965\n",
      "step 260 , total_loss: 0.6942, data_loss: 0.6942\n",
      "step 280 , total_loss: 0.6704, data_loss: 0.6704\n",
      "step 300 , total_loss: 0.7116, data_loss: 0.7116\n",
      "step 320 , total_loss: 0.6548, data_loss: 0.6548\n",
      "step 340 , total_loss: 0.6472, data_loss: 0.6472\n",
      "step 360 , total_loss: 0.6372, data_loss: 0.6372\n",
      "step 380 , total_loss: 0.7140, data_loss: 0.7140\n",
      "step 400 , total_loss: 0.6753, data_loss: 0.6753\n",
      "step 420 , total_loss: 0.6852, data_loss: 0.6852\n",
      "step 440 , total_loss: 0.6699, data_loss: 0.6699\n",
      "step 460 , total_loss: 0.6434, data_loss: 0.6434\n",
      "step 480 , total_loss: 0.6785, data_loss: 0.6785\n",
      "step 500 , total_loss: 0.7743, data_loss: 0.7743\n",
      "step 520 , total_loss: 0.7182, data_loss: 0.7182\n",
      "step 540 , total_loss: 0.7403, data_loss: 0.7403\n",
      "step 560 , total_loss: 0.6666, data_loss: 0.6666\n",
      "step 580 , total_loss: 0.7265, data_loss: 0.7265\n",
      "step 600 , total_loss: 0.6163, data_loss: 0.6163\n",
      "step 620 , total_loss: 0.6720, data_loss: 0.6720\n",
      "step 640 , total_loss: 0.7566, data_loss: 0.7566\n",
      "step 660 , total_loss: 0.6741, data_loss: 0.6741\n",
      "step 680 , total_loss: 0.7481, data_loss: 0.7481\n",
      "step 700 , total_loss: 0.7497, data_loss: 0.7497\n",
      "step 720 , total_loss: 0.6971, data_loss: 0.6971\n",
      "step 740 , total_loss: 0.6175, data_loss: 0.6175\n",
      "step 760 , total_loss: 0.6454, data_loss: 0.6454\n",
      "step 780 , total_loss: 0.7056, data_loss: 0.7056\n",
      "step 800 , total_loss: 0.6659, data_loss: 0.6659\n",
      "step 820 , total_loss: 0.7515, data_loss: 0.7515\n",
      "step 840 , total_loss: 0.6983, data_loss: 0.6983\n",
      "step 860 , total_loss: 0.6461, data_loss: 0.6461\n",
      "step 880 , total_loss: 0.6767, data_loss: 0.6767\n",
      "step 900 , total_loss: 0.6361, data_loss: 0.6361\n",
      "step 920 , total_loss: 0.6656, data_loss: 0.6656\n",
      "step 940 , total_loss: 0.7286, data_loss: 0.7286\n",
      "step 960 , total_loss: 0.7503, data_loss: 0.7503\n",
      "step 980 , total_loss: 0.6836, data_loss: 0.6836\n",
      "step 1000 , total_loss: 0.7183, data_loss: 0.7183\n",
      "step 1020 , total_loss: 0.7893, data_loss: 0.7893\n",
      "step 1040 , total_loss: 0.7076, data_loss: 0.7076\n",
      "step 1060 , total_loss: 0.6900, data_loss: 0.6900\n",
      "step 1080 , total_loss: 0.7046, data_loss: 0.7046\n",
      "step 1100 , total_loss: 0.6660, data_loss: 0.6660\n",
      "step 1120 , total_loss: 0.7646, data_loss: 0.7646\n",
      "step 1140 , total_loss: 0.6545, data_loss: 0.6545\n",
      "step 1160 , total_loss: 0.6761, data_loss: 0.6761\n",
      "step 1180 , total_loss: 0.6760, data_loss: 0.6760\n",
      "step 1200 , total_loss: 0.7563, data_loss: 0.7563\n",
      "step 1220 , total_loss: 0.6905, data_loss: 0.6905\n",
      "step 1240 , total_loss: 0.7100, data_loss: 0.7100\n",
      "step 1260 , total_loss: 0.7137, data_loss: 0.7137\n",
      "step 1280 , total_loss: 0.7167, data_loss: 0.7167\n",
      "step 1300 , total_loss: 0.7310, data_loss: 0.7310\n",
      "step 1320 , total_loss: 0.7529, data_loss: 0.7529\n",
      "step 1340 , total_loss: 0.6969, data_loss: 0.6969\n",
      "step 1360 , total_loss: 0.8164, data_loss: 0.8164\n",
      "step 1380 , total_loss: 0.7305, data_loss: 0.7305\n",
      "step 1400 , total_loss: 0.6812, data_loss: 0.6812\n",
      "step 1420 , total_loss: 0.6610, data_loss: 0.6610\n",
      "step 1440 , total_loss: 0.7269, data_loss: 0.7269\n",
      "step 1460 , total_loss: 0.6348, data_loss: 0.6348\n",
      "step 1480 , total_loss: 0.6385, data_loss: 0.6385\n",
      "step 1500 , total_loss: 0.7442, data_loss: 0.7442\n",
      "step 1520 , total_loss: 0.7122, data_loss: 0.7122\n",
      "step 1540 , total_loss: 0.6670, data_loss: 0.6670\n",
      "step 1560 , total_loss: 0.8001, data_loss: 0.8001\n",
      "step 1580 , total_loss: 0.6410, data_loss: 0.6410\n",
      "step 1600 , total_loss: 0.7066, data_loss: 0.7066\n",
      "step 1620 , total_loss: 0.7326, data_loss: 0.7326\n",
      "step 1640 , total_loss: 0.7484, data_loss: 0.7484\n",
      "step 1660 , total_loss: 0.7296, data_loss: 0.7296\n",
      "step 1680 , total_loss: 0.7124, data_loss: 0.7124\n",
      "step 1700 , total_loss: 0.8135, data_loss: 0.8135\n",
      "step 1720 , total_loss: 0.7613, data_loss: 0.7613\n",
      "step 1740 , total_loss: 0.7327, data_loss: 0.7327\n",
      "step 1760 , total_loss: 0.7867, data_loss: 0.7867\n",
      "step 1780 , total_loss: 0.7012, data_loss: 0.7012\n",
      "step 1800 , total_loss: 0.6971, data_loss: 0.6971\n",
      "step 1820 , total_loss: 0.7958, data_loss: 0.7958\n",
      "step 1840 , total_loss: 0.6962, data_loss: 0.6962\n",
      "step 1860 , total_loss: 0.7286, data_loss: 0.7286\n",
      "step 1880 , total_loss: 0.7057, data_loss: 0.7057\n",
      "step 1900 , total_loss: 0.6721, data_loss: 0.6721\n",
      "step 1920 , total_loss: 0.7672, data_loss: 0.7672\n",
      "step 1940 , total_loss: 0.7217, data_loss: 0.7217\n",
      "step 1960 , total_loss: 0.6089, data_loss: 0.6089\n",
      "step 1980 , total_loss: 0.7476, data_loss: 0.7476\n",
      "step 2000 , total_loss: 0.6920, data_loss: 0.6920\n",
      "step 2020 , total_loss: 0.6976, data_loss: 0.6976\n",
      "step 2040 , total_loss: 0.7447, data_loss: 0.7447\n",
      "step 2060 , total_loss: 0.7570, data_loss: 0.7570\n",
      "step 2080 , total_loss: 0.6933, data_loss: 0.6933\n",
      "step 2100 , total_loss: 0.7210, data_loss: 0.7210\n",
      "step 2120 , total_loss: 0.6634, data_loss: 0.6634\n",
      "step 2140 , total_loss: 0.7510, data_loss: 0.7510\n",
      "step 2160 , total_loss: 0.6613, data_loss: 0.6613\n",
      "step 2180 , total_loss: 0.7815, data_loss: 0.7815\n",
      "step 2200 , total_loss: 0.7738, data_loss: 0.7738\n",
      "step 2220 , total_loss: 0.7313, data_loss: 0.7313\n",
      "step 2240 , total_loss: 0.7680, data_loss: 0.7680\n",
      "step 2260 , total_loss: 0.7357, data_loss: 0.7357\n",
      "step 2280 , total_loss: 0.7085, data_loss: 0.7085\n",
      "step 2300 , total_loss: 0.7019, data_loss: 0.7019\n",
      "step 2320 , total_loss: 0.6805, data_loss: 0.6805\n",
      "step 2340 , total_loss: 0.7990, data_loss: 0.7990\n",
      "step 2360 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 2380 , total_loss: 0.6364, data_loss: 0.6364\n",
      "step 2400 , total_loss: 0.6099, data_loss: 0.6099\n",
      "step 2420 , total_loss: 0.6667, data_loss: 0.6667\n",
      "step 2440 , total_loss: 0.7211, data_loss: 0.7211\n",
      "step 2460 , total_loss: 0.6477, data_loss: 0.6477\n",
      "step 2480 , total_loss: 0.7379, data_loss: 0.7379\n",
      "step 2500 , total_loss: 0.6627, data_loss: 0.6627\n",
      "step 2520 , total_loss: 0.7047, data_loss: 0.7047\n",
      "step 2540 , total_loss: 0.8087, data_loss: 0.8087\n",
      "step 2560 , total_loss: 0.6932, data_loss: 0.6932\n",
      "step 2580 , total_loss: 0.7477, data_loss: 0.7477\n",
      "step 2600 , total_loss: 0.7032, data_loss: 0.7032\n",
      "step 2620 , total_loss: 0.6894, data_loss: 0.6894\n",
      "step 2640 , total_loss: 0.6746, data_loss: 0.6746\n",
      "step 2660 , total_loss: 0.7723, data_loss: 0.7723\n",
      "step 2680 , total_loss: 0.7344, data_loss: 0.7344\n",
      "step 2700 , total_loss: 0.8503, data_loss: 0.8503\n",
      "step 2720 , total_loss: 0.7913, data_loss: 0.7913\n",
      "step 2740 , total_loss: 0.7249, data_loss: 0.7249\n",
      "step 2760 , total_loss: 0.7335, data_loss: 0.7335\n",
      "step 2780 , total_loss: 0.7695, data_loss: 0.7695\n",
      "step 2800 , total_loss: 0.7385, data_loss: 0.7385\n",
      "step 2820 , total_loss: 0.7219, data_loss: 0.7219\n",
      "step 2840 , total_loss: 0.6820, data_loss: 0.6820\n",
      "step 2860 , total_loss: 0.7815, data_loss: 0.7815\n",
      "step 2880 , total_loss: 0.7311, data_loss: 0.7311\n",
      "step 2900 , total_loss: 0.6712, data_loss: 0.6712\n",
      "step 2920 , total_loss: 0.7085, data_loss: 0.7085\n",
      "step 2940 , total_loss: 0.6978, data_loss: 0.6978\n",
      "step 2960 , total_loss: 0.7358, data_loss: 0.7358\n",
      "step 2980 , total_loss: 0.6392, data_loss: 0.6392\n",
      "step 3000 , total_loss: 0.7492, data_loss: 0.7492\n",
      "step 3020 , total_loss: 0.6691, data_loss: 0.6691\n",
      "step 3040 , total_loss: 0.7282, data_loss: 0.7282\n",
      "step 3060 , total_loss: 0.7285, data_loss: 0.7285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3080 , total_loss: 0.7365, data_loss: 0.7365\n",
      "step 3100 , total_loss: 0.7337, data_loss: 0.7337\n",
      "step 3120 , total_loss: 0.7146, data_loss: 0.7146\n",
      "step 3140 , total_loss: 0.6949, data_loss: 0.6949\n",
      "step 3160 , total_loss: 0.8358, data_loss: 0.8358\n",
      "step 3180 , total_loss: 0.7386, data_loss: 0.7386\n",
      "step 3200 , total_loss: 0.7459, data_loss: 0.7459\n",
      "step 3220 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 3240 , total_loss: 0.7371, data_loss: 0.7371\n",
      "step 3260 , total_loss: 0.7931, data_loss: 0.7931\n",
      "step 3280 , total_loss: 0.7208, data_loss: 0.7208\n",
      "step 3300 , total_loss: 0.7402, data_loss: 0.7402\n",
      "eval valid at epoch 7: auc:0.8361,logloss:0.8996,mean_mrr:0.7833,ndcg@2:0.7567,ndcg@4:0.8259,ndcg@6:0.8378,group_auc:0.8376\n",
      "step 20 , total_loss: 0.6369, data_loss: 0.6369\n",
      "step 40 , total_loss: 0.6736, data_loss: 0.6736\n",
      "step 60 , total_loss: 0.6288, data_loss: 0.6288\n",
      "step 80 , total_loss: 0.6596, data_loss: 0.6596\n",
      "step 100 , total_loss: 0.6385, data_loss: 0.6385\n",
      "step 120 , total_loss: 0.6840, data_loss: 0.6840\n",
      "step 140 , total_loss: 0.6546, data_loss: 0.6546\n",
      "step 160 , total_loss: 0.6393, data_loss: 0.6393\n",
      "step 180 , total_loss: 0.6304, data_loss: 0.6304\n",
      "step 200 , total_loss: 0.6690, data_loss: 0.6690\n",
      "step 220 , total_loss: 0.5967, data_loss: 0.5967\n",
      "step 240 , total_loss: 0.6242, data_loss: 0.6242\n",
      "step 260 , total_loss: 0.6187, data_loss: 0.6187\n",
      "step 280 , total_loss: 0.6421, data_loss: 0.6421\n",
      "step 300 , total_loss: 0.7013, data_loss: 0.7013\n",
      "step 320 , total_loss: 0.6288, data_loss: 0.6288\n",
      "step 340 , total_loss: 0.6645, data_loss: 0.6645\n",
      "step 360 , total_loss: 0.5947, data_loss: 0.5947\n",
      "step 380 , total_loss: 0.6781, data_loss: 0.6781\n",
      "step 400 , total_loss: 0.6573, data_loss: 0.6573\n",
      "step 420 , total_loss: 0.7290, data_loss: 0.7290\n",
      "step 440 , total_loss: 0.7281, data_loss: 0.7281\n",
      "step 460 , total_loss: 0.6483, data_loss: 0.6483\n",
      "step 480 , total_loss: 0.6290, data_loss: 0.6290\n",
      "step 500 , total_loss: 0.6507, data_loss: 0.6507\n",
      "step 520 , total_loss: 0.6481, data_loss: 0.6481\n",
      "step 540 , total_loss: 0.7363, data_loss: 0.7363\n",
      "step 560 , total_loss: 0.6272, data_loss: 0.6272\n",
      "step 580 , total_loss: 0.6921, data_loss: 0.6921\n",
      "step 600 , total_loss: 0.6635, data_loss: 0.6635\n",
      "step 620 , total_loss: 0.6072, data_loss: 0.6072\n",
      "step 640 , total_loss: 0.6528, data_loss: 0.6528\n",
      "step 660 , total_loss: 0.6297, data_loss: 0.6297\n",
      "step 680 , total_loss: 0.6543, data_loss: 0.6543\n",
      "step 700 , total_loss: 0.6566, data_loss: 0.6566\n",
      "step 720 , total_loss: 0.6804, data_loss: 0.6804\n",
      "step 740 , total_loss: 0.6566, data_loss: 0.6566\n",
      "step 760 , total_loss: 0.6715, data_loss: 0.6715\n",
      "step 780 , total_loss: 0.6393, data_loss: 0.6393\n",
      "step 800 , total_loss: 0.7158, data_loss: 0.7158\n",
      "step 820 , total_loss: 0.7249, data_loss: 0.7249\n",
      "step 840 , total_loss: 0.7386, data_loss: 0.7386\n",
      "step 860 , total_loss: 0.7061, data_loss: 0.7061\n",
      "step 880 , total_loss: 0.6935, data_loss: 0.6935\n",
      "step 900 , total_loss: 0.6102, data_loss: 0.6102\n",
      "step 920 , total_loss: 0.6720, data_loss: 0.6720\n",
      "step 940 , total_loss: 0.6686, data_loss: 0.6686\n",
      "step 960 , total_loss: 0.6242, data_loss: 0.6242\n",
      "step 980 , total_loss: 0.6700, data_loss: 0.6700\n",
      "step 1000 , total_loss: 0.6890, data_loss: 0.6890\n",
      "step 1020 , total_loss: 0.6604, data_loss: 0.6604\n",
      "step 1040 , total_loss: 0.7249, data_loss: 0.7249\n",
      "step 1060 , total_loss: 0.7271, data_loss: 0.7271\n",
      "step 1080 , total_loss: 0.6461, data_loss: 0.6461\n",
      "step 1100 , total_loss: 0.6618, data_loss: 0.6618\n",
      "step 1120 , total_loss: 0.7577, data_loss: 0.7577\n",
      "step 1140 , total_loss: 0.6549, data_loss: 0.6549\n",
      "step 1160 , total_loss: 0.6118, data_loss: 0.6118\n",
      "step 1180 , total_loss: 0.6795, data_loss: 0.6795\n",
      "step 1200 , total_loss: 0.7266, data_loss: 0.7266\n",
      "step 1220 , total_loss: 0.6640, data_loss: 0.6640\n",
      "step 1240 , total_loss: 0.6995, data_loss: 0.6995\n",
      "step 1260 , total_loss: 0.6958, data_loss: 0.6958\n",
      "step 1280 , total_loss: 0.6352, data_loss: 0.6352\n",
      "step 1300 , total_loss: 0.6771, data_loss: 0.6771\n",
      "step 1320 , total_loss: 0.5774, data_loss: 0.5774\n",
      "step 1340 , total_loss: 0.7155, data_loss: 0.7155\n",
      "step 1360 , total_loss: 0.7837, data_loss: 0.7837\n",
      "step 1380 , total_loss: 0.6913, data_loss: 0.6913\n",
      "step 1400 , total_loss: 0.7898, data_loss: 0.7898\n",
      "step 1420 , total_loss: 0.7135, data_loss: 0.7135\n",
      "step 1440 , total_loss: 0.7101, data_loss: 0.7101\n",
      "step 1460 , total_loss: 0.7951, data_loss: 0.7951\n",
      "step 1480 , total_loss: 0.7138, data_loss: 0.7138\n",
      "step 1500 , total_loss: 0.6647, data_loss: 0.6647\n",
      "step 1520 , total_loss: 0.7230, data_loss: 0.7230\n",
      "step 1540 , total_loss: 0.7196, data_loss: 0.7196\n",
      "step 1560 , total_loss: 0.6631, data_loss: 0.6631\n",
      "step 1580 , total_loss: 0.8232, data_loss: 0.8232\n",
      "step 1600 , total_loss: 0.6778, data_loss: 0.6778\n",
      "step 1620 , total_loss: 0.6459, data_loss: 0.6459\n",
      "step 1640 , total_loss: 0.7356, data_loss: 0.7356\n",
      "step 1660 , total_loss: 0.7724, data_loss: 0.7724\n",
      "step 1680 , total_loss: 0.6626, data_loss: 0.6626\n",
      "step 1700 , total_loss: 0.6224, data_loss: 0.6224\n",
      "step 1720 , total_loss: 0.8307, data_loss: 0.8307\n",
      "step 1740 , total_loss: 0.6684, data_loss: 0.6684\n",
      "step 1760 , total_loss: 0.6874, data_loss: 0.6874\n",
      "step 1780 , total_loss: 0.7172, data_loss: 0.7172\n",
      "step 1800 , total_loss: 0.7398, data_loss: 0.7398\n",
      "step 1820 , total_loss: 0.6244, data_loss: 0.6244\n",
      "step 1840 , total_loss: 0.6900, data_loss: 0.6900\n",
      "step 1860 , total_loss: 0.6727, data_loss: 0.6727\n",
      "step 1880 , total_loss: 0.6036, data_loss: 0.6036\n",
      "step 1900 , total_loss: 0.6464, data_loss: 0.6464\n",
      "step 1920 , total_loss: 0.7425, data_loss: 0.7425\n",
      "step 1940 , total_loss: 0.7111, data_loss: 0.7111\n",
      "step 1960 , total_loss: 0.7069, data_loss: 0.7069\n",
      "step 1980 , total_loss: 0.7173, data_loss: 0.7173\n",
      "step 2000 , total_loss: 0.7946, data_loss: 0.7946\n",
      "step 2020 , total_loss: 0.6342, data_loss: 0.6342\n",
      "step 2040 , total_loss: 0.6502, data_loss: 0.6502\n",
      "step 2060 , total_loss: 0.7213, data_loss: 0.7213\n",
      "step 2080 , total_loss: 0.7456, data_loss: 0.7456\n",
      "step 2100 , total_loss: 0.6826, data_loss: 0.6826\n",
      "step 2120 , total_loss: 0.6780, data_loss: 0.6780\n",
      "step 2140 , total_loss: 0.7240, data_loss: 0.7240\n",
      "step 2160 , total_loss: 0.6389, data_loss: 0.6389\n",
      "step 2180 , total_loss: 0.6888, data_loss: 0.6888\n",
      "step 2200 , total_loss: 0.6810, data_loss: 0.6810\n",
      "step 2220 , total_loss: 0.7001, data_loss: 0.7001\n",
      "step 2240 , total_loss: 0.7302, data_loss: 0.7302\n",
      "step 2260 , total_loss: 0.6958, data_loss: 0.6958\n",
      "step 2280 , total_loss: 0.7284, data_loss: 0.7284\n",
      "step 2300 , total_loss: 0.6982, data_loss: 0.6982\n",
      "step 2320 , total_loss: 0.7429, data_loss: 0.7429\n",
      "step 2340 , total_loss: 0.6798, data_loss: 0.6798\n",
      "step 2360 , total_loss: 0.6378, data_loss: 0.6378\n",
      "step 2380 , total_loss: 0.7496, data_loss: 0.7496\n",
      "step 2400 , total_loss: 0.8083, data_loss: 0.8083\n",
      "step 2420 , total_loss: 0.7760, data_loss: 0.7760\n",
      "step 2440 , total_loss: 0.7375, data_loss: 0.7375\n",
      "step 2460 , total_loss: 0.7704, data_loss: 0.7704\n",
      "step 2480 , total_loss: 0.6940, data_loss: 0.6940\n",
      "step 2500 , total_loss: 0.7274, data_loss: 0.7274\n",
      "step 2520 , total_loss: 0.7111, data_loss: 0.7111\n",
      "step 2540 , total_loss: 0.7007, data_loss: 0.7007\n",
      "step 2560 , total_loss: 0.6630, data_loss: 0.6630\n",
      "step 2580 , total_loss: 0.6533, data_loss: 0.6533\n",
      "step 2600 , total_loss: 0.7293, data_loss: 0.7293\n",
      "step 2620 , total_loss: 0.7108, data_loss: 0.7108\n",
      "step 2640 , total_loss: 0.8413, data_loss: 0.8413\n",
      "step 2660 , total_loss: 0.7096, data_loss: 0.7096\n",
      "step 2680 , total_loss: 0.6382, data_loss: 0.6382\n",
      "step 2700 , total_loss: 0.7021, data_loss: 0.7021\n",
      "step 2720 , total_loss: 0.6346, data_loss: 0.6346\n",
      "step 2740 , total_loss: 0.6722, data_loss: 0.6722\n",
      "step 2760 , total_loss: 0.7343, data_loss: 0.7343\n",
      "step 2780 , total_loss: 0.7110, data_loss: 0.7110\n",
      "step 2800 , total_loss: 0.6727, data_loss: 0.6727\n",
      "step 2820 , total_loss: 0.7531, data_loss: 0.7531\n",
      "step 2840 , total_loss: 0.6507, data_loss: 0.6507\n",
      "step 2860 , total_loss: 0.7548, data_loss: 0.7548\n",
      "step 2880 , total_loss: 0.6641, data_loss: 0.6641\n",
      "step 2900 , total_loss: 0.7246, data_loss: 0.7246\n",
      "step 2920 , total_loss: 0.6628, data_loss: 0.6628\n",
      "step 2940 , total_loss: 0.6243, data_loss: 0.6243\n",
      "step 2960 , total_loss: 0.6492, data_loss: 0.6492\n",
      "step 2980 , total_loss: 0.6329, data_loss: 0.6329\n",
      "step 3000 , total_loss: 0.7770, data_loss: 0.7770\n",
      "step 3020 , total_loss: 0.6188, data_loss: 0.6188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3040 , total_loss: 0.7484, data_loss: 0.7484\n",
      "step 3060 , total_loss: 0.6658, data_loss: 0.6658\n",
      "step 3080 , total_loss: 0.8069, data_loss: 0.8069\n",
      "step 3100 , total_loss: 0.7356, data_loss: 0.7356\n",
      "step 3120 , total_loss: 0.6312, data_loss: 0.6312\n",
      "step 3140 , total_loss: 0.6954, data_loss: 0.6954\n",
      "step 3160 , total_loss: 0.7290, data_loss: 0.7290\n",
      "step 3180 , total_loss: 0.7459, data_loss: 0.7459\n",
      "step 3200 , total_loss: 0.6729, data_loss: 0.6729\n",
      "step 3220 , total_loss: 0.7228, data_loss: 0.7228\n",
      "step 3240 , total_loss: 0.7415, data_loss: 0.7415\n",
      "step 3260 , total_loss: 0.6821, data_loss: 0.6821\n",
      "step 3280 , total_loss: 0.7131, data_loss: 0.7131\n",
      "step 3300 , total_loss: 0.7243, data_loss: 0.7243\n",
      "eval valid at epoch 8: auc:0.8347,logloss:0.9789,mean_mrr:0.7816,ndcg@2:0.7539,ndcg@4:0.8242,ndcg@6:0.8365,group_auc:0.8357\n",
      "step 20 , total_loss: 0.6788, data_loss: 0.6788\n",
      "step 40 , total_loss: 0.5983, data_loss: 0.5983\n",
      "step 60 , total_loss: 0.6500, data_loss: 0.6500\n",
      "step 80 , total_loss: 0.6450, data_loss: 0.6450\n",
      "step 100 , total_loss: 0.6177, data_loss: 0.6177\n",
      "step 120 , total_loss: 0.6595, data_loss: 0.6595\n",
      "step 140 , total_loss: 0.7092, data_loss: 0.7092\n",
      "step 160 , total_loss: 0.6479, data_loss: 0.6479\n",
      "step 180 , total_loss: 0.6835, data_loss: 0.6835\n",
      "step 200 , total_loss: 0.6765, data_loss: 0.6765\n",
      "step 220 , total_loss: 0.6585, data_loss: 0.6585\n",
      "step 240 , total_loss: 0.6634, data_loss: 0.6634\n",
      "step 260 , total_loss: 0.6083, data_loss: 0.6083\n",
      "step 280 , total_loss: 0.6384, data_loss: 0.6384\n",
      "step 300 , total_loss: 0.6778, data_loss: 0.6778\n",
      "step 320 , total_loss: 0.5897, data_loss: 0.5897\n",
      "step 340 , total_loss: 0.7466, data_loss: 0.7466\n",
      "step 360 , total_loss: 0.6249, data_loss: 0.6249\n",
      "step 380 , total_loss: 0.6191, data_loss: 0.6191\n",
      "step 400 , total_loss: 0.6205, data_loss: 0.6205\n",
      "step 420 , total_loss: 0.6312, data_loss: 0.6312\n",
      "step 440 , total_loss: 0.6269, data_loss: 0.6269\n",
      "step 460 , total_loss: 0.6784, data_loss: 0.6784\n",
      "step 480 , total_loss: 0.5955, data_loss: 0.5955\n",
      "step 500 , total_loss: 0.6453, data_loss: 0.6453\n",
      "step 520 , total_loss: 0.6818, data_loss: 0.6818\n",
      "step 540 , total_loss: 0.6319, data_loss: 0.6319\n",
      "step 560 , total_loss: 0.6132, data_loss: 0.6132\n",
      "step 580 , total_loss: 0.7497, data_loss: 0.7497\n",
      "step 600 , total_loss: 0.6829, data_loss: 0.6829\n",
      "step 620 , total_loss: 0.6491, data_loss: 0.6491\n",
      "step 640 , total_loss: 0.7144, data_loss: 0.7144\n",
      "step 660 , total_loss: 0.6656, data_loss: 0.6656\n",
      "step 680 , total_loss: 0.7064, data_loss: 0.7064\n",
      "step 700 , total_loss: 0.7329, data_loss: 0.7329\n",
      "step 720 , total_loss: 0.5935, data_loss: 0.5935\n",
      "step 740 , total_loss: 0.6391, data_loss: 0.6391\n",
      "step 760 , total_loss: 0.7048, data_loss: 0.7048\n",
      "step 780 , total_loss: 0.6759, data_loss: 0.6759\n",
      "step 800 , total_loss: 0.6255, data_loss: 0.6255\n",
      "step 820 , total_loss: 0.6121, data_loss: 0.6121\n",
      "step 840 , total_loss: 0.6860, data_loss: 0.6860\n",
      "step 860 , total_loss: 0.6868, data_loss: 0.6868\n",
      "step 880 , total_loss: 0.6617, data_loss: 0.6617\n",
      "step 900 , total_loss: 0.7459, data_loss: 0.7459\n",
      "step 920 , total_loss: 0.7142, data_loss: 0.7142\n",
      "step 940 , total_loss: 0.5895, data_loss: 0.5895\n",
      "step 960 , total_loss: 0.6874, data_loss: 0.6874\n",
      "step 980 , total_loss: 0.6956, data_loss: 0.6956\n",
      "step 1000 , total_loss: 0.6252, data_loss: 0.6252\n",
      "step 1020 , total_loss: 0.6637, data_loss: 0.6637\n",
      "step 1040 , total_loss: 0.5965, data_loss: 0.5965\n",
      "step 1060 , total_loss: 0.6256, data_loss: 0.6256\n",
      "step 1080 , total_loss: 0.7343, data_loss: 0.7343\n",
      "step 1100 , total_loss: 0.6646, data_loss: 0.6646\n",
      "step 1120 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 1140 , total_loss: 0.7001, data_loss: 0.7001\n",
      "step 1160 , total_loss: 0.6677, data_loss: 0.6677\n",
      "step 1180 , total_loss: 0.6203, data_loss: 0.6203\n",
      "step 1200 , total_loss: 0.5775, data_loss: 0.5775\n",
      "step 1220 , total_loss: 0.6258, data_loss: 0.6258\n",
      "step 1240 , total_loss: 0.6973, data_loss: 0.6973\n",
      "step 1260 , total_loss: 0.6269, data_loss: 0.6269\n",
      "step 1280 , total_loss: 0.7480, data_loss: 0.7480\n",
      "step 1300 , total_loss: 0.6618, data_loss: 0.6618\n",
      "step 1320 , total_loss: 0.6105, data_loss: 0.6105\n",
      "step 1340 , total_loss: 0.6766, data_loss: 0.6766\n",
      "step 1360 , total_loss: 0.7256, data_loss: 0.7256\n",
      "step 1380 , total_loss: 0.5999, data_loss: 0.5999\n",
      "step 1400 , total_loss: 0.6399, data_loss: 0.6399\n",
      "step 1420 , total_loss: 0.7447, data_loss: 0.7447\n",
      "step 1440 , total_loss: 0.7094, data_loss: 0.7094\n",
      "step 1460 , total_loss: 0.7282, data_loss: 0.7282\n",
      "step 1480 , total_loss: 0.6300, data_loss: 0.6300\n",
      "step 1500 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 1520 , total_loss: 0.6519, data_loss: 0.6519\n",
      "step 1540 , total_loss: 0.6790, data_loss: 0.6790\n",
      "step 1560 , total_loss: 0.7069, data_loss: 0.7069\n",
      "step 1580 , total_loss: 0.7174, data_loss: 0.7174\n",
      "step 1600 , total_loss: 0.6891, data_loss: 0.6891\n",
      "step 1620 , total_loss: 0.6669, data_loss: 0.6669\n",
      "step 1640 , total_loss: 0.7310, data_loss: 0.7310\n",
      "step 1660 , total_loss: 0.6727, data_loss: 0.6727\n",
      "step 1680 , total_loss: 0.6862, data_loss: 0.6862\n",
      "step 1700 , total_loss: 0.6918, data_loss: 0.6918\n",
      "step 1720 , total_loss: 0.6436, data_loss: 0.6436\n",
      "step 1740 , total_loss: 0.6017, data_loss: 0.6017\n",
      "step 1760 , total_loss: 0.6804, data_loss: 0.6804\n",
      "step 1780 , total_loss: 0.6592, data_loss: 0.6592\n",
      "step 1800 , total_loss: 0.6765, data_loss: 0.6765\n",
      "step 1820 , total_loss: 0.6619, data_loss: 0.6619\n",
      "step 1840 , total_loss: 0.6754, data_loss: 0.6754\n",
      "step 1860 , total_loss: 0.7207, data_loss: 0.7207\n",
      "step 1880 , total_loss: 0.7085, data_loss: 0.7085\n",
      "step 1900 , total_loss: 0.6479, data_loss: 0.6479\n",
      "step 1920 , total_loss: 0.7277, data_loss: 0.7277\n",
      "step 1940 , total_loss: 0.6572, data_loss: 0.6572\n",
      "step 1960 , total_loss: 0.6778, data_loss: 0.6778\n",
      "step 1980 , total_loss: 0.7023, data_loss: 0.7023\n",
      "step 2000 , total_loss: 0.6540, data_loss: 0.6540\n",
      "step 2020 , total_loss: 0.6982, data_loss: 0.6982\n",
      "step 2040 , total_loss: 0.6307, data_loss: 0.6307\n",
      "step 2060 , total_loss: 0.7720, data_loss: 0.7720\n",
      "step 2080 , total_loss: 0.7033, data_loss: 0.7033\n",
      "step 2100 , total_loss: 0.7072, data_loss: 0.7072\n",
      "step 2120 , total_loss: 0.5984, data_loss: 0.5984\n",
      "step 2140 , total_loss: 0.6756, data_loss: 0.6756\n",
      "step 2160 , total_loss: 0.6456, data_loss: 0.6456\n",
      "step 2180 , total_loss: 0.6093, data_loss: 0.6093\n",
      "step 2200 , total_loss: 0.7797, data_loss: 0.7797\n",
      "step 2220 , total_loss: 0.6386, data_loss: 0.6386\n",
      "step 2240 , total_loss: 0.6900, data_loss: 0.6900\n",
      "step 2260 , total_loss: 0.6889, data_loss: 0.6889\n",
      "step 2280 , total_loss: 0.6988, data_loss: 0.6988\n",
      "step 2300 , total_loss: 0.6361, data_loss: 0.6361\n",
      "step 2320 , total_loss: 0.7767, data_loss: 0.7767\n",
      "step 2340 , total_loss: 0.7468, data_loss: 0.7468\n",
      "step 2360 , total_loss: 0.6431, data_loss: 0.6431\n",
      "step 2380 , total_loss: 0.6680, data_loss: 0.6680\n",
      "step 2400 , total_loss: 0.6484, data_loss: 0.6484\n",
      "step 2420 , total_loss: 0.7537, data_loss: 0.7537\n",
      "step 2440 , total_loss: 0.6646, data_loss: 0.6646\n",
      "step 2460 , total_loss: 0.6610, data_loss: 0.6610\n",
      "step 2480 , total_loss: 0.6614, data_loss: 0.6614\n",
      "step 2500 , total_loss: 0.6871, data_loss: 0.6871\n",
      "step 2520 , total_loss: 0.6387, data_loss: 0.6387\n",
      "step 2540 , total_loss: 0.6426, data_loss: 0.6426\n",
      "step 2560 , total_loss: 0.6592, data_loss: 0.6592\n",
      "step 2580 , total_loss: 0.6773, data_loss: 0.6773\n",
      "step 2600 , total_loss: 0.6440, data_loss: 0.6440\n",
      "step 2620 , total_loss: 0.7032, data_loss: 0.7032\n",
      "step 2640 , total_loss: 0.6338, data_loss: 0.6338\n",
      "step 2660 , total_loss: 0.7602, data_loss: 0.7602\n",
      "step 2680 , total_loss: 0.6825, data_loss: 0.6825\n",
      "step 2700 , total_loss: 0.6833, data_loss: 0.6833\n",
      "step 2720 , total_loss: 0.6798, data_loss: 0.6798\n",
      "step 2740 , total_loss: 0.6709, data_loss: 0.6709\n",
      "step 2760 , total_loss: 0.7255, data_loss: 0.7255\n",
      "step 2780 , total_loss: 0.6702, data_loss: 0.6702\n",
      "step 2800 , total_loss: 0.6608, data_loss: 0.6608\n",
      "step 2820 , total_loss: 0.6963, data_loss: 0.6963\n",
      "step 2840 , total_loss: 0.6856, data_loss: 0.6856\n",
      "step 2860 , total_loss: 0.7453, data_loss: 0.7453\n",
      "step 2880 , total_loss: 0.7284, data_loss: 0.7284\n",
      "step 2900 , total_loss: 0.6732, data_loss: 0.6732\n",
      "step 2920 , total_loss: 0.7311, data_loss: 0.7311\n",
      "step 2940 , total_loss: 0.6503, data_loss: 0.6503\n",
      "step 2960 , total_loss: 0.7664, data_loss: 0.7664\n",
      "step 2980 , total_loss: 0.6796, data_loss: 0.6796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000 , total_loss: 0.6524, data_loss: 0.6524\n",
      "step 3020 , total_loss: 0.6968, data_loss: 0.6968\n",
      "step 3040 , total_loss: 0.7295, data_loss: 0.7295\n",
      "step 3060 , total_loss: 0.6833, data_loss: 0.6833\n",
      "step 3080 , total_loss: 0.5644, data_loss: 0.5644\n",
      "step 3100 , total_loss: 0.7564, data_loss: 0.7564\n",
      "step 3120 , total_loss: 0.7400, data_loss: 0.7400\n",
      "step 3140 , total_loss: 0.6652, data_loss: 0.6652\n",
      "step 3160 , total_loss: 0.6115, data_loss: 0.6115\n",
      "step 3180 , total_loss: 0.7058, data_loss: 0.7058\n",
      "step 3200 , total_loss: 0.7259, data_loss: 0.7259\n",
      "step 3220 , total_loss: 0.6702, data_loss: 0.6702\n",
      "step 3240 , total_loss: 0.7557, data_loss: 0.7557\n",
      "step 3260 , total_loss: 0.6779, data_loss: 0.6779\n",
      "step 3280 , total_loss: 0.7235, data_loss: 0.7235\n",
      "step 3300 , total_loss: 0.7912, data_loss: 0.7912\n",
      "eval valid at epoch 9: auc:0.8359,logloss:1.0327,mean_mrr:0.783,ndcg@2:0.7557,ndcg@4:0.8252,ndcg@6:0.8375,group_auc:0.8365\n",
      "step 20 , total_loss: 0.5823, data_loss: 0.5823\n",
      "step 40 , total_loss: 0.6590, data_loss: 0.6590\n",
      "step 60 , total_loss: 0.6122, data_loss: 0.6122\n",
      "step 80 , total_loss: 0.5662, data_loss: 0.5662\n",
      "step 100 , total_loss: 0.6146, data_loss: 0.6146\n",
      "step 120 , total_loss: 0.6316, data_loss: 0.6316\n",
      "step 140 , total_loss: 0.6045, data_loss: 0.6045\n",
      "step 160 , total_loss: 0.6349, data_loss: 0.6349\n",
      "step 180 , total_loss: 0.7035, data_loss: 0.7035\n",
      "step 200 , total_loss: 0.5580, data_loss: 0.5580\n",
      "step 220 , total_loss: 0.6534, data_loss: 0.6534\n",
      "step 240 , total_loss: 0.6315, data_loss: 0.6315\n",
      "step 260 , total_loss: 0.5992, data_loss: 0.5992\n",
      "step 280 , total_loss: 0.5924, data_loss: 0.5924\n",
      "step 300 , total_loss: 0.5891, data_loss: 0.5891\n",
      "step 320 , total_loss: 0.6531, data_loss: 0.6531\n",
      "step 340 , total_loss: 0.6255, data_loss: 0.6255\n",
      "step 360 , total_loss: 0.6777, data_loss: 0.6777\n",
      "step 380 , total_loss: 0.6286, data_loss: 0.6286\n",
      "step 400 , total_loss: 0.7755, data_loss: 0.7755\n",
      "step 420 , total_loss: 0.5768, data_loss: 0.5768\n",
      "step 440 , total_loss: 0.6474, data_loss: 0.6474\n",
      "step 460 , total_loss: 0.6806, data_loss: 0.6806\n",
      "step 480 , total_loss: 0.5603, data_loss: 0.5603\n",
      "step 500 , total_loss: 0.6820, data_loss: 0.6820\n",
      "step 520 , total_loss: 0.6850, data_loss: 0.6850\n",
      "step 540 , total_loss: 0.6300, data_loss: 0.6300\n",
      "step 560 , total_loss: 0.6088, data_loss: 0.6088\n",
      "step 580 , total_loss: 0.5765, data_loss: 0.5765\n",
      "step 600 , total_loss: 0.6174, data_loss: 0.6174\n",
      "step 620 , total_loss: 0.5660, data_loss: 0.5660\n",
      "step 640 , total_loss: 0.6222, data_loss: 0.6222\n",
      "step 660 , total_loss: 0.6189, data_loss: 0.6189\n",
      "step 680 , total_loss: 0.6314, data_loss: 0.6314\n",
      "step 700 , total_loss: 0.5608, data_loss: 0.5608\n",
      "step 720 , total_loss: 0.6403, data_loss: 0.6403\n",
      "step 740 , total_loss: 0.6236, data_loss: 0.6236\n",
      "step 760 , total_loss: 0.6572, data_loss: 0.6572\n",
      "step 780 , total_loss: 0.6030, data_loss: 0.6030\n",
      "step 800 , total_loss: 0.6241, data_loss: 0.6241\n",
      "step 820 , total_loss: 0.6766, data_loss: 0.6766\n",
      "step 840 , total_loss: 0.6565, data_loss: 0.6565\n",
      "step 860 , total_loss: 0.6677, data_loss: 0.6677\n",
      "step 880 , total_loss: 0.6291, data_loss: 0.6291\n",
      "step 900 , total_loss: 0.6456, data_loss: 0.6456\n",
      "step 920 , total_loss: 0.5860, data_loss: 0.5860\n",
      "step 940 , total_loss: 0.6899, data_loss: 0.6899\n",
      "step 960 , total_loss: 0.6976, data_loss: 0.6976\n",
      "step 980 , total_loss: 0.6093, data_loss: 0.6093\n",
      "step 1000 , total_loss: 0.6103, data_loss: 0.6103\n",
      "step 1020 , total_loss: 0.6799, data_loss: 0.6799\n",
      "step 1040 , total_loss: 0.6604, data_loss: 0.6604\n",
      "step 1060 , total_loss: 0.7100, data_loss: 0.7100\n",
      "step 1080 , total_loss: 0.7095, data_loss: 0.7095\n",
      "step 1100 , total_loss: 0.7036, data_loss: 0.7036\n",
      "step 1120 , total_loss: 0.6132, data_loss: 0.6132\n",
      "step 1140 , total_loss: 0.5869, data_loss: 0.5869\n",
      "step 1160 , total_loss: 0.7473, data_loss: 0.7473\n",
      "step 1180 , total_loss: 0.6736, data_loss: 0.6736\n",
      "step 1200 , total_loss: 0.6396, data_loss: 0.6396\n",
      "step 1220 , total_loss: 0.7235, data_loss: 0.7235\n",
      "step 1240 , total_loss: 0.6395, data_loss: 0.6395\n",
      "step 1260 , total_loss: 0.5807, data_loss: 0.5807\n",
      "step 1280 , total_loss: 0.6464, data_loss: 0.6464\n",
      "step 1300 , total_loss: 0.6676, data_loss: 0.6676\n",
      "step 1320 , total_loss: 0.6551, data_loss: 0.6551\n",
      "step 1340 , total_loss: 0.6594, data_loss: 0.6594\n",
      "step 1360 , total_loss: 0.6694, data_loss: 0.6694\n",
      "step 1380 , total_loss: 0.6789, data_loss: 0.6789\n",
      "step 1400 , total_loss: 0.6850, data_loss: 0.6850\n",
      "step 1420 , total_loss: 0.6429, data_loss: 0.6429\n",
      "step 1440 , total_loss: 0.6811, data_loss: 0.6811\n",
      "step 1460 , total_loss: 0.6970, data_loss: 0.6970\n",
      "step 1480 , total_loss: 0.6919, data_loss: 0.6919\n",
      "step 1500 , total_loss: 0.6706, data_loss: 0.6706\n",
      "step 1520 , total_loss: 0.6755, data_loss: 0.6755\n",
      "step 1540 , total_loss: 0.6511, data_loss: 0.6511\n",
      "step 1560 , total_loss: 0.6911, data_loss: 0.6911\n",
      "step 1580 , total_loss: 0.6900, data_loss: 0.6900\n",
      "step 1600 , total_loss: 0.6551, data_loss: 0.6551\n",
      "step 1620 , total_loss: 0.6631, data_loss: 0.6631\n",
      "step 1640 , total_loss: 0.6616, data_loss: 0.6616\n",
      "step 1660 , total_loss: 0.6972, data_loss: 0.6972\n",
      "step 1680 , total_loss: 0.5895, data_loss: 0.5895\n",
      "step 1700 , total_loss: 0.6640, data_loss: 0.6640\n",
      "step 1720 , total_loss: 0.6684, data_loss: 0.6684\n",
      "step 1740 , total_loss: 0.6549, data_loss: 0.6549\n",
      "step 1760 , total_loss: 0.6933, data_loss: 0.6933\n",
      "step 1780 , total_loss: 0.6415, data_loss: 0.6415\n",
      "step 1800 , total_loss: 0.6391, data_loss: 0.6391\n",
      "step 1820 , total_loss: 0.6861, data_loss: 0.6861\n",
      "step 1840 , total_loss: 0.6503, data_loss: 0.6503\n",
      "step 1860 , total_loss: 0.7276, data_loss: 0.7276\n",
      "step 1880 , total_loss: 0.7076, data_loss: 0.7076\n",
      "step 1900 , total_loss: 0.6552, data_loss: 0.6552\n",
      "step 1920 , total_loss: 0.6434, data_loss: 0.6434\n",
      "step 1940 , total_loss: 0.6699, data_loss: 0.6699\n",
      "step 1960 , total_loss: 0.6931, data_loss: 0.6931\n",
      "step 1980 , total_loss: 0.7323, data_loss: 0.7323\n",
      "step 2000 , total_loss: 0.6778, data_loss: 0.6778\n",
      "step 2020 , total_loss: 0.6793, data_loss: 0.6793\n",
      "step 2040 , total_loss: 0.6682, data_loss: 0.6682\n",
      "step 2060 , total_loss: 0.7204, data_loss: 0.7204\n",
      "step 2080 , total_loss: 0.6593, data_loss: 0.6593\n",
      "step 2100 , total_loss: 0.6667, data_loss: 0.6667\n",
      "step 2120 , total_loss: 0.6499, data_loss: 0.6499\n",
      "step 2140 , total_loss: 0.5807, data_loss: 0.5807\n",
      "step 2160 , total_loss: 0.6374, data_loss: 0.6374\n",
      "step 2180 , total_loss: 0.6551, data_loss: 0.6551\n",
      "step 2200 , total_loss: 0.6377, data_loss: 0.6377\n",
      "step 2220 , total_loss: 0.6809, data_loss: 0.6809\n",
      "step 2240 , total_loss: 0.7067, data_loss: 0.7067\n",
      "step 2260 , total_loss: 0.6441, data_loss: 0.6441\n",
      "step 2280 , total_loss: 0.7281, data_loss: 0.7281\n",
      "step 2300 , total_loss: 0.6623, data_loss: 0.6623\n",
      "step 2320 , total_loss: 0.7025, data_loss: 0.7025\n",
      "step 2340 , total_loss: 0.7532, data_loss: 0.7532\n",
      "step 2360 , total_loss: 0.6752, data_loss: 0.6752\n",
      "step 2380 , total_loss: 0.6820, data_loss: 0.6820\n",
      "step 2400 , total_loss: 0.6977, data_loss: 0.6977\n",
      "step 2420 , total_loss: 0.6694, data_loss: 0.6694\n",
      "step 2440 , total_loss: 0.7061, data_loss: 0.7061\n",
      "step 2460 , total_loss: 0.6391, data_loss: 0.6391\n",
      "step 2480 , total_loss: 0.7226, data_loss: 0.7226\n",
      "step 2500 , total_loss: 0.6842, data_loss: 0.6842\n",
      "step 2520 , total_loss: 0.6742, data_loss: 0.6742\n",
      "step 2540 , total_loss: 0.7094, data_loss: 0.7094\n",
      "step 2560 , total_loss: 0.5827, data_loss: 0.5827\n",
      "step 2580 , total_loss: 0.6305, data_loss: 0.6305\n",
      "step 2600 , total_loss: 0.5780, data_loss: 0.5780\n",
      "step 2620 , total_loss: 0.7107, data_loss: 0.7107\n",
      "step 2640 , total_loss: 0.7070, data_loss: 0.7070\n",
      "step 2660 , total_loss: 0.6358, data_loss: 0.6358\n",
      "step 2680 , total_loss: 0.6566, data_loss: 0.6566\n",
      "step 2700 , total_loss: 0.6697, data_loss: 0.6697\n",
      "step 2720 , total_loss: 0.6434, data_loss: 0.6434\n",
      "step 2740 , total_loss: 0.7000, data_loss: 0.7000\n",
      "step 2760 , total_loss: 0.6288, data_loss: 0.6288\n",
      "step 2780 , total_loss: 0.6433, data_loss: 0.6433\n",
      "step 2800 , total_loss: 0.6754, data_loss: 0.6754\n",
      "step 2820 , total_loss: 0.6360, data_loss: 0.6360\n",
      "step 2840 , total_loss: 0.6964, data_loss: 0.6964\n",
      "step 2860 , total_loss: 0.6886, data_loss: 0.6886\n",
      "step 2880 , total_loss: 0.6779, data_loss: 0.6779\n",
      "step 2900 , total_loss: 0.6558, data_loss: 0.6558\n",
      "step 2920 , total_loss: 0.6632, data_loss: 0.6632\n",
      "step 2940 , total_loss: 0.7497, data_loss: 0.7497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2960 , total_loss: 0.6458, data_loss: 0.6458\n",
      "step 2980 , total_loss: 0.6806, data_loss: 0.6806\n",
      "step 3000 , total_loss: 0.6706, data_loss: 0.6706\n",
      "step 3020 , total_loss: 0.7140, data_loss: 0.7140\n",
      "step 3040 , total_loss: 0.7453, data_loss: 0.7453\n",
      "step 3060 , total_loss: 0.6766, data_loss: 0.6766\n",
      "step 3080 , total_loss: 0.6905, data_loss: 0.6905\n",
      "step 3100 , total_loss: 0.7347, data_loss: 0.7347\n",
      "step 3120 , total_loss: 0.6883, data_loss: 0.6883\n",
      "step 3140 , total_loss: 0.6120, data_loss: 0.6120\n",
      "step 3160 , total_loss: 0.7504, data_loss: 0.7504\n",
      "step 3180 , total_loss: 0.6173, data_loss: 0.6173\n",
      "step 3200 , total_loss: 0.6259, data_loss: 0.6259\n",
      "step 3220 , total_loss: 0.6590, data_loss: 0.6590\n",
      "step 3240 , total_loss: 0.6311, data_loss: 0.6311\n",
      "step 3260 , total_loss: 0.7021, data_loss: 0.7021\n",
      "step 3280 , total_loss: 0.6560, data_loss: 0.6560\n",
      "step 3300 , total_loss: 0.6647, data_loss: 0.6647\n",
      "eval valid at epoch 10: auc:0.8353,logloss:1.1057,mean_mrr:0.7822,ndcg@2:0.7547,ndcg@4:0.8245,ndcg@6:0.8369,group_auc:0.8358\n",
      "[(1, {'auc': 0.793, 'logloss': 0.6768, 'mean_mrr': 0.734, 'ndcg@2': 0.6958, 'ndcg@4': 0.7835, 'ndcg@6': 0.8007, 'group_auc': 0.7947}), (2, {'auc': 0.8189, 'logloss': 0.633, 'mean_mrr': 0.7634, 'ndcg@2': 0.7326, 'ndcg@4': 0.8091, 'ndcg@6': 0.8228, 'group_auc': 0.8209}), (3, {'auc': 0.8284, 'logloss': 0.6861, 'mean_mrr': 0.775, 'ndcg@2': 0.7465, 'ndcg@4': 0.8188, 'ndcg@6': 0.8315, 'group_auc': 0.8306}), (4, {'auc': 0.832, 'logloss': 0.7149, 'mean_mrr': 0.7789, 'ndcg@2': 0.7514, 'ndcg@4': 0.8221, 'ndcg@6': 0.8344, 'group_auc': 0.834}), (5, {'auc': 0.8336, 'logloss': 0.7827, 'mean_mrr': 0.78, 'ndcg@2': 0.7528, 'ndcg@4': 0.8233, 'ndcg@6': 0.8353, 'group_auc': 0.8351}), (6, {'auc': 0.8347, 'logloss': 0.8523, 'mean_mrr': 0.7807, 'ndcg@2': 0.7536, 'ndcg@4': 0.8238, 'ndcg@6': 0.8358, 'group_auc': 0.8353}), (7, {'auc': 0.8361, 'logloss': 0.8996, 'mean_mrr': 0.7833, 'ndcg@2': 0.7567, 'ndcg@4': 0.8259, 'ndcg@6': 0.8378, 'group_auc': 0.8376}), (8, {'auc': 0.8347, 'logloss': 0.9789, 'mean_mrr': 0.7816, 'ndcg@2': 0.7539, 'ndcg@4': 0.8242, 'ndcg@6': 0.8365, 'group_auc': 0.8357}), (9, {'auc': 0.8359, 'logloss': 1.0327, 'mean_mrr': 0.783, 'ndcg@2': 0.7557, 'ndcg@4': 0.8252, 'ndcg@6': 0.8375, 'group_auc': 0.8365}), (10, {'auc': 0.8353, 'logloss': 1.1057, 'mean_mrr': 0.7822, 'ndcg@2': 0.7547, 'ndcg@4': 0.8245, 'ndcg@6': 0.8369, 'group_auc': 0.8358})]\n",
      "best epoch: 7\n",
      "Time cost for training is 66.66 mins\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e41391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.8204, 'logloss': 1.3573, 'mean_mrr': 0.6512, 'ndcg@2': 0.5965, 'ndcg@4': 0.6727, 'ndcg@6': 0.7058, 'group_auc': 0.8205}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/Desktop/code/recommenders/examples/00_quick_start/../../recommenders/models/deeprec/models/base_model.py:724: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved model in ../../tests/resources/deeprec/slirec/model/din_dice/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 18:19:28.711161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 18:19:28.711360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 18:19:28.711486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 18:19:28.711654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 18:19:28.711779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 18:19:28.711877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9634 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.8221,\n",
       " 'logloss': 1.0988,\n",
       " 'mean_mrr': 0.6528,\n",
       " 'ndcg@2': 0.5977,\n",
       " 'ndcg@4': 0.6752,\n",
       " 'ndcg@6': 0.708,\n",
       " 'group_auc': 0.8229}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
