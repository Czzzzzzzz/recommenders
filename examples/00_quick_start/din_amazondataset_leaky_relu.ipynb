{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=20,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/din_dice/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/din_dice/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "                          attention_mode=\"outer_product\",\n",
    "                          activation=['leaky_relu', 'leaky_relu']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 20,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['leaky_relu', 'leaky_relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'attention_mod': 'inner_product',\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/din_dice/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/din_dice/',\n",
       " 'attention_mode': 'outer_product'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a1733",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/Desktop/code/recommenders/examples/00_quick_start/../../recommenders/models/deeprec/models/base_model.py:726: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-05 19:12:02.005638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:02.010546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:02.010668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:02.903540: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-05 19:12:02.904613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:02.904776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:02.904886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:03.219314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:03.219463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:03.219574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 19:12:03.219664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9647 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-05 19:12:03.234741: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-06-05 19:13:51.285372: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.5686, data_loss: 1.5686\n",
      "step 40 , total_loss: 1.5845, data_loss: 1.5845\n",
      "step 60 , total_loss: 1.5582, data_loss: 1.5582\n",
      "step 80 , total_loss: 1.5066, data_loss: 1.5066\n",
      "step 100 , total_loss: 1.5195, data_loss: 1.5195\n",
      "step 120 , total_loss: 1.5572, data_loss: 1.5572\n",
      "step 140 , total_loss: 1.5150, data_loss: 1.5150\n",
      "step 160 , total_loss: 1.5110, data_loss: 1.5110\n",
      "step 180 , total_loss: 1.5518, data_loss: 1.5518\n",
      "step 200 , total_loss: 1.5386, data_loss: 1.5386\n",
      "step 220 , total_loss: 1.5153, data_loss: 1.5153\n",
      "step 240 , total_loss: 1.5166, data_loss: 1.5166\n",
      "step 260 , total_loss: 1.5102, data_loss: 1.5102\n",
      "step 280 , total_loss: 1.5007, data_loss: 1.5007\n",
      "step 300 , total_loss: 1.4651, data_loss: 1.4651\n",
      "step 320 , total_loss: 1.4460, data_loss: 1.4460\n",
      "step 340 , total_loss: 1.4850, data_loss: 1.4850\n",
      "step 360 , total_loss: 1.4781, data_loss: 1.4781\n",
      "step 380 , total_loss: 1.4396, data_loss: 1.4396\n",
      "step 400 , total_loss: 1.4529, data_loss: 1.4529\n",
      "step 420 , total_loss: 1.4916, data_loss: 1.4916\n",
      "step 440 , total_loss: 1.4655, data_loss: 1.4655\n",
      "step 460 , total_loss: 1.4007, data_loss: 1.4007\n",
      "step 480 , total_loss: 1.4357, data_loss: 1.4357\n",
      "step 500 , total_loss: 1.4227, data_loss: 1.4227\n",
      "step 520 , total_loss: 1.3828, data_loss: 1.3828\n",
      "step 540 , total_loss: 1.3639, data_loss: 1.3639\n",
      "step 560 , total_loss: 1.3972, data_loss: 1.3972\n",
      "step 580 , total_loss: 1.4402, data_loss: 1.4402\n",
      "step 600 , total_loss: 1.3764, data_loss: 1.3764\n",
      "step 620 , total_loss: 1.4040, data_loss: 1.4040\n",
      "step 640 , total_loss: 1.4173, data_loss: 1.4173\n",
      "step 660 , total_loss: 1.3654, data_loss: 1.3654\n",
      "step 680 , total_loss: 1.3343, data_loss: 1.3343\n",
      "step 700 , total_loss: 1.3096, data_loss: 1.3096\n",
      "step 720 , total_loss: 1.3249, data_loss: 1.3249\n",
      "step 740 , total_loss: 1.3329, data_loss: 1.3329\n",
      "step 760 , total_loss: 1.3676, data_loss: 1.3676\n",
      "step 780 , total_loss: 1.3515, data_loss: 1.3515\n",
      "step 800 , total_loss: 1.3222, data_loss: 1.3222\n",
      "step 820 , total_loss: 1.3655, data_loss: 1.3655\n",
      "step 840 , total_loss: 1.3768, data_loss: 1.3768\n",
      "step 860 , total_loss: 1.2900, data_loss: 1.2900\n",
      "step 880 , total_loss: 1.3245, data_loss: 1.3245\n",
      "step 900 , total_loss: 1.2725, data_loss: 1.2725\n",
      "step 920 , total_loss: 1.3945, data_loss: 1.3945\n",
      "step 940 , total_loss: 1.2547, data_loss: 1.2547\n",
      "step 960 , total_loss: 1.3160, data_loss: 1.3160\n",
      "step 980 , total_loss: 1.2447, data_loss: 1.2447\n",
      "step 1000 , total_loss: 1.3479, data_loss: 1.3479\n",
      "step 1020 , total_loss: 1.3301, data_loss: 1.3301\n",
      "step 1040 , total_loss: 1.3167, data_loss: 1.3167\n",
      "step 1060 , total_loss: 1.3005, data_loss: 1.3005\n",
      "step 1080 , total_loss: 1.3076, data_loss: 1.3076\n",
      "step 1100 , total_loss: 1.2962, data_loss: 1.2962\n",
      "step 1120 , total_loss: 1.2877, data_loss: 1.2877\n",
      "step 1140 , total_loss: 1.2727, data_loss: 1.2727\n",
      "step 1160 , total_loss: 1.2598, data_loss: 1.2598\n",
      "step 1180 , total_loss: 1.2422, data_loss: 1.2422\n",
      "step 1200 , total_loss: 1.2896, data_loss: 1.2896\n",
      "step 1220 , total_loss: 1.3124, data_loss: 1.3124\n",
      "step 1240 , total_loss: 1.3014, data_loss: 1.3014\n",
      "step 1260 , total_loss: 1.2693, data_loss: 1.2693\n",
      "step 1280 , total_loss: 1.2755, data_loss: 1.2755\n",
      "step 1300 , total_loss: 1.2206, data_loss: 1.2206\n",
      "step 1320 , total_loss: 1.2948, data_loss: 1.2948\n",
      "step 1340 , total_loss: 1.2223, data_loss: 1.2223\n",
      "step 1360 , total_loss: 1.2691, data_loss: 1.2691\n",
      "step 1380 , total_loss: 1.2605, data_loss: 1.2605\n",
      "step 1400 , total_loss: 1.2028, data_loss: 1.2028\n",
      "step 1420 , total_loss: 1.2511, data_loss: 1.2511\n",
      "step 1440 , total_loss: 1.1500, data_loss: 1.1500\n",
      "step 1460 , total_loss: 1.2885, data_loss: 1.2885\n",
      "step 1480 , total_loss: 1.1253, data_loss: 1.1253\n",
      "step 1500 , total_loss: 1.2046, data_loss: 1.2046\n",
      "step 1520 , total_loss: 1.2413, data_loss: 1.2413\n",
      "step 1540 , total_loss: 1.2992, data_loss: 1.2992\n",
      "step 1560 , total_loss: 1.2611, data_loss: 1.2611\n",
      "step 1580 , total_loss: 1.2943, data_loss: 1.2943\n",
      "step 1600 , total_loss: 1.2850, data_loss: 1.2850\n",
      "step 1620 , total_loss: 1.2893, data_loss: 1.2893\n",
      "step 1640 , total_loss: 1.1532, data_loss: 1.1532\n",
      "step 1660 , total_loss: 1.2183, data_loss: 1.2183\n",
      "step 1680 , total_loss: 1.2322, data_loss: 1.2322\n",
      "step 1700 , total_loss: 1.2353, data_loss: 1.2353\n",
      "step 1720 , total_loss: 1.2718, data_loss: 1.2718\n",
      "step 1740 , total_loss: 1.2401, data_loss: 1.2401\n",
      "step 1760 , total_loss: 1.2506, data_loss: 1.2506\n",
      "step 1780 , total_loss: 1.2191, data_loss: 1.2191\n",
      "step 1800 , total_loss: 1.1830, data_loss: 1.1830\n",
      "step 1820 , total_loss: 1.1458, data_loss: 1.1458\n",
      "step 1840 , total_loss: 1.2004, data_loss: 1.2004\n",
      "step 1860 , total_loss: 1.1537, data_loss: 1.1537\n",
      "step 1880 , total_loss: 1.1615, data_loss: 1.1615\n",
      "step 1900 , total_loss: 1.2463, data_loss: 1.2463\n",
      "step 1920 , total_loss: 1.1133, data_loss: 1.1133\n",
      "step 1940 , total_loss: 1.2069, data_loss: 1.2069\n",
      "step 1960 , total_loss: 1.1580, data_loss: 1.1580\n",
      "step 1980 , total_loss: 1.1390, data_loss: 1.1390\n",
      "step 2000 , total_loss: 1.2183, data_loss: 1.2183\n",
      "step 2020 , total_loss: 1.1442, data_loss: 1.1442\n",
      "step 2040 , total_loss: 1.1882, data_loss: 1.1882\n",
      "step 2060 , total_loss: 1.1417, data_loss: 1.1417\n",
      "step 2080 , total_loss: 1.1677, data_loss: 1.1677\n",
      "step 2100 , total_loss: 1.1485, data_loss: 1.1485\n",
      "step 2120 , total_loss: 1.1955, data_loss: 1.1955\n",
      "step 2140 , total_loss: 1.2239, data_loss: 1.2239\n",
      "step 2160 , total_loss: 1.0863, data_loss: 1.0863\n",
      "step 2180 , total_loss: 1.1190, data_loss: 1.1190\n",
      "step 2200 , total_loss: 1.1440, data_loss: 1.1440\n"
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e41391",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
